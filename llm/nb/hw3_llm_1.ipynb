{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3\n",
    "\n",
    "В этом задании мы напишем архитектуру transformer с нуля. Мы пройдемся по всем слоям трансформера - от эмбеддингов и аттеншена до FFN и финального выходного слоя. В конце также напишем различные техники сэмплирования для генерации текста!\n",
    "\n",
    "\n",
    "В качестве весов мы будем использовать веса gpt2, однако уже в следующем задании попробуем обучить свой мини-трансформер с нуля!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnqvoOGqZ2oX"
   },
   "source": [
    "# Устанавливаем зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcuFItWyPbey",
    "outputId": "b0000c9b-9a0d-4eca-a32c-c9ce7e3f4df3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformer_lens in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (2.10.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (1.2.1)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (3.1.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (0.8.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (0.2.36)\n",
      "Requirement already satisfied: numpy>=1.24 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (2.2.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (13.9.4)\n",
      "Requirement already satisfied: sentencepiece in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (2.5.1)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (4.47.0)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformer_lens) (0.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (0.26.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n",
      "Requirement already satisfied: filelock in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
      "Requirement already satisfied: networkx in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from sympy==1.13.1->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers>=4.37.2->transformer_lens) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers>=4.37.2->transformer_lens) (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (5.29.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (2.10.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (75.6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (0.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jaxtyping in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (0.2.36)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
      "  Cloning https://github.com/callummcdougall/CircuitsVis.git to /private/var/folders/gf/733p_9w13nqf8l5ss3r78ybw0000gp/T/pip-req-build-ms6qzozr\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /private/var/folders/gf/733p_9w13nqf8l5ss3r78ybw0000gp/T/pip-req-build-ms6qzozr\n",
      "  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit 1e6129d08cae7af9242d9ab5d3ed322dd44b4dd3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from circuitsvis==0.0.0) (5.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.23 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from circuitsvis==0.0.0) (1.26.4)\n",
      "Requirement already satisfied: torch<3.0,>=2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from circuitsvis==0.0.0) (2.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.21.0)\n",
      "Requirement already satisfied: filelock in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from sympy==1.13.1->torch<3.0,>=2.0->circuitsvis==0.0.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from jinja2->torch<3.0,>=2.0->circuitsvis==0.0.0) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformer_lens\n",
    "%pip install einops\n",
    "%pip install jaxtyping\n",
    "%pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "f4b70daf7cf54dd298ee036a3dfdc0ab",
      "77745b9f81bd4536a735223fe5576f8c",
      "bf38836fc6bc4a4089b6e6120cf7c514",
      "0ea6e4509dc14359bb2bb8fc0582279d",
      "f189563e5e2d48f1a0e5ae34f803b80e",
      "2247decbf6cd456281e7d1f4e3773ff3",
      "fd904cda52ef4cf68a0cd041ecc65778",
      "82e19fdc343d486b92f6566f2fee44db",
      "a2259801c249461f98dd85959ec728aa",
      "e73183d327d44a31ac79fbd9d197bfea",
      "ff28d14dea69478bbb056d0b5466f72d",
      "7ae5f36ecd2a415ba14e28adc73e22dc",
      "cceb35b8e5cd4aa0a59f707219b21fc2",
      "71481143751245e88117717cdfa5fc32",
      "cbfcc250548e4b3c91664737a0a51b19",
      "56bfe4ef487e459fb726dce5fe640818",
      "06cb382af5c340ca8947830a0fdf3b0f",
      "8bbbe9f9f3f444adae68756ee6392004",
      "215dd213859547488d189fffbb1a9caf",
      "0faaf18fdf644d6eb034744810f06f16",
      "3d5493baf4014e54a0c1f42d05d62350",
      "ac49d95d3a7f4e45a1fa13a1f2e2bf5c",
      "bff8fd936bbe4ec6b7213dfdc9d4e719",
      "dd156c5a79824cbbb14a1c4fa1b7f669",
      "4b9e0984643f4de5bb9abafeb3ec2ce4",
      "da3cd1c51d784c3bb85f5bc8895f91c0",
      "1bfbfb53c0f149a4ac060cd4d4aef104",
      "52decc6f4908454fb5f97e879e99e119",
      "cda6ef6083474717955a3dd799478ed3",
      "25175593166143a38e5be680b610b53a",
      "c80699870fb64527bd52dc1aec9d7bf9",
      "32bdaa7649844365ab7b58a3544b6908",
      "f8293198a7b14853995a15ec1562ae6e",
      "5c9f5868bdfa4183ac041e2c7c3e0aa8",
      "6bbcca4c0fbd482fbf07ac69542cdad3",
      "42f1428814134733a9d7dae52707256c",
      "952b2e46ef674fdbb6e72a1e38e335f1",
      "2bd693deab614db5bc2213278951fc6f",
      "52431aa699134bf0a86f6a94d4468ba5",
      "ae57136539d54b7b9715b871bb1b2373",
      "80a0e22dd9d44adf987e0d1b841bf1d2",
      "afcbd2194b0c4ccf8754008564859516",
      "d7f8f3433d934bff96fcece7b345363d",
      "9c25a21d619e4a6d93216bca13afe45e",
      "495fe8e8bfdd43849d2a8270cb111368",
      "a9195052de6849709bb9e89e673d5b0c",
      "01679c391541478ab4c1037c9d72a0cc",
      "48c086c7c25f42728d4f367c171a5d49",
      "4db1e0f6b230488a9231fe8a24067c99",
      "eed87c1ad6034bc2a07f37a63942eb3e",
      "520b09f8db3f419c9a11d63bae3c3f4b",
      "69b08166db214508b144a5d83d813f65",
      "cb9c5cec8df94fd69c7e768c6f6c7368",
      "f336f98362eb4fde8dd589faa654379a",
      "acca64c6a5ad4ae984b9d864899430ec",
      "74561d5cdd5846fa851b92a39bf6f74c",
      "d38e6d822c9c4405bb8e6dea40b0c1db",
      "66c082e996ec47bead802a56531f1c6b",
      "e218c564f9924f1da13f383d7802ab94",
      "dcb36c7d2b4e41bbb54ba30b983862b6",
      "991a58d1f2ef42b5a07cf43d47f50c42",
      "88085ba590f34bd593d252143d4b8369",
      "9e5bb2fc59eb46bb93ccf006b01743ad",
      "04b1bc72372846b0ae4d83520421b185",
      "e801d84d9ad643c5b7a59cc493ec9f65",
      "afb1e8d11fe84284baa9adb8a334239c",
      "18704ce82e504b14820ae2ad64c99816",
      "9da7a4e0811b4af69d9f2ab95720ef71",
      "695bcf9a3acd4f929c5cc01e7606cd75",
      "5eee35d4db464c49b1942a1d564917b6",
      "9dc1a00a273b4f229b0678458f961abd",
      "46d4984f3d90431d8b0b639ba8900fb3",
      "c140a813a05a4f1383da579db95bdcb0",
      "45666d125d34436fbbd0cbfa6fcfe0e3",
      "22c8259dc73b4b1086b41ea8a91df943",
      "f0eb8208ac7b4d518c8d4d69f7c4f54f",
      "f46bcd12670842ed81a03dbec6922d25"
     ]
    },
    "id": "bAVtXXZiPJlr",
    "outputId": "58e02951-e34a-4015-a477-61efa0229d58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import os; os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch as t\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Загружаем веса gpt2 для проверки\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "reference_gpt2 = reference_gpt2.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конфиг, который хранит в себе всю информацию о размерностях модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjdS6H9pO9E0",
    "outputId": "d969637c-358d-43c6-d179-db31132d6e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, debug_match_weights=False, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768 # он же hidden_dim - внутрення размерность модели\n",
    "    debug: bool = True\n",
    "    debug_match_weights: bool = False\n",
    "    layer_norm_eps: float = 1e-5 \n",
    "    d_vocab: int = 50257 # он же vocab_size, размер словаря модели\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024 # число позиционных эмбеддингов\n",
    "    d_head: int = 64 # размерность головы аттеншена\n",
    "    d_mlp: int = 3072 # внутренняя размерность FFN-слоя\n",
    "    n_heads: int = 12 # число голов аттеншена\n",
    "    n_layers: int = 12 # число слоев трансформера\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для генерации тестов, которые мы будем использовать для проверки слоев!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ctCDi2ccPx-H"
   },
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): \n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input, debug_match_weights = False):\n",
    "    cfg = Config(debug=True, debug_match_weights = debug_match_weights)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    print(\"Input shape:\", input.shape)\n",
    "    output = layer(input)\n",
    "    if isinstance(output, tuple): \n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    try: \n",
    "        reference_output = gpt2_layer(input)\n",
    "    except: \n",
    "        reference_output = gpt2_layer(input, input, input)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSNvnHYaQrqZ",
    "outputId": "8f6eca33-9869-4324-e355-8e1bc3d3002a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]])\n",
      "torch.Size([1, 35])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iN1wz9mUNTY",
    "outputId": "7fc249d2-88e3-4238-e851-c5e71d36f086",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n",
      "Все работает, мы готовы к выполнению задания!\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "print(logits.shape)\n",
    "\n",
    "print(\"Все работает, мы готовы к выполнению задания!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура Transformer - 40 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kIRmiHZ87sw"
   },
   "source": [
    "# Embeddings - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь нам даются токены размерности `[batch_size, seq_len]` - индексы слов в словаре. Нужно описать слой Embed, который будет отображать каждый токен в соответствующий вектор из матрицы эмбеддингов. Таким образом каждому токену предоставляется вектор, который будет иметь размерности `[batch_size, seq_len, d_model]`\n",
    "\n",
    "Внимание - здесь не нужно исользовать цикл for и проходиться по матрице. Все стандартные операции доступны в [документации](https://pytorch.org/docs/stable/nn.functional.html), в частности тут нам понадобится одна из операций в секции [sparse functions](https://pytorch.org/docs/stable/nn.functional.html#sparse-functions).\n",
    "\n",
    "Важное замечание - на самом деле этот слой уже есть [готовый в pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), но мы в учебных целях переписываем его сами.\n",
    "\n",
    "\n",
    "Также можно решить этот пример через индексацию или через einops.\n",
    "\n",
    "**Вообще почти во всех примерах есть несколько возможных стилей описания операций над тензорами - через torch.nn.functional, через различные индексации и трюки pytorch, через einops - можно делать любым удобным способом!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srX7Xj8FaMHX",
    "outputId": "3387370e-0710-4c15-b76d-091dc7ef34a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        return self.W_E[input_ids]\n",
    "    \n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "rand_int_test(Embed, [batch_size, seq_len])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embeddings - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В трансформерах есть не только обычные эмбеддинги, которые отвечают за \"смысл\" токенов, но и позиционные эмбеддинги! Вход у них такой же, как и у обычных эмбеддингов, только они должны эмбеддить позиции токенов, а не сами токены. Т.е. в матрице W_pos хранятся не эмбеддинги токенов, а эмбеддинги позиций.\n",
    "\n",
    "Поэтому в этом слое нужно:\n",
    "1. По tokens получить тензор positions размера `[batch_size, seq_len]`\n",
    "2. Заэмбеддить тензор positions, как в предыдущем слое.\n",
    "\n",
    "Важно - как и в предыдущем случае, для этот слой обычно используется через [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "\n",
    "Вспомним еще про то, откуда берутся позиционные эмбеддинги: в оригинальном трансформере позиционные эмбеддинги состояли из синусов и косинусов (см. пункт 3.5 из оригинальной статьи https://arxiv.org/pdf/1706.03762), однако позиционные эмбеддинги можно учить и с нуля, как и обычные эмбеддинги. **В рамках данного задания не нужно никак дополнительно инициализировать веса, только применить позиционные эмбеддинги**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trB2m2P8Rgrk",
    "outputId": "1405f0c5-586f-42aa-eb77-6ada974b74e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # !вопрос! Тут нужно более эффективно делать? Каждый раз тензор для индексирования создавать\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        index_tensor = torch.arange(seq_len).repeat(batch_size, 1)\n",
    "\n",
    "        return self.W_pos[index_tensor]\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "rand_int_test(PosEmbed, [batch_size, seq_len])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM head - 5 баллов\n",
    "\n",
    "Финальный слой. У нас есть выходы из трансформера размерности `[batch_size, seq_len, d_model]`. Это контекстуализированные представления каждого токена. По ним мы предсказываем следующий токен, т.е. применяем линейный слой - умножаем на матрицу `[d_model, vocab_size]`.\n",
    "\n",
    "В этом нам поможет секция [linear functions](https://pytorch.org/docs/stable/nn.functional.html#linear-functions). Не забудьте про bias!\n",
    "\n",
    "В pytorch этот слой тоже есть - [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLXcAsSMU58C",
    "outputId": "f12c9d13-f3bf-477b-bdd0-a40a3ccad998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LM_head, но для совместимости с библиотекой для проверки пришлось назвать его Unembed\n",
    "# по аналогии с тем, что мы из индексов в словаре получаем эмбеддинги, а тут из эмбеддингов обратно\n",
    "# распределение по словарю\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        # using functional\n",
    "        # result = nn.functional.linear(x, self.W_U.T, bias=self.b_U)\n",
    "        \n",
    "        # or manually\n",
    "        # !вопрос! нет разницы какой вариант использовать - матричное умножение или linear?\n",
    "        result = x @ self.W_U + self.b_U\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 768\n",
    "rand_float_test(Unembed, [batch_size, seq_len, d_model])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfxZG2l7A-wQ"
   },
   "source": [
    "# Attention-формулы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30Q7cmNtFudx"
   },
   "source": [
    "1. **Входные эмбеддинги**:\n",
    "   $$X \\in \\mathbb{R}^{seq \\times d} $$\n",
    "2. **Маскированный мультихед-аттеншен (Masked Multi-Head Attention)**:\n",
    "$$M = \\begin{cases}\n",
    " &  m_{ij} = -\\infty, \\quad i < j \\\\\n",
    " &  m_{ij} = 0\n",
    "\\end{cases} $$\n",
    "\n",
    "$$\n",
    "M = \\begin{pmatrix}\n",
    "0 & -\\infty & -\\infty & \\ldots & -\\infty \\\\\n",
    "0 & 0 & -\\infty & \\ldots & -\\infty \\\\\n",
    "0 & 0 & 0 & \\ldots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FwfAHyzQ1Bs"
   },
   "source": [
    "3. Для каждой головы $ h_i $:\n",
    "\n",
    "    3.1 **Матрицы весов для запросов, ключей и значений**:\n",
    "     - $ W_Q \\in \\mathbb{R}^{d \\times d_h} $\n",
    "     - $ W_K \\in \\mathbb{R}^{d \\times d_h} $\n",
    "     - $ W_V \\in \\mathbb{R}^{d \\times d_h} $\n",
    "     \n",
    "    3.2. **Запросы, ключи и значения**:\n",
    "     - $ Q = X W_Q \\in \\mathbb{R}^{seq \\times d_h} $\n",
    "     - $ K = X W_K \\in \\mathbb{R}^{seq \\times d_h} $\n",
    "     - $ V = X W_V \\in \\mathbb{R}^{seq \\times d_h} $\n",
    "\n",
    "    3.3. **Скалярные произведения запросов и ключей**:\n",
    "     - $ \\frac{Q K^T}{\\sqrt{d_h}} + M \\in \\mathbb{R}^{seq \\times seq} $\n",
    "\n",
    "    3.4. **Веса внимания**:\n",
    "     - $ \\alpha = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_h}} + M\\right) \\in \\mathbb{R}^{seq \\times seq} $\n",
    "\n",
    "    3.5. **Агрегация значений**:\n",
    "     - $ z = \\alpha V \\in \\mathbb{R}^{seq \\times d_h} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEuVgMeMR6ce"
   },
   "source": [
    "4. **Конкатенация выходов всех голов**:\n",
    "   - $ Z = \\text{Concat}(z_1, z_2, \\ldots, z_h) \\in \\mathbb{R}^{seq \\times d} $\n",
    "\n",
    "5. **Выходной линейный слой**:\n",
    "   - Матрица весов: $ W^O \\in \\mathbb{R}^{d \\times d} $\n",
    "   - Итоговый выход: $ O = Z W^O + X \\in \\mathbb{R}^{seq \\times d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention - детали реализации\n",
    "Самое сложное в этом домашнем задании - подсчет механизма внимания. Как и в предыдущих вариантах, считать можно через torch или с помощью einops и любыми другими удобными способами.\n",
    "\n",
    "\n",
    "В данном задании нужно реализовать multihead attention с маскированием. Давайте разбираться по шагам, что нам нужно сделать.\n",
    "\n",
    "Далее будет описан один из возможных алогритмов написания аттеншена, но повторимся - писать можно любым удобным способом (голый torch или einops).\n",
    "\n",
    "1. Нам попадает на вход вектор x `[batch, seq_len, d_model]`. Нужно превратить его в матрицы проекций i-й головы аттеншена: Q_i, K_i, V_i. Для этого у нас есть матрицы W_Q, W_K, W_V (и их bias!). Это набор n_heads матриц размеров `[d_model, d_head]`. Зачастую число голов n_head и d_head подобраны так, что d_model == n_head * d_head, наш случай не исключение. Предлагается перевести (этот шаг сделан) матрицу `[num_heads, d_model, d_head]` в матрицу `[d_model, num_heads * d_head]` = `[d_model, d_model]`, после чего получить через матричное умножение на X размерности `[batch_size, seq_len, d_model]` получить матрицы Q, K, V размерностей `[batch_size, seq_len, d_model] = [batch_size, seq_len, num_heads * d_head]` и преобразовать их к виду `[batch_size, seq_len, num_heads, d_head]`. Не забудьте при матричном умножении транспонировать матрицы W_Q, W_K, W_V, если пойдете этим путем! В качестве шпаргалки посмотрите, как происходило умножение в lm_head!\n",
    "\n",
    "2. После этого можно сделать первый шаг и посчитать attention_scores, т.е. домножить $Q \\times K^T$. Тут нам поможет .transpose или .permute вместе с torch.matmul. Нужно переставить размерности матриц таким образом, чтобы финальное матричное умножение происходило по двум последним размерностям `[seq_len, d_head]` на `[d_head, seq_len]`, а все предыдущие размерности `[batch_size, num_heads]` совпадали\n",
    "\n",
    "\n",
    "3. Не забудем нормализацию, т.е. делим attention_scores на sqrt(d_head)\n",
    "\n",
    "4. Теперь нужно исползьовать маскирование! В данных заданиях предполагается, что у нас нет паддингов, поэтому нам нужно наложить маску с одним простым условием: i-й элемент не может смотреть на j-й элемент, если j > i. Это треугольная маска, с ней нам поможет приведение треугольной форме, которое вам предлагается найти в pytorch! Замаскированные значения нужно заполнить каким-нибудь большим по модулю отрицательным числом В классе уже определено значение IGNORE, можно использовать его. Для этого реализуйте и используйте функцию `apply_causal_mask`. Заполнять значениями можно через индексацию, например через `torch.masked_fill`.\n",
    "\n",
    "5. Теперь к замаскированным attention_scores `[batch_size, num_heads, seq_len, seq_len]` нужно применить softmax. Подумайте, по какой размерности его применять и на что это повлияет.\n",
    "\n",
    "6. После этого остается последнее матричное умножение softmax(attention_scores) на V, к которому тоже придется применить .view, .permute и torch.matmul\n",
    "\n",
    "7. Теперь, если вы следовали этому плану у вас остается матрица `output` размерностей `[batch_size, num_heads, seq_len, d_head]`. С помощью permute и view собираем (конкатенируем) ее обратно в матрицу `[batch_size, seq_len, num_heads * d_head] = [batch_size, seq_len, d_model]` и применяем к ней выходной линейный слой W_O. Всё, аттеншен готов!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging attention intermediate steps\n",
    "debug_attention_weights_loaded = False\n",
    "\n",
    "try:\n",
    "    import json\n",
    "\n",
    "    with open(\"../data/hw03/attention_tensors.json\", \"r\") as f:\n",
    "        debug_attention_data = json.load(f)\n",
    "\n",
    "    for k, v in debug_attention_data.items():\n",
    "        debug_attention_data[k] = torch.tensor(v)\n",
    "\n",
    "    debug_attention_weights_loaded = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65yvUiIvTbTk",
    "outputId": "65effb74-04d4-4e74-b7b5-a961075de222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Q match: True\n",
      "K match: True\n",
      "V match: True\n",
      "QK^T match: True\n",
      "QK^T/sqrt(d_head) match: True\n",
      "masked match: True\n",
      "softmax match: True\n",
      "softmaxV match: True\n",
      "softmaxV W_O match: True\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        \n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        \n",
    "        # Берем размерности\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        num_heads = self.cfg.n_heads\n",
    "        d_head = self.cfg.d_head\n",
    "        \n",
    "        # 1. Трансформируем матрицы проекций в формат [d_model, d_model]\n",
    "        W_Q = self.W_Q.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        W_K = self.W_K.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        W_V = self.W_V.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        \n",
    "        b_Q = self.b_Q.view(-1)\n",
    "        b_K = self.b_K.view(-1)\n",
    "        b_V = self.b_V.view(-1)\n",
    "        \n",
    "        # 1. получаем проекции  Q, K, V\n",
    "        Q = x @ W_Q + b_Q\n",
    "        K = x @ W_K + b_K\n",
    "        V = x @ W_V + b_V\n",
    "\n",
    "        self._check_weight_correctness(\"Q\", Q)\n",
    "        self._check_weight_correctness(\"K\", K)\n",
    "        self._check_weight_correctness(\"V\", V)\n",
    "        \n",
    "        # 2. Q x K^T\n",
    "        # !вопрос! \n",
    "        # надеюсь не напутал с порадком измерений во view, следовал следующей логике:\n",
    "        # При получении K/Q/V мы получаем тензор [batch seq_len d_model] = [batch seq_len num_heads d_head] как в шаге 1\n",
    "        # Так что использую тот же порядок\n",
    "        Q_prepared = Q.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)      # batch num_heads seq_len d_head\n",
    "        KT_prepared = K.view(batch_size, seq_len, num_heads, d_head).permute(0, 2, 3, 1) # batch num_heads d_head seq_len\n",
    "        \n",
    "        attn_scores = Q_prepared @ KT_prepared # batch num_heads seq_len seq_len\n",
    "        self._check_weight_correctness(\"QK^T\", attn_scores)\n",
    "        \n",
    "        # 3. Нормализация\n",
    "        attn_scores /= d_head ** 0.5\n",
    "        self._check_weight_correctness(\"QK^T/sqrt(d_head)\", attn_scores)\n",
    "        \n",
    "        # 4. Маскирование\n",
    "        attn_scores_masked = self.apply_causal_mask(attn_scores) # batch num_heads seq_len seq_len\n",
    "        self._check_weight_correctness(\"masked\", attn_scores_masked)\n",
    "\n",
    "        # 5. Softmax\n",
    "        attn_scores_normalized = nn.functional.softmax(attn_scores_masked, dim=3) # batch num_heads seq_len seq_len\n",
    "        self._check_weight_correctness(\"softmax\", attn_scores_normalized)\n",
    "\n",
    "        # 6. Финальная проекция\n",
    "        V_prepared = V.view(batch_size, seq_len, num_heads, d_head).permute(0, 2, 1, 3) # batch num_heads seq_len d_head\n",
    "        attn_scores_mult_by_V = attn_scores_normalized @ V_prepared # batch num_heads seq_len d_head\n",
    "        self._check_weight_correctness(\"softmaxV\", attn_scores_mult_by_V)\n",
    "        \n",
    "        linear_prepared = attn_scores_mult_by_V.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model) # [batch, num_heads, seq_len, d_head] -> [batch_size, seq_len, num_heads * d_head] = [batch_size, seq_len, d_model]\n",
    "        result = linear_prepared @ self.W_O.view(d_model, d_model) + self.b_O # [batch, seq_len, d_model]\n",
    "        self._check_weight_correctness(\"softmaxV W_O\", result) \n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _check_weight_correctness(self, param_name: str, data: Tensor) -> None:\n",
    "        if debug_attention_weights_loaded and self.cfg.debug_match_weights:\n",
    "            print(f\"{param_name} match:\", t.allclose(data.view(debug_attention_data[param_name].shape), debug_attention_data[param_name], atol=1e-4, rtol=1e-3))        \n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads seq_len seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads seq_len seq_len\"]:\n",
    "        '''\n",
    "        Используем треугольную маску, чтобы не смотреть в будущее, паддингов нет\n",
    "        В качестве масикировочного значения перед софтмаксом можно использовать self.IGNORE (-inf)\n",
    "        '''\n",
    "        seq_len = attn_scores.shape[-1]\n",
    "        mask_shape = (seq_len, seq_len)\n",
    "\n",
    "        mask = t.tril(t.ones(mask_shape, dtype=t.long))\n",
    "\n",
    "        masked_attn_scored = torch.masked_fill(attn_scores, mask == 0, value=self.IGNORE)\n",
    "        return masked_attn_scored\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 768\n",
    "rand_float_test(Attention, [batch_size, seq_len, d_model])\n",
    "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"], debug_match_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы справились с этим, то поздравляю - ничего сложнее мы сегодня уже не будем делать)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anGg23a4_aTy"
   },
   "source": [
    "# MLP (или FFN в других терминологиях) - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRLuXGJb6NT2"
   },
   "source": [
    "Реализуем MLP слой - это 2 матричных умножения с нелинейностью GELU.\n",
    "\n",
    "- $$ \\text{MLP}(X) = (\\text{GeLU}(X W_1 + b_1)) W_2 + b_2 \\in \\mathbb{R}^{\\text{seq} \\times d}$$\n",
    "-    $$W_1 \\in \\mathbb{R}^{d \\times d_{mlp}}, \\quad b_1 \\in \\mathbb{R}^{d_{mlp}} \\\\\n",
    "W_2 \\in \\mathbb{R}^{d_{mlp} \\times d}, \\quad b_2 \\in \\mathbb{R}^{d} \\\\ $$\n",
    "\n",
    "\n",
    "$$GELU(X) = 0.5 * x * (1 + tanh(\\sqrt {\\frac {2} {\\pi}} * (x + 0.44715 * x^3)))$$\n",
    "\n",
    "если будете использовать gelu из pytorch, то **обязательно** проставьте approximate=\"tanh\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hQCIDevT0h3",
    "outputId": "069564c9-b46c-43ea-e4e3-a37162a03882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        linear_1 = x @ self.W_in + self.b_in\n",
    "        non_linear = nn.functional.gelu(linear_1, approximate=\"tanh\")\n",
    "        linear_2 = non_linear @ self.W_out + self.b_out\n",
    "\n",
    "        return linear_2\n",
    "        \n",
    "torch.manual_seed(1)\n",
    "\n",
    "rand_float_test(MLP, [batch_size, seq_len, d_model])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVAp7w1gR8tU"
   },
   "source": [
    "# Normalization - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aM9JUorIwwj"
   },
   "source": [
    "**Layer Normalization**:\n",
    "   - $ \\text{LayerNorm}(X) = \\frac{X - \\mu}{\\sigma} \\cdot \\gamma + \\beta $\n",
    "   - $\\mu = \\text{mean}(X, \\text{dim}=-1) \\in \\mathbb{R}^{d}$\n",
    "   - $\\sigma = \\sqrt{\\text{var}(X, \\text{dim}=-1) + \\epsilon} \\in \\mathbb{R}^{d}$\n",
    "   - $\\gamma \\in \\mathbb{R}^{d}$\n",
    "   - $\\beta \\in \\mathbb{R}^{d}$\n",
    "   \n",
    "   \n",
    "1. Не забудьте про эпсилон, который хранится в cfg!\n",
    "2. В [подсчете дисперсии](https://pytorch.org/docs/stable/generated/torch.var.html) не используете коррекцию Бесселя! Для этого в зависимости от версии pytorch поставьте `unbiased=False` или `correction=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnNVykkAP49l",
    "outputId": "eeef206b-fd72-4550-827f-f39652652c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model)) # gamma\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model)) # beta\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        var, mean = t.var_mean(x, dim=-1, correction=0, keepdim=True)\n",
    "        eps = cfg.layer_norm_eps\n",
    "\n",
    "        layer_norm = ((x - mean) / (var + eps) ** 0.5) * self.w + self.b\n",
    "\n",
    "        return layer_norm\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9RPPHidUzRB"
   },
   "source": [
    "# Transformer Block - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это блок трансформера, который получает на вход тензор x `[batch_size, seq_len, d_model]` и выдает тензор таких же размерностей. Блок GPT2 немного отличается от классического трансформера, который мы изучали на лекции.\n",
    "\n",
    "\n",
    "![image.png](https://camo.githubusercontent.com/ebd052b635f156d5d24224f25fa078d804156be51125cd6626b92d9f8b406bbb/68747470733a2f2f6c6f6e6570617469656e742d313235373934353937382e636f732e61702d6368656e6764752e6d7971636c6f75642e636f6d2f53656c656374696f6e5f3030312e706e67)\n",
    "\n",
    "GPT2 следует схеме PreLN, а \"классический\" трансформер схеме PostLN. **Реализовать нужно PreLN схему!**\n",
    "\n",
    "В PostLN схеме нормализация происходит после слоев attention и MLP, а в PreLN до них согласно иллюстрации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7fEX6-8VRjV",
    "outputId": "49b095dc-8583-4f06-b39a-5ee08d8475c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        x_ln1 = self.ln1.forward(x)\n",
    "        x_attn = self.attn.forward(x_ln1)\n",
    "\n",
    "        x_with_attn = x + x_attn\n",
    "\n",
    "        x_ln2 = self.ln2.forward(x_with_attn)\n",
    "        x_ffn = self.mlp.forward(x_ln2)\n",
    "\n",
    "        x_res = x_with_attn + x_ffn\n",
    "        return x_res\n",
    "\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем все в один большой трансформер.\n",
    "1. Применяем эмбеддинги и позиционные эмбеддинги, складываем результаты\n",
    "2. Прогоняем в цикле через все блоки трансформера\n",
    "3. Применяем финальную нормализацию и lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdbjvO9sVLrk",
    "outputId": "5038a7f1-cb31-4ca6-cf06-961169472c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        token_emb = self.embed.forward(input_ids)\n",
    "        pos_emb = self.pos_embed.forward(input_ids)\n",
    "        emb = token_emb + pos_emb\n",
    "\n",
    "        x = emb\n",
    "        for transformer_block in self.blocks:\n",
    "            x = transformer_block.forward(x)\n",
    "\n",
    "        x_ln = self.ln_final.forward(x)\n",
    "        out_ids = self.unembed(x_ln)\n",
    "\n",
    "        return out_ids\n",
    "\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RcNrSDzgVjQg"
   },
   "outputs": [],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "demo_logits = demo_gpt2(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rb2_gL9uVmPj",
    "outputId": "d2e534f6-f1b0-4c99-a2d7-ef7d781f9b78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DemoTransformer(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkrsjh5SVpl3",
    "outputId": "b6f9b98e-3bbc-456b-a21c-f33c847b98a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyiHpWcvV2mn",
    "outputId": "5cbc4e40-c5ec-4219-f8da-08fbb25b357f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cross entropy loss: 4.5647\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.087911\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "    # !вопрос! В чем слысл использовать log после softmax? Для loss функции cross entropy?\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "232e0abc7e3c4f3a8001ec0f852a3ac2",
      "8efba10344284c60b3cbcf02ff4f61f1",
      "09c2ff62b0df42dda88fc4b8478a37a0",
      "cd20171f0b954a909725512f59272ab7",
      "6a14609670a1434cbb4fb3d008e2bfae",
      "d5dbfdc3d54d470e8e9d9b9f227867cc",
      "025d89a614cb4ed5af31164947e081ff",
      "3cc3f99054de414d89b892615399902f",
      "647473d0568b455e9b72db68fbac9026",
      "439c969b7310428994978ac840a334b2",
      "35c6015817ef444cb5c2ec772f95a684"
     ]
    },
    "id": "jFqu7OlPV685",
    "outputId": "ea78b4f5-c285-4fcc-9bf0-cd3ec943c829"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8bf5f9794e4ecdbc579a91eeeddc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The\n"
     ]
    }
   ],
   "source": [
    "test_string = '''The Total Perspective Vortex derives its picture of the whole Universe on the principle of'''\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
    "    demo_logits = demo_gpt2(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG0zGo4iJFMa"
   },
   "source": [
    "# Сэмплирование - 10 баллов\n",
    "Теперь разберем различные техники сэмплирования. За каждую из функций `apply_temperature`, `apply_frequency_penalty`, `sample_basic`, `sample_top_k`, `sample_top_p` по 2 балла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK5fbWLLWwSS"
   },
   "source": [
    "\n",
    "1. **Temperature Sampling**:\n",
    "   - Применяется первым, поскольку изменение температуры изменяет масштабы логитов перед дальнейшими операциями.\n",
    "\n",
    "2. **Frequency Penalty**:\n",
    "   - Применяется следующим, чтобы учесть частоты токенов до того, как логиты будут обрезаны методами top-k или top-p.\n",
    "\n",
    "3. **Top-k Sampling**:\n",
    "   - Применяется после temperature sampling и frequency penalty, так как он отбирает фиксированное количество наиболее вероятных токенов.\n",
    "\n",
    "4. **Top-p (Nucleus Sampling)**:\n",
    "   - Применяется после top-k sampling, чтобы отфильтровать токены на основе совокупной вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgdKJfPfbR-J"
   },
   "source": [
    "Обозначим размер словаря для удобства $\\Sigma = vocab\\_size$\n",
    "\n",
    "Пусть $ \\text{logits} \\in \\mathbb{R}^{\\text{seq} \\times \\Sigma} $:\n",
    "\n",
    "1. **Temperature Sampling**:\n",
    "   $$\n",
    "   \\text{logits}'_{i,j} = \\frac{\\text{logits}_{i,j}}{T} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, |\\Sigma|]\n",
    "   $$\n",
    "\n",
    "2. **Frequency Penalty**:\n",
    "   $$\n",
    "   \\text{penalty}(t_j) = 1 + \\alpha \\cdot f(t_j) \\\\\n",
    "   \\text{logits}''_{i,j} = \\text{logits}'_{i,j} - \\text{penalty}(t_j) \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
    "%    \\text{logits}''_{i,j} = \\frac{\\text{logits}'_{i,j}}{\\text{penalty}(t_j)} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
    "   $$\n",
    "\n",
    "3. **Top-k Sampling**:\n",
    "   $$\n",
    "   top\\_k\\_indices_i = \\text{argtop-k}(\\text{logits}''_i, k) \\quad \\forall \\ i \\in [1, \\text{seq}] \\\\\n",
    "   \\text{mask}_{i,j} =\n",
    "   \\begin{cases}\n",
    "   1 & \\text{если} \\ j \\in top\\_k\\_indices_i \\\\\n",
    "   0 & \\text{иначе}\n",
    "   \\end{cases} \\\\\n",
    "   \\text{logits}'''_{i,j} = \\text{logits}''_{i,j} \\cdot \\text{mask}_{i,j} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
    "   $$\n",
    "\n",
    "4. **Top-p (Nucleus Sampling)**:\n",
    "   $$\n",
    "   sorted\\_logits_i, sorted\\_indices_i = \\text{sort}(\\text{logits}'''_i, \\text{descending=True}) \\quad ∀ \\ i \\in [1, \\text{seq}] \\\\\n",
    "   probs_i = softmax(sorted\\_logits_i) \\quad \\\\\n",
    "    cumulative\\_probs_{i,j} = \\sum_{k=1}^{j} \\text{probs}_{i,k} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma\n",
    "    \\quad \\forall \\ i \\in [1, \\text{seq}] \\\\\n",
    "   top\\_p\\_mask_{i,j} =\n",
    "   \\begin{cases}\n",
    "   1, & cumulative\\_probs_{i,j} \\leq p \\\\\n",
    "   0 &\n",
    "   \\end{cases} \\\\\n",
    "   \\text{logits}^{\\text{final}}_{i,j} = sorted\\_logits_{i,j} \\cdot top\\_p\\_mask_{i,j} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
    "   $$\n",
    "\n",
    "5. **Softmax**:\n",
    "   $$\n",
    "   \\mathbf{probs}_{i,j} = \\text{softmax}(\\text{logits}^{\\text{final}}_{i,j}) \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, |\\Sigma|] \\\\\n",
    "   \\mathbf{probs}_{i,j} = \\frac{e^{\\text{logits}^{\\text{final}}_{i,j}}}{\\sum_{k=1}^{|\\Sigma|} e^{\\text{logits}^{\\text{final}}_{i,k}}}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KhLvuGw5UpOh"
   },
   "outputs": [],
   "source": [
    "model_cfg = Config()\n",
    "model = DemoTransformer(model_cfg).to(device)\n",
    "model.load_state_dict(reference_gpt2.state_dict(), strict=False) # загружаем веса gpt2\n",
    "\n",
    "tokenizer = reference_gpt2.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "opp-1FmGWgGn"
   },
   "outputs": [],
   "source": [
    "class TransformerSampler:\n",
    "\n",
    "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
    "        self.model = model\n",
    "        self.cfg = model.cfg\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_token_id = self.tokenizer.all_special_ids[0]\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "        '''\n",
    "        Возвращаем сгенерированную строку, включая промпт.\n",
    "        Генерация заканчивается после max_tokens_generated токенов или по генерации EOS.\n",
    "        \n",
    "        kwargs передаются в sample_next_token\n",
    "        '''\n",
    "\n",
    "        # !вопрос! Во время инференса нам нужен только один токен на один инпут в батче, а модель возвращает batch x seq_len x vocab\n",
    "        # то есть вместо предсказания одного токена, предсказываем seq_len векторов. Это можно оптимизировать убрав лишние вычисления?\n",
    "\n",
    "        #token_ids = reference_gpt2.to_tokens(prompt).to(device) # batch x seq_len\n",
    "        token_ids = t.tensor(self.tokenizer.encode(prompt))[None, :] # batch x seq_len\n",
    "        \n",
    "        tokens_generated = 0\n",
    "        while tokens_generated <= max_tokens_generated:\n",
    "            # inference\n",
    "            predicted = self.model.forward(token_ids) # batch x seq_len x vocab\n",
    "\n",
    "            # get next token logits, as batch size == 1 take first batch\n",
    "            next_token_logits = predicted[0, -1] # [vocab] - logits - how likely each of vocab tokens comes next\n",
    "\n",
    "            # sample next token\n",
    "            next_token_id = TransformerSampler.sample_next_token(token_ids[0, :], next_token_logits, **kwargs)\n",
    "            \n",
    "            # handle end of generation token\n",
    "            if next_token_id == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # append generated token to context\n",
    "            token_ids = t.cat((token_ids, torch.tensor([[next_token_id]])), dim=1)\n",
    "            tokens_generated += 1\n",
    "        \n",
    "        # convert tokens to string, taking index 0 as batch_size = 1\n",
    "        result = reference_gpt2.tokenizer.decode(token_ids[0])\n",
    "        \n",
    "        return result.strip()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        temperature=1.0,\n",
    "        top_k=0,\n",
    "        top_p=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "        seed=None\n",
    "    ) -> int:\n",
    "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "        assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            t.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Apply all the specialized sampling methods\n",
    "        if temperature == 0:\n",
    "            return TransformerSampler.greedy_search(logits)\n",
    "        elif temperature != 1.0:\n",
    "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
    "        if frequency_penalty != 0.0:\n",
    "            logits = TransformerSampler.apply_frequency_penalty(input_ids, logits, frequency_penalty)\n",
    "        if top_k > 0:\n",
    "            return TransformerSampler.sample_top_k(logits, top_k)\n",
    "        if top_p > 0.0:\n",
    "            return TransformerSampler.sample_top_p(logits, top_p)\n",
    "        return TransformerSampler.sample_basic(logits)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        '''\n",
    "        Возвращаем самый вероятный токен жадно\n",
    "        '''\n",
    "        out = logits.argmax().item()\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_temperature(logits: Float[Tensor, \"d_vocab\"], temperature: float) -> Float[Tensor, \"d_vocab\"]:\n",
    "        '''\n",
    "        Применяем температуру к логитам\n",
    "        '''\n",
    "        with_temp = logits / temperature\n",
    "\n",
    "        return with_temp\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_frequency_penalty(input_ids: Int[Tensor, \"seq_len\"], logits: Float[Tensor, \"d_vocab\"], freq_penalty: float) -> Float[Tensor, \"d_vocab\"]:\n",
    "        '''\n",
    "        Применяем frequency penalty к логитам\n",
    "        '''\n",
    "        prev_token_ids, prev_token_repeats = input_ids.unique(return_counts=True)\n",
    "\n",
    "        # avoid in-place update\n",
    "        result = logits.clone()\n",
    "        result[prev_token_ids] -= freq_penalty * prev_token_repeats\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        '''\n",
    "        Простое сэмплирование! Тут нам поможет torch.multinomial\n",
    "        '''\n",
    "        probs = nn.functional.softmax(logits, dim=0)\n",
    "\n",
    "        return t.multinomial(probs, 1).item()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
    "        '''\n",
    "        top-k сэмплирование\n",
    "        '''\n",
    "        _, indices = torch.topk(logits, k=k, largest=True)\n",
    "        filtered_logits = logits[indices]\n",
    "        \n",
    "        return TransformerSampler.sample_basic(filtered_logits)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
    "        '''\n",
    "        top_p сэмплирование\n",
    "        '''\n",
    "        probs = nn.functional.softmax(logits, dim=0)\n",
    "        probs_sorted, indexes_of_sorted = t.sort(probs, descending=True)\n",
    "\n",
    "        # get number of tokens to include\n",
    "        cum_sum = t.cumsum(probs_sorted, dim=0)\n",
    "        num_tokens_to_keep = (cum_sum < top_p).sum() + 1\n",
    "        num_tokens_to_keep = max(num_tokens_to_keep, min_tokens_to_keep)\n",
    "\n",
    "        token_ids_to_sample = indexes_of_sorted[:num_tokens_to_keep]\n",
    "        chosen_idx = t.multinomial(probs[token_ids_to_sample], 1).item()\n",
    "\n",
    "        return indexes_of_sorted[chosen_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tear0Nn0WjZY",
    "outputId": "5b8e4387-ebab-4159-9c98-422b949398d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with prompt: 'Jingle bells, jingle bells, jingle all the way'\n",
      "\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\\n'\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour model said: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJingle bells, jingle bells, jingle all the way up to the top of the mountain.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m output \u001b[38;5;241m==\u001b[39m expected\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# у меня получилось\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# output = 'Jingle bells, jingle bells, jingle all the way down to the top of the mountain.' \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "print(f\"Greedy decoding with prompt: {prompt!r}\\n\")\n",
    "\n",
    "output = sampler.sample(prompt, max_tokens_generated=8, temperature=0.0)\n",
    "print(f\"Your model said: {output!r}\\n\")\n",
    "\n",
    "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    "assert output == expected\n",
    "# у меня получилось\n",
    "# output = 'Jingle bells, jingle bells, jingle all the way down to the top of the mountain.' \n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3VYdChTdNLc",
    "outputId": "f39d0a54-e575-499a-c6bd-2998202246ee"
   },
   "outputs": [],
   "source": [
    "logits = t.tensor([1, 2]).log()\n",
    "\n",
    "cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)\n",
    "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
    "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
    "\n",
    "hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)\n",
    "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
    "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0no5kj-dTos",
    "outputId": "afcb8c9c-ddf4-4475-b9b9-1381c8d6e56a"
   },
   "outputs": [],
   "source": [
    "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
    "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\")\n",
    "logits = t.ones(tokenizer.vocab_size)\n",
    "penalized_logits = TransformerSampler.apply_frequency_penalty(input_ids.squeeze(), logits, 2.0)\n",
    "\n",
    "assert penalized_logits[5156].item() == -11, \"Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11\"\n",
    "assert penalized_logits[14801].item() == -5, \"Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5\"\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "15a506f8c441472189f93b06502d22b0",
      "16fd2a6b3de144a78e7b7a5c451dd1f5",
      "97594c410495432b9718c739b53b7cc5",
      "b223b9a25f214653a5c1a067e74fdb94",
      "7eea662ece234f0099af1f76a25cd85a",
      "c4805e8c27844ced8605ad43fcfe1965",
      "fd2aeb43be224a9ca18bd902acf3b63a",
      "20ae1143770f4ea49481a59e43441367",
      "af9e2fba57784568a046620d9fc60c1a",
      "8e338bc0159f4f08ab8dff76448bb3e4",
      "d2d32c02cb9b42e2b4df01032585fd9f"
     ]
    },
    "id": "AjQi5zDMWtTJ",
    "outputId": "65ed6a4a-e13f-44b7-8516-4b0c9b5dda33"
   },
   "outputs": [],
   "source": [
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_10pct = {\n",
    "    \" church\": 0.0648,\n",
    "    \" house\": 0.0367, # These are the two most likely tokens, and add up to >10%\n",
    "}\n",
    "top_10pct_sum = sum(expected_top_10pct.values())\n",
    "\n",
    "observed_freqs = defaultdict(int)\n",
    "\n",
    "N = 10000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_p=0.1)\n",
    "    observed_freqs[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_10pct:\n",
    "    expected_freq = expected_top_10pct[word] / top_10pct_sum\n",
    "    observed_freq = observed_freqs[word] / N\n",
    "    print(f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\")\n",
    "    assert abs(observed_freq - expected_freq) < 0.01, \"Try increasing N if this fails by a small amount.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пометил вопросы тегом !вопрос! в ноутбуке\n",
    "Поправил некоторые опечатки и добавил сравнение весов в attention слое. Самое забавное, что с первого раза получилось сделать все, кроме последнего шага умножения на матрицу (была мелкая ошибка) и пришлось добавлять логгирование чтобы понять где проблема\n",
    "\n",
    "Спасибо за лекцию, прям очень понятно рассказали про трансформер, Дмитрий Калашников отличный лектор"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xnqvoOGqZ2oX"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01679c391541478ab4c1037c9d72a0cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69b08166db214508b144a5d83d813f65",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb9c5cec8df94fd69c7e768c6f6c7368",
      "value": 1042301
     }
    },
    "025d89a614cb4ed5af31164947e081ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04b1bc72372846b0ae4d83520421b185": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06cb382af5c340ca8947830a0fdf3b0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09c2ff62b0df42dda88fc4b8478a37a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cc3f99054de414d89b892615399902f",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_647473d0568b455e9b72db68fbac9026",
      "value": 100
     }
    },
    "0ea6e4509dc14359bb2bb8fc0582279d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e73183d327d44a31ac79fbd9d197bfea",
      "placeholder": "​",
      "style": "IPY_MODEL_ff28d14dea69478bbb056d0b5466f72d",
      "value": " 665/665 [00:00&lt;00:00, 12.6kB/s]"
     }
    },
    "0faaf18fdf644d6eb034744810f06f16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "18704ce82e504b14820ae2ad64c99816": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9da7a4e0811b4af69d9f2ab95720ef71",
       "IPY_MODEL_695bcf9a3acd4f929c5cc01e7606cd75",
       "IPY_MODEL_5eee35d4db464c49b1942a1d564917b6"
      ],
      "layout": "IPY_MODEL_9dc1a00a273b4f229b0678458f961abd"
     }
    },
    "1bfbfb53c0f149a4ac060cd4d4aef104": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "215dd213859547488d189fffbb1a9caf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2247decbf6cd456281e7d1f4e3773ff3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22c8259dc73b4b1086b41ea8a91df943": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "232e0abc7e3c4f3a8001ec0f852a3ac2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8efba10344284c60b3cbcf02ff4f61f1",
       "IPY_MODEL_09c2ff62b0df42dda88fc4b8478a37a0",
       "IPY_MODEL_cd20171f0b954a909725512f59272ab7"
      ],
      "layout": "IPY_MODEL_6a14609670a1434cbb4fb3d008e2bfae"
     }
    },
    "25175593166143a38e5be680b610b53a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bd693deab614db5bc2213278951fc6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32bdaa7649844365ab7b58a3544b6908": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35c6015817ef444cb5c2ec772f95a684": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cc3f99054de414d89b892615399902f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d5493baf4014e54a0c1f42d05d62350": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42f1428814134733a9d7dae52707256c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80a0e22dd9d44adf987e0d1b841bf1d2",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_afcbd2194b0c4ccf8754008564859516",
      "value": 26
     }
    },
    "439c969b7310428994978ac840a334b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45666d125d34436fbbd0cbfa6fcfe0e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46d4984f3d90431d8b0b639ba8900fb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48c086c7c25f42728d4f367c171a5d49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f336f98362eb4fde8dd589faa654379a",
      "placeholder": "​",
      "style": "IPY_MODEL_acca64c6a5ad4ae984b9d864899430ec",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 5.17MB/s]"
     }
    },
    "495fe8e8bfdd43849d2a8270cb111368": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9195052de6849709bb9e89e673d5b0c",
       "IPY_MODEL_01679c391541478ab4c1037c9d72a0cc",
       "IPY_MODEL_48c086c7c25f42728d4f367c171a5d49"
      ],
      "layout": "IPY_MODEL_4db1e0f6b230488a9231fe8a24067c99"
     }
    },
    "4b9e0984643f4de5bb9abafeb3ec2ce4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25175593166143a38e5be680b610b53a",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c80699870fb64527bd52dc1aec9d7bf9",
      "value": 124
     }
    },
    "4db1e0f6b230488a9231fe8a24067c99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "520b09f8db3f419c9a11d63bae3c3f4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52431aa699134bf0a86f6a94d4468ba5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52decc6f4908454fb5f97e879e99e119": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56bfe4ef487e459fb726dce5fe640818": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c9f5868bdfa4183ac041e2c7c3e0aa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6bbcca4c0fbd482fbf07ac69542cdad3",
       "IPY_MODEL_42f1428814134733a9d7dae52707256c",
       "IPY_MODEL_952b2e46ef674fdbb6e72a1e38e335f1"
      ],
      "layout": "IPY_MODEL_2bd693deab614db5bc2213278951fc6f"
     }
    },
    "5eee35d4db464c49b1942a1d564917b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0eb8208ac7b4d518c8d4d69f7c4f54f",
      "placeholder": "​",
      "style": "IPY_MODEL_f46bcd12670842ed81a03dbec6922d25",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 5.20MB/s]"
     }
    },
    "647473d0568b455e9b72db68fbac9026": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66c082e996ec47bead802a56531f1c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e5bb2fc59eb46bb93ccf006b01743ad",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04b1bc72372846b0ae4d83520421b185",
      "value": 456318
     }
    },
    "695bcf9a3acd4f929c5cc01e7606cd75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45666d125d34436fbbd0cbfa6fcfe0e3",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22c8259dc73b4b1086b41ea8a91df943",
      "value": 1355256
     }
    },
    "69b08166db214508b144a5d83d813f65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a14609670a1434cbb4fb3d008e2bfae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbcca4c0fbd482fbf07ac69542cdad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52431aa699134bf0a86f6a94d4468ba5",
      "placeholder": "​",
      "style": "IPY_MODEL_ae57136539d54b7b9715b871bb1b2373",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "71481143751245e88117717cdfa5fc32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_215dd213859547488d189fffbb1a9caf",
      "max": 548105171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0faaf18fdf644d6eb034744810f06f16",
      "value": 548105171
     }
    },
    "74561d5cdd5846fa851b92a39bf6f74c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d38e6d822c9c4405bb8e6dea40b0c1db",
       "IPY_MODEL_66c082e996ec47bead802a56531f1c6b",
       "IPY_MODEL_e218c564f9924f1da13f383d7802ab94"
      ],
      "layout": "IPY_MODEL_dcb36c7d2b4e41bbb54ba30b983862b6"
     }
    },
    "77745b9f81bd4536a735223fe5576f8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2247decbf6cd456281e7d1f4e3773ff3",
      "placeholder": "​",
      "style": "IPY_MODEL_fd904cda52ef4cf68a0cd041ecc65778",
      "value": "config.json: 100%"
     }
    },
    "7ae5f36ecd2a415ba14e28adc73e22dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cceb35b8e5cd4aa0a59f707219b21fc2",
       "IPY_MODEL_71481143751245e88117717cdfa5fc32",
       "IPY_MODEL_cbfcc250548e4b3c91664737a0a51b19"
      ],
      "layout": "IPY_MODEL_56bfe4ef487e459fb726dce5fe640818"
     }
    },
    "80a0e22dd9d44adf987e0d1b841bf1d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82e19fdc343d486b92f6566f2fee44db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88085ba590f34bd593d252143d4b8369": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bbbe9f9f3f444adae68756ee6392004": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8efba10344284c60b3cbcf02ff4f61f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5dbfdc3d54d470e8e9d9b9f227867cc",
      "placeholder": "​",
      "style": "IPY_MODEL_025d89a614cb4ed5af31164947e081ff",
      "value": "100%"
     }
    },
    "952b2e46ef674fdbb6e72a1e38e335f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7f8f3433d934bff96fcece7b345363d",
      "placeholder": "​",
      "style": "IPY_MODEL_9c25a21d619e4a6d93216bca13afe45e",
      "value": " 26.0/26.0 [00:00&lt;00:00, 541B/s]"
     }
    },
    "991a58d1f2ef42b5a07cf43d47f50c42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c25a21d619e4a6d93216bca13afe45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9da7a4e0811b4af69d9f2ab95720ef71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46d4984f3d90431d8b0b639ba8900fb3",
      "placeholder": "​",
      "style": "IPY_MODEL_c140a813a05a4f1383da579db95bdcb0",
      "value": "tokenizer.json: 100%"
     }
    },
    "9dc1a00a273b4f229b0678458f961abd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e5bb2fc59eb46bb93ccf006b01743ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2259801c249461f98dd85959ec728aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a9195052de6849709bb9e89e673d5b0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eed87c1ad6034bc2a07f37a63942eb3e",
      "placeholder": "​",
      "style": "IPY_MODEL_520b09f8db3f419c9a11d63bae3c3f4b",
      "value": "vocab.json: 100%"
     }
    },
    "ac49d95d3a7f4e45a1fa13a1f2e2bf5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acca64c6a5ad4ae984b9d864899430ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae57136539d54b7b9715b871bb1b2373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afb1e8d11fe84284baa9adb8a334239c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afcbd2194b0c4ccf8754008564859516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf38836fc6bc4a4089b6e6120cf7c514": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82e19fdc343d486b92f6566f2fee44db",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a2259801c249461f98dd85959ec728aa",
      "value": 665
     }
    },
    "bff8fd936bbe4ec6b7213dfdc9d4e719": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd156c5a79824cbbb14a1c4fa1b7f669",
       "IPY_MODEL_4b9e0984643f4de5bb9abafeb3ec2ce4",
       "IPY_MODEL_da3cd1c51d784c3bb85f5bc8895f91c0"
      ],
      "layout": "IPY_MODEL_1bfbfb53c0f149a4ac060cd4d4aef104"
     }
    },
    "c140a813a05a4f1383da579db95bdcb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c80699870fb64527bd52dc1aec9d7bf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cb9c5cec8df94fd69c7e768c6f6c7368": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cbfcc250548e4b3c91664737a0a51b19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d5493baf4014e54a0c1f42d05d62350",
      "placeholder": "​",
      "style": "IPY_MODEL_ac49d95d3a7f4e45a1fa13a1f2e2bf5c",
      "value": " 548M/548M [00:03&lt;00:00, 170MB/s]"
     }
    },
    "cceb35b8e5cd4aa0a59f707219b21fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06cb382af5c340ca8947830a0fdf3b0f",
      "placeholder": "​",
      "style": "IPY_MODEL_8bbbe9f9f3f444adae68756ee6392004",
      "value": "model.safetensors: 100%"
     }
    },
    "cd20171f0b954a909725512f59272ab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_439c969b7310428994978ac840a334b2",
      "placeholder": "​",
      "style": "IPY_MODEL_35c6015817ef444cb5c2ec772f95a684",
      "value": " 100/100 [00:06&lt;00:00, 18.81it/s]"
     }
    },
    "cda6ef6083474717955a3dd799478ed3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d38e6d822c9c4405bb8e6dea40b0c1db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_991a58d1f2ef42b5a07cf43d47f50c42",
      "placeholder": "​",
      "style": "IPY_MODEL_88085ba590f34bd593d252143d4b8369",
      "value": "merges.txt: 100%"
     }
    },
    "d5dbfdc3d54d470e8e9d9b9f227867cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7f8f3433d934bff96fcece7b345363d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da3cd1c51d784c3bb85f5bc8895f91c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32bdaa7649844365ab7b58a3544b6908",
      "placeholder": "​",
      "style": "IPY_MODEL_f8293198a7b14853995a15ec1562ae6e",
      "value": " 124/124 [00:00&lt;00:00, 2.65kB/s]"
     }
    },
    "dcb36c7d2b4e41bbb54ba30b983862b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd156c5a79824cbbb14a1c4fa1b7f669": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52decc6f4908454fb5f97e879e99e119",
      "placeholder": "​",
      "style": "IPY_MODEL_cda6ef6083474717955a3dd799478ed3",
      "value": "generation_config.json: 100%"
     }
    },
    "e218c564f9924f1da13f383d7802ab94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e801d84d9ad643c5b7a59cc493ec9f65",
      "placeholder": "​",
      "style": "IPY_MODEL_afb1e8d11fe84284baa9adb8a334239c",
      "value": " 456k/456k [00:00&lt;00:00, 2.34MB/s]"
     }
    },
    "e73183d327d44a31ac79fbd9d197bfea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e801d84d9ad643c5b7a59cc493ec9f65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eed87c1ad6034bc2a07f37a63942eb3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0eb8208ac7b4d518c8d4d69f7c4f54f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f189563e5e2d48f1a0e5ae34f803b80e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f336f98362eb4fde8dd589faa654379a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46bcd12670842ed81a03dbec6922d25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4b70daf7cf54dd298ee036a3dfdc0ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77745b9f81bd4536a735223fe5576f8c",
       "IPY_MODEL_bf38836fc6bc4a4089b6e6120cf7c514",
       "IPY_MODEL_0ea6e4509dc14359bb2bb8fc0582279d"
      ],
      "layout": "IPY_MODEL_f189563e5e2d48f1a0e5ae34f803b80e"
     }
    },
    "f8293198a7b14853995a15ec1562ae6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd904cda52ef4cf68a0cd041ecc65778": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff28d14dea69478bbb056d0b5466f72d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
