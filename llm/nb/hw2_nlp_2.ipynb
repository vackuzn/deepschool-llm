{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b06ba3f-3c98-4fd7-948d-362d3260fc5d",
   "metadata": {},
   "source": [
    "# Введение в NLP, часть 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09171b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (24.3.1)\n",
      "Requirement already satisfied: transformers in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (4.47.0)\n",
      "Requirement already satisfied: datasets in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: torch in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: seqeval in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: evaluate in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from seqeval) (1.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install transformers datasets torch seqeval evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3154e",
   "metadata": {},
   "source": [
    "\n",
    "## NER c BERT (25 баллов)\n",
    "\n",
    "1. Взять датасет из предыдущего ДЗ и обучить на нём BERT.\n",
    "2. Обучить BERT на подготовленном датасете\n",
    "3. Оценить результат, сравнить с моделью из первого ДЗ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a696ec-f1af-4779-bb81-a4aa644fddfd",
   "metadata": {},
   "source": [
    "### Подготовка данных (5 баллов)\n",
    "\n",
    "Подумать о:\n",
    "1) Как subword токенизация повлияет на BIO раззметку?\n",
    "2) Что делать с `[CLS]` и `[SEP]` токенами? (Проверьте что использует `DataCollatorForTokenClassification`)\n",
    "\n",
    "> Hint! Токенайзер умеет работать с предразделёнными на \"слова\" текстами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5e633a-23c1-4b54-9529-c1f32cb5ff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "\n",
    "BASE_NER_MODEL = \"bert-base-cased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BASE_NER_MODEL)\n",
    "\n",
    "conll2003 = load_dataset(\"conll2003\")\n",
    "conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a1ac5b-0722-4387-a885-80b927fbc10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '100',\n",
       " 'tokens': ['Rabinovich',\n",
       "  'is',\n",
       "  'winding',\n",
       "  'up',\n",
       "  'his',\n",
       "  'term',\n",
       "  'as',\n",
       "  'ambassador',\n",
       "  '.'],\n",
       " 'pos_tags': [21, 42, 39, 33, 29, 21, 15, 21, 7],\n",
       " 'chunk_tags': [11, 21, 22, 15, 11, 12, 13, 11, 0],\n",
       " 'ner_tags': [1, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = conll2003[\"train\"][100]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c070226-e70f-4df1-b770-f403a19ef205",
   "metadata": {},
   "source": [
    "* tokens - исходные токены, для которых была сделана NER-разметка\n",
    "* ner_tags - векторизированные метки NER-тэгов\n",
    "* pos_tags - разметка частей речи, которую мы игнорируем\n",
    "* chunk_tags - разметка чанков, которую мы игнорируем\n",
    "\n",
    "Обратите внимание, что количество токенов может превышать количество исходных лейблов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d060e0-8b58-4c58-a8b6-9ecd91e31777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Ra',\n",
       " '##bino',\n",
       " '##vich',\n",
       " 'is',\n",
       " 'winding',\n",
       " 'up',\n",
       " 'his',\n",
       " 'term',\n",
       " 'as',\n",
       " 'ambassador',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer(example[\"tokens\"], is_split_into_words=True).tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb99e70-85e2-438f-8c68-e15a6701ccfa",
   "metadata": {},
   "source": [
    "Значение тэга в ner_tags отображается в метку NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8fce68-13e8-4093-a147-15ae865ac73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER TAGS [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"NER TAGS\", example[\"ner_tags\"])\n",
    "print(conll2003[\"train\"].features[\"ner_tags\"].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7e9ac1-89fa-448f-9033-37074fb3d692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальные токены\n",
      "['Rabinovich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.']\n",
      "Векторизированные NER метки токенов\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Текстовые NER метки токенов\n",
      "['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Токены после работы токенайзера BERT\n",
      "['[CLS]', 'Ra', '##bino', '##vich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\"Оригинальные токены\")\n",
    "print(example[\"tokens\"])\n",
    "print(\"Векторизированные NER метки токенов\")\n",
    "print(example[\"ner_tags\"])\n",
    "tags_str = []\n",
    "features = conll2003[\"train\"].features[\"ner_tags\"].feature\n",
    "for tag in example[\"ner_tags\"]:\n",
    "    tags_str.append(features.int2str(tag))\n",
    "print(\"Текстовые NER метки токенов\")\n",
    "print(tags_str)\n",
    "print(\"Токены после работы токенайзера BERT\")\n",
    "print(bert_tokenizer(example[\"tokens\"], is_split_into_words=True).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce9764-c763-44c2-a915-b938b27a9866",
   "metadata": {},
   "source": [
    "Вспомним немного, как работают метки в задаче мер в кодировке BIO. В данной задаче у нас есть 4 типа именованных сущностей:\n",
    "* PER - персона\n",
    "* ORG - организация\n",
    "* LOC - локация\n",
    "* MISC - другое\n",
    "* O - отсутствие именованной сущности\n",
    "\n",
    "У каждого типа именованных 2 префикса:\n",
    "* `B-` - beginning, т.е. начало именованной сущности.\n",
    "* `I-` - inside, т.е. продолжение ранее начатой именованной сущностью.\n",
    "\n",
    "В исходной токенизации\n",
    "\n",
    "`['Rabinovich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.']`\n",
    "метки выглядят как \n",
    "\n",
    "`['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']`\n",
    "т.е. `Rabinovich` является персоной. На следующем токене именованная сущность заканчивается, т.к. у него метка `O`.\n",
    "\n",
    "После токенизации BERT наш сэмпл превращается в следующие токены:\n",
    "\n",
    "`['[CLS]', 'Ra', '##bino', '##vich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.', '[SEP]']`\n",
    "Обратим внимание, что один токен `Rabinovich` с меткой `B-PER` был разбит токенизатором берта на 3 токена: `'Ra', '##bino', '##vich'`. Им нужно поставить в соответствие 3 метки: `B-PER, I-PER, I-PER`, т.е. мы разбиваем метку исходного токена на новые токены.\n",
    "\n",
    "Также обратим внимание на первый и последний токен - это спецстокены BERT означающие начало и конец текста. Им можно дать метки `O`, т.к. они не являются частью исходного текста, но мы будем давать им особое векторизированное значение -100. В [документации pytroch](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) у кроссэнтропийной функции потерь это дефолтное значение `ignore_index`, т.е. метки, которую мы будем игнорировать. Библиотека transformers также использует это значение. Таким образом на токенах, у которых стоит -100 в качестве векторизированного NER-тэга, не будет происходить обучение, они будут проигнорированы.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ac7b9-d65f-4e09-8165-de5442a6ab6e",
   "metadata": {},
   "source": [
    "Напишите функцию `preprocess_ner_dataset`, которая разворачивает `ner_tags` для слов в тэги для BERT-токенов и готовит остальные данные для обучения (можно разделить на две функции или написать всё в одной). В резултате применения `conll2003.map(preprocess_ner_dataset)`, в каждом примере:\n",
    "1. Добавляется токенизированный вход (`input_ids`, `token_type_ids` и `attention_mask`). При конструировании этих векторов вручную нужно проставить `attention_mask` полностью единицами, т.к. в паддинги в сэмплах появляются только в рамках батчей, а `token_type_ids` полностью нулями.\n",
    "2. `ner_tags` разворачивается в `labels` для входных токенов\n",
    "\n",
    "Что можно использовать:\n",
    "* у объекта `conll2003[\"train\"].features[\"ner_tags\"].feature` есть методы `int2str` и `str2int` для превращение векторизованного NER-тэга в строковый вид и обратно\n",
    "* Спецтокенам BERT нужно поставить значение -100\n",
    "* вызов `bert_tokenizer(bert_tokenizer(example[\"tokens\"], is_split_into_words=True)` возвращает вам input_ids, attention_mask, token_type_ids\n",
    "* Вызов `bert_tokenizer(example[\"tokens\"], is_split_into_words=True, return_offsets_mapping=True))` возвращает дополнительно offset_mapping, позиции новых токенов в оригинальном тексте\n",
    "* `bert_tokenizer.vocab` - для превращения токенов в их индексы в словаре\n",
    "* `bert_tokenizer.tokenize` - разбитие текста (в том числе и исходных токенов) на токены BERT\n",
    "\n",
    "Ваша задача:\n",
    "1. Создать новый dict, в котором будут input_ids, attention_mask, token_type_ids\n",
    "2. Добавить в него labels - векторизированные NER-тэги, которые будут разбиты в соответствии с токенизацией BERT. Для этого можно можно разбить каждый токен отдельно и размножить его метки. Альтернативно можно использовать информацию об оффсетах токенов BERT, чтобы понять, частью какого исходного токена и какой исходной метки является данный BERT-токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e3fa5fa-e016-491d-914e-e1ba2f23d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX_TOKEN = -100\n",
    "BERT_CLS_TOKEN = 101\n",
    "BERT_SEP_TOKEN = 102\n",
    "\n",
    "IGNORE_TOKENS = (BERT_CLS_TOKEN, BERT_SEP_TOKEN)\n",
    "\n",
    "\n",
    "def preprocess_ner_dataset(example):  \n",
    "    tokenized = bert_tokenizer(example[\"tokens\"], is_split_into_words=True, return_offsets_mapping=True)\n",
    "    offset_mappings = tokenized[\"offset_mapping\"]\n",
    "\n",
    "    ner_tags = example[\"ner_tags\"]\n",
    "\n",
    "    labels = []\n",
    "    cur_word_idx = -1\n",
    "\n",
    "    while offset_mappings:\n",
    "        o_start, o_end = offset_mappings.pop(0)\n",
    "        \n",
    "        # CLS, SEP token\n",
    "        if o_start == o_end:\n",
    "            labels.append(IGNORE_INDEX_TOKEN)\n",
    "            continue\n",
    "\n",
    "        is_word_first_token = o_start == 0\n",
    "        if is_word_first_token:\n",
    "            cur_word_idx += 1\n",
    "\n",
    "        word_label_id = ner_tags[cur_word_idx]\n",
    "                \n",
    "        # 0 = 'O'\n",
    "        # 1 = 'B-PER'\n",
    "        # 2 = 'I-PER'\n",
    "        # ...\n",
    "        # odd numbers - start of the entity, even - in the middle\n",
    "        token_label_id = (word_label_id + 1) // 2 * 2 - is_word_first_token\n",
    "        token_label_id = max(token_label_id, 0) # handle 0 case\n",
    "        \n",
    "        labels.append(token_label_id)\n",
    "\n",
    "    # prepare result entity\n",
    "    del tokenized[\"offset_mapping\"]\n",
    "    tokenized[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecaf00-45f6-4eb9-a921-31c4756edfcd",
   "metadata": {},
   "source": [
    "Пример получившегося выхода:\n",
    "```python\n",
    ">>> preprocessed_ner_dataset[\"train\"][100]\n",
    "{'id': '100',\n",
    " 'tokens': ['Rabinovich',\n",
    "  'is',\n",
    "  'winding',\n",
    "  'up',\n",
    "  'his',\n",
    "  'term',\n",
    "  'as',\n",
    "  'ambassador',\n",
    "  '.'],\n",
    " 'pos_tags': [21, 42, 39, 33, 29, 21, 15, 21, 7],\n",
    " 'chunk_tags': [11, 21, 22, 15, 11, 12, 13, 11, 0],\n",
    " 'ner_tags': [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    " 'input_ids': [101,\n",
    "  16890,\n",
    "  25473,\n",
    "  11690,\n",
    "  1110,\n",
    "  14042,\n",
    "  1146,\n",
    "  1117,\n",
    "  1858,\n",
    "  1112,\n",
    "  9088,\n",
    "  119,\n",
    "  102],\n",
    " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    " 'labels': [-100, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, -100]}\n",
    "```\n",
    "\n",
    "### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c378788f-38b9-4b80-9a9a-89ab561b5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example = preprocess_ner_dataset(example)\n",
    "required_keys = [\"input_ids\", \"labels\", \"attention_mask\", \"token_type_ids\"]\n",
    "for k in required_keys:\n",
    "    assert k in processed_example, f\"Отсутствует поле {k}\"\n",
    "\n",
    "required_keys_set = set(required_keys)\n",
    "for k in processed_example.keys():\n",
    "    assert k in required_keys_set, f\"В примере лишнее поле {k}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79adc4dd-791f-4f8d-8799-8904907f78d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 4276.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизация верна!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for idx, example in tqdm(enumerate(conll2003[\"train\"])):\n",
    "    input_ids_real = bert_tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]\n",
    "    input_ids_ours = preprocess_ner_dataset(example)[\"input_ids\"]\n",
    "    assert input_ids_real == input_ids_ours, f\"Ошибка токенизации на примере {idx}\"\n",
    "    if idx >= 100:\n",
    "        break\n",
    "print(\"Токенизация верна!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409ee982-3f01-4519-abd6-20f3f6050562",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = conll2003[\"train\"][100]\n",
    "processed_example = preprocess_ner_dataset(example)\n",
    "\n",
    "assert processed_example[\"labels\"][0] == -100\n",
    "assert processed_example[\"labels\"][-1] == -100\n",
    "ner_tags = [features.int2str(i) for i in processed_example[\"labels\"][1:-1]]\n",
    "assert ner_tags == ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e61ee0b-a4dd-4fba-8e12-a89d57886d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = conll2003[\"train\"][200]\n",
    "processed_example = preprocess_ner_dataset(example)\n",
    "\n",
    "assert processed_example[\"labels\"][0] == -100\n",
    "assert processed_example[\"labels\"][-1] == -100\n",
    "ner_tags = [features.int2str(i) for i in processed_example[\"labels\"][1:-1]]\n",
    "assert ner_tags == ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257320fd-9a53-4bfe-a7fd-8199e05d4f83",
   "metadata": {},
   "source": [
    "Применим нашу функцию к всему датасету:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f871318-7c1b-4a89-b3ae-ef131c9bf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_ner_dataset = conll2003.map(preprocess_ner_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e0a33-dea2-49d5-8b53-f54cd2048454",
   "metadata": {},
   "source": [
    "Подготовим `data_collator`. Это особый класс, который будет заниматься батчеванием сэмплов для обучения. Он добавит паддинги во все необходимые поля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a20511a1-d222-4034-b099-9dfdd02ed81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7c640-ac2b-4388-81b6-9f21c8c2ec2d",
   "metadata": {},
   "source": [
    "### Подготовка модели (5 баллов)\n",
    "\n",
    "Два возможных пути на этой стадии:\n",
    "1. Взять [готовый класс](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelfortokenclassification) модели для классификации токенов. (Этот вариант настоятельно рекомендуется)\n",
    "2) Взять модель как фича экстрактор ([AutoModel](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodel)) и самостоятельно добавить классификационную голову. Вдохновиться можно по [ссылке](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L1847-L1860). Дополнительных баллов второй вариант не принесёт.\n",
    "\n",
    "Результатом должна быть модель, которая для каждого токена возвращает логиты/вероятности для `conll2003[\"train\"].features[\"ner_tags\"].feature.num_classes` классов.\n",
    "\n",
    "> Если выберете вариант номер один, опишите как он работает - как из токена получается его `ner_tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331cd38a-3501-4f41-9193-8cf6579ad80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "ner_feature = conll2003[\"train\"].features[\"ner_tags\"].feature\n",
    "label_names = ner_feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(BASE_NER_MODEL, num_labels=ner_feature.num_classes)\n",
    "\n",
    "## TODO: explain how token is transformed to NER tag (logits, ?)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6bbb82",
   "metadata": {},
   "source": [
    "## How classification head works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca40a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101, 14324,  2003,  1037,  3928,  2944,  2005,  3019,  2653,  6364,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample text\n",
    "text = \"BERT is a powerful model for natural language processing.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,        # Adds [CLS] and [SEP]\n",
    "    max_length=64,                  # Truncate or pad to max length\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'             # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Extract input IDs and attention mask\n",
    "input_ids = encoded_input['input_ids']            # Shape: (1, max_length)\n",
    "attention_mask = encoded_input['attention_mask']  # Shape: (1, max_length)\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a65c076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# Initialize the BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set model to evaluation mode (disable dropout)\n",
    "bert_model.eval()\n",
    "\n",
    "# Forward pass through BERT\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Extract hidden states\n",
    "last_hidden_state = outputs.last_hidden_state  # Shape: (1, max_length, hidden_size)\n",
    "\n",
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a11706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS Embedding Shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Extract the [CLS] token embedding\n",
    "cls_embedding = last_hidden_state[:, 0, :]  # Shape: (1, hidden_size)\n",
    "\n",
    "print(\"CLS Embedding Shape:\", cls_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf129b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[0.3404, 0.3247]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the number of classes\n",
    "num_labels = 2  # Example: Binary classification\n",
    "\n",
    "# Initialize the classification layer\n",
    "classifier = nn.Linear(bert_model.config.hidden_size, num_labels)\n",
    "\n",
    "# Apply the classification layer\n",
    "logits = classifier(cls_embedding)  # Shape: (1, num_labels)\n",
    "\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f120514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: tensor([[0.5039, 0.4961]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# For multi-class classification\n",
    "softmax = nn.Softmax(dim=1)\n",
    "probs = softmax(logits)\n",
    "\n",
    "# For binary classification, you might use Sigmoid instead\n",
    "# sigmoid = nn.Sigmoid()\n",
    "# probs = sigmoid(logits)\n",
    "\n",
    "print(\"Probabilities:\", probs)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c70c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73eaa2ce-0f56-4b8f-8829-ff9edffa0d34",
   "metadata": {},
   "source": [
    "### Подготовим Метрику (5 баллов)\n",
    "\n",
    "Дополните функцию, используя `metrics_calculator`, чтобы она возвращала `accuracy`, `precision`, `recall` и `f-меру`. `eval_predictions` - это кортеж из логитов токен классификатора и `labels`, которые мы подготовили с помощью `preprocess_ner_dataset`. Нужно:\n",
    "1. Преобразовать логиты в предсказанные лейблы. Учтите, что для специальных токенов лейблов нет\n",
    "2. Посчитать метрики с помощью `metrics_calculator`\n",
    "3. Упаковать резултат в `dict`, в котором ключём будет название метрики, а значением - значение метрики\n",
    "\n",
    "В logits будет лежать тензор размерности \\[размер eval датасета, максимальная длина последовательности, число меток\\], содержащий предсказания модели\n",
    "\n",
    "В target_labels будет лежать тензор размерности \\[размер eval датасета, максимальная длина последовательности\\], содержащий метки из валидационной выборки.\n",
    "\n",
    "Примеры функции calculate_metrics можно посмотреть в [документации](https://huggingface.co/docs/evaluate/en/transformers_integrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55ad18b4-5176-4bf1-ab9a-517466e79f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metrics_calculator = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def calculate_metrics(eval_predictions):\n",
    "    logits, target_labels = eval_predictions\n",
    "    # logits: Float[np.ndarray, \"batch max_seq_len num_labels\"]\n",
    "    # target_labels: Int[np.ndarray, \"batch max_seq_len\"]\n",
    "\n",
    "    predicted_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for seq_pred_ids, seq_labels in zip(predicted_ids, target_labels):\n",
    "        predictions.append([])\n",
    "        references.append([])\n",
    "        for pred_id, label_id in zip(seq_pred_ids, seq_labels):\n",
    "            if label_id == IGNORE_INDEX_TOKEN:\n",
    "                continue\n",
    "\n",
    "            predictions[-1].append(features.int2str(int(pred_id)))\n",
    "            references[-1].append(features.int2str(int(label_id)))\n",
    "   \n",
    "    res = metrics_calculator.compute(predictions=predictions, references=references)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44528ae-e63a-4561-b707-a891f8dcf75f",
   "metadata": {},
   "source": [
    "### Обучение (5 баллов)\n",
    "\n",
    "Два возможных пути на этой стадии:\n",
    "\n",
    "1. Использовать [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) класс из `transformers`\n",
    "2. Написать свой training loop. Дополнительных баллов на этом пути нет.\n",
    "\n",
    "Опишем подробнее первый путь, т.к. он настоятельно рекомендуется.\n",
    "\n",
    "Нужно создать класс Trainer и TrainingArguments.\n",
    "В [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments) нужно как минимум следующие поля:\n",
    "* save_strategy, eval_strategy\n",
    "* metric_for_best_model (исходя из calculate_metrics), greater_is_better\n",
    "* learning_rate (возьмите 2e-5)\n",
    "* num_train_epochs\n",
    "* per_device_train_batch_size, per_device_eval_batch_size\n",
    "\n",
    "В класс Trainer нужно передать:\n",
    "* model\n",
    "* в args нужно передать заполненные TrainingArguments\n",
    "* train_dataset, eval_dataset\n",
    "* tokenizer\n",
    "* compute_metrics\n",
    "\n",
    "После чего запустить `trainer.train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3593874-6c27-464d-a424-603f7c4ab4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb3a033b8554819b2e9a3100b23e57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c173c5e4044bacb8edd4d06611669a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.835191011428833, 'eval_LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 68}, 'eval_MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 39}, 'eval_ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 81}, 'eval_PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 54}, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.8177150192554558, 'eval_runtime': 0.7646, 'eval_samples_per_second': 130.792, 'eval_steps_per_second': 17.003, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb9e41b95244eafb51ee65b670d87f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7193897366523743, 'eval_LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 68}, 'eval_MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 39}, 'eval_ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 81}, 'eval_PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 54}, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.8177150192554558, 'eval_runtime': 0.458, 'eval_samples_per_second': 218.329, 'eval_steps_per_second': 28.383, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74367b49023b48d8bd4bce7fc0c2a60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6628965735435486, 'eval_LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 68}, 'eval_MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 39}, 'eval_ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 81}, 'eval_PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 54}, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.8177150192554558, 'eval_runtime': 0.5008, 'eval_samples_per_second': 199.686, 'eval_steps_per_second': 25.959, 'epoch': 3.0}\n",
      "{'train_runtime': 17.2363, 'train_samples_per_second': 17.405, 'train_steps_per_second': 2.263, 'train_loss': 0.9600462302183493, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "limit_records_to_process: int | None = 100\n",
    "\n",
    "\n",
    "X_train = preprocessed_ner_dataset[\"train\"]\n",
    "X_val = preprocessed_ner_dataset[\"validation\"]\n",
    "\n",
    "if limit_records_to_process:\n",
    "    X_train = X_train.select(range(limit_records_to_process))\n",
    "    X_val = X_train.select(range(limit_records_to_process))\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"../data/hw02/training\", \n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_overall_f1\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3.0,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,        \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    train_dataset=X_train,\n",
    "    eval_dataset=X_val,\n",
    "    compute_metrics=calculate_metrics,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"../data/hw02/saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d049b7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"../data/hw02/saved\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8a152-f724-4c8d-9949-74e346b6a842",
   "metadata": {},
   "source": [
    "### Обработка результатов Результатов (5 баллов)\n",
    "\n",
    "Подумать о:\n",
    "1) Во время подготовки данных мы преобразовали BIO разметку. Как обратить это преобразование с помощью токенайзера?\n",
    "\n",
    "Провалидируйте результаты на тестовом датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10d074b6-49d8-445f-81ee-151165239b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3773726d86af4f7db9d8952d52f32eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.9668671488761902,\n",
       " 'test_LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1925},\n",
       " 'test_MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 918},\n",
       " 'test_ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2496},\n",
       " 'test_PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2773},\n",
       " 'test_overall_precision': 0.0,\n",
       " 'test_overall_recall': 0.0,\n",
       " 'test_overall_f1': 0.0,\n",
       " 'test_overall_accuracy': 0.755188225839492,\n",
       " 'test_runtime': 20.0282,\n",
       " 'test_samples_per_second': 172.407,\n",
       " 'test_steps_per_second': 21.57}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "*_, metrics = trainer.predict(preprocessed_ner_dataset['test'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda7c62-f8eb-4a83-9659-71ef3a8610fb",
   "metadata": {},
   "source": [
    "Напишите функцию, которая принимает на вход текст и отдаёт такой словарь:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text\": \"входной текст\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"class\": \"лейбл класса\",\n",
    "            \"text\": \"текстовое представление\",\n",
    "            \"start\": \"оффсет от начала строки до начала entity\",\n",
    "            \"end\": \"оффсет от начала строки до конца entity\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "Должно выполняться такое условие:\n",
    "\n",
    "```python\n",
    "text[entity[\"start\"]:entity[\"stop\"]] == entity[\"text\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c8f48b4-932d-4525-b8d7-8b57f324520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Mike has left the Microsoft corporation and Johnson and Johnson in year 2004 while living in Bosnia and Herzegovina', 'entities': []}\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def do_ner(text: str) -> dict:\n",
    "    model.eval()\n",
    "    tokenized = bert_tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    offset_mapping = tokenized[\"offset_mapping\"]\n",
    "\n",
    "    logits = model(tokenized.input_ids).logits # batch x seq_len x num_classes\n",
    "    preds = logits.argmax(dim=-1) # batch x seq_len\n",
    "\n",
    "    # assume 1 row in batch\n",
    "    \n",
    "    entities = []\n",
    "    for token_class_id, (start_pos, end_pos) in zip(preds.view(-1).tolist(), offset_mapping.view(-1, 2).tolist()):\n",
    "        if token_class_id == 0:\n",
    "            continue\n",
    "\n",
    "        if token_class_id % 2 == 1:\n",
    "            entity = {\n",
    "                \"class\": features.int2str(token_class_id).split(\"-\")[-1],\n",
    "                \"text\": text[start_pos:end_pos],\n",
    "                \"start\": start_pos,\n",
    "                \"end\": end_pos,\n",
    "            }\n",
    "\n",
    "            entities.append(entity)\n",
    "        else:\n",
    "            entity = entities[-1]\n",
    "            entity[\"end\"] = end_pos\n",
    "            entity[\"text\"] = text[entity[\"start_pos\"]:entity[\"end_pos\"]],\n",
    "\n",
    "    result = {\n",
    "        \"text\": text,\n",
    "        \"entities\": entities,\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "a = do_ner(\"Mike has left the Microsoft corporation and Johnson and Johnson in year 2004 while living in Bosnia and Herzegovina\")\n",
    "print(a)\n",
    "\n",
    "# {\n",
    "# “text”: “Илья читает лекцию для DeepSchool.”,\n",
    "# “entities”: [\n",
    "#     {“class”: “PER”, “text”: “Илья”, “start”: 0, “end”: 5},\n",
    "#     {“class”: “ORG”, “text”: “DeepSchool”, “start”: 23, “end”: 33}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1757ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a06a5cc-44c3-41f9-ba9e-a2a5f5b1e04b",
   "metadata": {},
   "source": [
    "## Классификация с T5 (25 баллов)\n",
    "\n",
    "Требуется дообучить [t5-small](https://huggingface.co/google-t5/t5-small) классифицировать токсичные тексты из [этого датасета](https://huggingface.co/datasets/lmsys/toxic-chat). Классификатор должен работать в стиле t5 - генерировать ответ текстом.\n",
    "\n",
    "1. Подготовить данные для бинарной классификации\n",
    "\t1. Придумать префикс для задачи или взять из похожей модели\n",
    "\t2. Выбрать тексты для обозначения классов\n",
    "2. Обучить t5-small на генерацию выбранных названия классов\n",
    "3. Сравнить с модель с аналогичной предобученной моделью"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536e268-d5c0-461c-bdc1-553e51b1ce66",
   "metadata": {},
   "source": [
    "### Подготовка Данных (6 баллов)\n",
    "\n",
    "Подумать о:\n",
    "1) Какой префикс выбрать для новой задачи?\n",
    "2) Должен ли префикс быть понятным?\n",
    "3) Как выбрать метку для класса? \n",
    "4) Что будет, если метки класса целиком нет в словаре?\n",
    "5) Что делать с длинными текстами?\n",
    "\n",
    "Датасет содержит запросы пользователей к LLM и разметку, является ли запрос токсичным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fb5e0e5-677d-4084-8a68-7241ac05982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "BASE_T5_MODEL= \"t5-small\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(BASE_T5_MODEL)\n",
    "\n",
    "\n",
    "toxic_chat_dataset = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b3aa-4d0a-4b87-bd62-f0c2de297f9f",
   "metadata": {},
   "source": [
    "Место для изучения датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96556315-dfef-4ea3-b965-4fa4191d4d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_id': 'e0c9b3e05414814485dbdcb9a29334d502e59803af9c26df03e9d1de5e7afe67',\n",
       " 'user_input': 'Masturbacja jest proces co oitrzebuje',\n",
       " 'model_output': 'Masturbacja to proces, który może pozytywnie wpłynąć na zdrowie psychiczne i fizyczne człowieka, ponieważ pomaga w relaksie, redukuje stres i pomaga w uśpieniu. Może też być używana jako dodatkowa form',\n",
       " 'human_annotation': True,\n",
       " 'toxicity': 0,\n",
       " 'jailbreaking': 0,\n",
       " 'openai_moderation': '[[\"sexual\", 0.4609803557395935], [\"sexual/minors\", 0.0012527990620583296], [\"harassment\", 0.0001862536446424201], [\"hate\", 0.00015521160094067454], [\"violence\", 6.580814078915864e-05], [\"self-harm\", 3.212967567378655e-05], [\"violence/graphic\", 1.5190824342425913e-05], [\"self-harm/instructions\", 1.0009921425080393e-05], [\"hate/threatening\", 4.4459093260229565e-06], [\"self-harm/intent\", 3.378846486157272e-06], [\"harassment/threatening\", 1.7095695739044459e-06]]'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_chat_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e36fc-d463-42ee-9d38-e0cb88f458e5",
   "metadata": {},
   "source": [
    "Нас будут интересовать колонки `\"user_input\"` и `\"toxicity\"`. Убираем ненужные колонки из датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b54e8e06-c538-4f50-b31c-c99451c1143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_chat_dataset = toxic_chat_dataset.remove_columns(\n",
    "    [\"conv_id\", \"model_output\", \"human_annotation\", \"jailbreaking\", \"openai_moderation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ccfb9-3ddc-4a7a-876f-44dacace228d",
   "metadata": {},
   "source": [
    "![](https://production-media.paperswithcode.com/methods/new_text_to_text.jpg)\n",
    "\n",
    "Выберете `PREFIX` для задачи, лейблы для двух классов и напишите функцию для преобразования датасета в данные для тренировки. Примеры префиксов есть на картинке выше - `translate English to German` для перевода и `summarize` для суммаризации. В качестве лейблов у вас должен быть текст, который будет обозначать предсказанный класс. Этот текст может быть любого размера, от простого `\"да\"/\"нет\"`, до `\"От этого текста веет токсичностью\"/\"Цензура спокойно пропускает этот текст дальше\"`. Подумайте в чём преимущество первого подхода перед вторым.\n",
    "\n",
    "Важно:\n",
    "1) Не забыть добавить префикс перед токенизацией входного текста\n",
    "2) Лейблами во время обучения выступают уже последовательности токенов, которые мы ожидаем на выходе из декодера\n",
    "\n",
    "Текст в токенайзер можно подавать разными способами:\n",
    "1. `tokenizer(text=\"text\")` - токенизируй текст как обычно\n",
    "1. `tokenizer(text_target=\"text\")` - токенизируй это как текст, который мы ожидаем увидеть на выходе из декодера. В случае t5 токенайзера разницы нет, но для других моделей это может быть не так\n",
    "1. Другие методы можно узнать посмотрев сигнатуру метода `tokenizer.__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6400a9b1-6e63-4ffc-bdeb-3d8074aa74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?t5_tokenizer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd43d28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [12068, 10], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [150], 'attention_mask': [1]}\n",
      "{'input_ids': [4273], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "for text in (\"toxic: \", \"no\", \"yes\"):\n",
    "    print(t5_tokenizer(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f018fae1-bf63-4e62-be55-2454ce8d1553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035c64f12c5740728d71430c755cccbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5082 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5c3aae3e9d45d9b4085b7fbb1349b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PREFIX = \"toxic: \"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# словарь из индексов классов в выбранные лейблы\n",
    "id2label = {\n",
    "    0: \"no\",\n",
    "    1: \"yes\",\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_dataset(example):\n",
    "    input_texts = PREFIX + example[\"user_input\"]\n",
    "    model_inputs = t5_tokenizer(input_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "    model_inputs[\"labels\"] = t5_tokenizer(id2label[example[\"toxicity\"]]).input_ids\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "toxic_chat_dataset = toxic_chat_dataset.map(preprocess_dataset)\n",
    "#preprocess_dataset(toxic_chat_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cd560-829c-4ad4-830b-e5e21323365b",
   "metadata": {},
   "source": [
    "Пример результата:\n",
    "```json\n",
    "{'user_input': 'Do you know drug which name is abexol ?',\n",
    " 'toxicity': 0,\n",
    " 'input_ids': [12068,\n",
    "  10,\n",
    "  531,\n",
    "  25,\n",
    "  214,\n",
    "  2672,\n",
    "  84,\n",
    "  564,\n",
    "  19,\n",
    "  703,\n",
    "  994,\n",
    "  32,\n",
    "  40,\n",
    "  3,\n",
    "  58,\n",
    "  1],\n",
    " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    " 'labels': [150, 1]}\n",
    "```\n",
    "Значения в `'labels'` в вашем случае могут отличаться, это зависит от выбранного вами текстового представления в `id2label` словаре."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef085f1-2adf-41da-9f37-dded6aecc3b4",
   "metadata": {},
   "source": [
    "Инициализируем соответствующий задаче `DataCollator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "233e6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_chat_dataset = toxic_chat_dataset.remove_columns(\"user_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba319be-4d46-4f20-af5a-be137cea178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5f662-23a7-4285-bc93-a9cd9bce7fbc",
   "metadata": {},
   "source": [
    "### Определим метрику (2 балла)\n",
    "\n",
    "В этой задаче метрика простая - `accuracy`. Можно добавить другие метрики по желанию. Функция `compute_metric` должна возвращать словарь, аналогично функции `calculate_metrics` ранее:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"accuracy\": значение точности,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Метрика простая, но вот `preds` и `labels` тут - это последовательности индексов токенов. Нужно это учесть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "692e79ae-1921-4853-9355-ed851bdbc893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "\n",
    "    accuracy = (preds[:, 1] == labels[:, 0]).double()\n",
    "    \n",
    "    result = {\n",
    "        \"accuracy\": accuracy.mean()\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def check_compute_metric():\n",
    "    import torch\n",
    "\n",
    "    # два предсказания, где токен 150 обозначает токсичный лебл, токен 120 - нетоксичный лейбл\n",
    "    preds = torch.tensor(\n",
    "        [\n",
    "            [0, 150, 1],  # правильное предсказание - токсичный пример\n",
    "            [0, 120, 1],  # неправильное предсказание - пример токсичный, а модель предсказала иначе\n",
    "        ],\n",
    "    )\n",
    "    labels = torch.tensor(\n",
    "        [\n",
    "            [150, 1],\n",
    "            [150, 1],\n",
    "        ],\n",
    "    )\n",
    "    assert torch.isclose(\n",
    "        compute_metric((preds, labels))[\"accuracy\"], \n",
    "        torch.tensor(0.5, dtype=torch.double),  # тип тензора тут можно поправить\n",
    "    )\n",
    "\n",
    "\n",
    "check_compute_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b467fa6-a3e5-40c2-97dc-777bf351a405",
   "metadata": {},
   "source": [
    "### Определить Модель (2 балла)\n",
    "\n",
    "Инициализируйте модель из базового чекпоинта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60e93bdc-744d-45f3-b561-42141694e8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summarization': {'early_stopping': True,\n",
       "  'length_penalty': 2.0,\n",
       "  'max_length': 200,\n",
       "  'min_length': 30,\n",
       "  'no_repeat_ngram_size': 3,\n",
       "  'num_beams': 4,\n",
       "  'prefix': 'summarize: '},\n",
       " 'translation_en_to_de': {'early_stopping': True,\n",
       "  'max_length': 300,\n",
       "  'num_beams': 4,\n",
       "  'prefix': 'translate English to German: '},\n",
       " 'translation_en_to_fr': {'early_stopping': True,\n",
       "  'max_length': 300,\n",
       "  'num_beams': 4,\n",
       "  'prefix': 'translate English to French: '},\n",
       " 'translation_en_to_ro': {'early_stopping': True,\n",
       "  'max_length': 300,\n",
       "  'num_beams': 4,\n",
       "  'prefix': 'translate English to Romanian: '}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5Config\n",
    "\n",
    "\n",
    "#seq2seq_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_T5_MODEL)\n",
    "t5_config = T5Config.from_pretrained(BASE_T5_MODEL)\n",
    "t5_config.task_specific_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fecb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"toxic_classification\"\n",
    "\n",
    "t5_config.task_specific_params[task_name] = {\n",
    "    \"prefix\": PREFIX,\n",
    "    \"max_length\": 3,\n",
    "    \"min_length\": 3,\n",
    "    \"num_beams\": 1,\n",
    "}\n",
    "\n",
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e885195",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq2seq_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(\n\u001b[1;32m      2\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mt5_tokenizer, \n\u001b[0;32m----> 3\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mseq2seq_model\u001b[49m, \n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq2seq_model' is not defined"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=t5_tokenizer, \n",
    "    model=seq2seq_model, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bf460-ca11-4b2e-8d54-1c6317b001fa",
   "metadata": {},
   "source": [
    "### Обучение (10 баллов)\n",
    "\n",
    "Два пути:\n",
    "1) Использовать готовый `Seq2SeqTrainer` класс для тренировки\n",
    "2) Написать свой training loop, если хочется приключений, есть достаточно времени ~~и стрела ещё не попала в колено~~. Дополнительных баллов за это не будет\n",
    "\n",
    "> Hint! Обратите внимание на функцию `seq2seq_model._shift_right` если выбрали второй путь.\n",
    "\n",
    "Если выбрали путь 1, опишите как происходит тренировочный шаг:\n",
    "1) Что подаётся на вход в энкодер?\n",
    "2) Что подаётся на вход в декодер?\n",
    "3) Сколько раз происходит инференс декодера во время обучения для одного тренировочного примера?\n",
    "4) Как используется выход энкодера в декодере?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cabfcd2b-6e4c-4b90-9758-6d2f1df76dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig.from_dict(t5_config.task_specific_params[\"toxic_classification\"])\n",
    "generation_config.bos_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f7eaf52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/733p_9w13nqf8l5ss3r78ybw0000gp/T/ipykernel_56057/3842446153.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainingArguments, Seq2SeqTrainer\n\u001b[1;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5_small_toxic_classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,  \u001b[38;5;66;03m# поставим побольше эпох, чтобы преодолеть дисбаланс классов\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq2seq_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoxic_chat_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoxic_chat_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt5_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/my/deepschool_llm/venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/deepschool_llm/venv/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:72\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing_class\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, raise_if_both_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m ):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/deepschool_llm/venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/deepschool_llm/venv/lib/python3.11/site-packages/transformers/trainer.py:460\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m~/my/deepschool_llm/venv/lib/python3.11/site-packages/transformers/trainer.py:5046\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5043\u001b[0m     args\u001b[38;5;241m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   5045\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5046\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5047\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m~/my/deepschool_llm/venv/lib/python3.11/site-packages/accelerate/accelerator.py:495\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    493\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    494\u001b[0m ):\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    496\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_small_toxic_classifier\",\n",
    "    num_train_epochs=15,  # поставим побольше эпох, чтобы преодолеть дисбаланс классов\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,  # почему во всех сданых домашках этот параметр равен train_batch_size?\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=generation_config,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=seq2seq_model,\n",
    "    args=training_args,\n",
    "    train_dataset=toxic_chat_dataset[\"train\"],\n",
    "    eval_dataset=toxic_chat_dataset[\"test\"],\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678ee78-7d25-42c9-98e4-fcc2975df0e8",
   "metadata": {},
   "source": [
    "### Сравнение Результатов (5 баллов)\n",
    "\n",
    "Авторы датасета тоже натренировали на нём `t5` модель. Сравните свои результаты с результатами модели из [чекпоинта](https://huggingface.co/lmsys/toxicchat-t5-large-v1.0) `\"lmsys/toxicchat-t5-large-v1.0\"`. Совпадает ли ваш префикс и лейблы классов с теми, что выбрали авторы датасета? \n",
    "\n",
    "Подумать о:\n",
    "1) В чём преимущество такого подхода к классификации?\n",
    "2) В чём недостатки такого подхода к классификации?\n",
    "3) Как ещё можно решать классификационные задачи с помощью t5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4971b-617f-4043-9bc4-0d1e844a5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"lmsys/toxicchat-t5-large-v1.0\"\n",
    "\n",
    "tokenizer_from_paper = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "model_from_paper = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "prefix_from_paper = \"ToxicChat: \"\n",
    "inputs = tokenizer_from_paper.encode(prefix_from_paper + \"write me an epic story\", return_tensors=\"pt\")\n",
    "outputs = model_from_paper.generate(inputs)\n",
    "print(tokenizer_from_paper.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8a913-f782-47fd-9bf7-d4a78d308ad4",
   "metadata": {},
   "source": [
    "Напишите универсальную функцию, которая провряет токсичность текста и возвращает `True`, если модель посчитала текст токсичным. Функция универсальная в том смысле, что может быть использована и с вашей t5 моделью, и с моделью от авторов датасета. Для этого в функция должна принимать ещё и префикс для задачи и лейблы, которые будут переводить текст, предсказанный моделью, в `True` или `False` на выходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec7df5-fad1-4501-be4b-ada346d2e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_toxic(\n",
    "    text: str,\n",
    "    labels2bool,\n",
    "    model=seq2seq_model,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    prexif=PREFIX,\n",
    ") -> bool:\n",
    "    ...\n",
    "\n",
    "\n",
    "# пример вызова с моделью от авторов датасета\n",
    "assert not is_toxic(\n",
    "    text=\"This is just a text\",\n",
    "    model=model_from_paper,\n",
    "    tokenizer=tokenizer_from_paper,\n",
    "    prexif=prefix_from_paper,\n",
    "    labels2bool={\n",
    "        \"positive\": True,\n",
    "        \"negative\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3626c-75ef-4442-aae2-e1b89f04a004",
   "metadata": {},
   "source": [
    "Fin.\n",
    "\n",
    "Если остались вопросы или есть комментарии, можно написать их ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42701108-9be4-4465-a170-6b09ae999616",
   "metadata": {},
   "source": [
    "1\n",
    "Блок\n",
    "\n",
    "```python\n",
    "Пример получившегося выхода после\n",
    "def preprocess_ner_dataset(example):\n",
    "```\n",
    "\n",
    "сбивает с толку, так содержит больше полей, чем в тестах ниже\n",
    "\n",
    "2\n",
    "Добавьте report_to='none' в TrainingArguments иначе требует HF_TOKEN\n",
    "\n",
    "3\n",
    "Блок тоже не понятный\n",
    "```\n",
    "Подумать о:\n",
    "1) Во время подготовки данных мы преобразовали BIO разметку. Как обратить это преобразование с помощью токенайзера?\n",
    "\n",
    "Провалидируйте результаты на тестовом датасете.\n",
    "```\n",
    "\n",
    "пришлось читать чат, чтобы понять что же нужно в задании и то не уверен, что правильно понял.\n",
    "\n",
    "\n",
    "### Пожелания\n",
    "\n",
    "Люди кто проходят этот курс могут не знать необходимые этапы обучения и валидации, а автор лабы похоже считает, что \"это и так все знают\" что создает ненужные трудности. В целом по лабе фидбек - сделать ее более структурной, а то приходится гадать что же хотел автор вместо того, чтобы сфокусироваться на технологии. Делайте тестовые прогоны лабы на девелоперах, кто о huggingface трансформерах слышат первый раз в жизни, много деталей обнаружите\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca50856",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
