{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11861b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# можете сменить на mps на макбуке, но лично у меня он криво работает\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca3247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# можете сменить на mps на макбуке, но лично у меня он криво работает\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3",
   "metadata": {},
   "source": [
    "# Знакомство с Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df5693",
   "metadata": {},
   "source": [
    "## Создание модели и предсказание следующего токена - 5 баллов\n",
    "Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и получить следующий токен через жадную генерацию!\n",
    "\n",
    "Для загрузки модели и токенайзера вам помогут функции `.from_pretrained`\n",
    "\n",
    "**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec7e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) # Ваш код здесь\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # ваш код здесь\n",
    "\n",
    "\n",
    "text = \"This is a sample text\"\n",
    "\n",
    "# Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n",
    "# после этого мы получим logits [batch_size = 1, seq_len, d_model]\n",
    "# По этому тензору нужно предсказать следующее слово!\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "next_token_idx: int = torch.argmax(logits[0][-1])\n",
    "\n",
    "next_token = tokenizer.decode([next_token_idx])\n",
    "\n",
    "assert next_token.strip() == \"file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6809813",
   "metadata": {},
   "source": [
    "## Используем Generate - 5 баллов\n",
    "\n",
    "Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n",
    "Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n",
    "\n",
    "Для генерации есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
    "\n",
    "Ваша задача написать для модели выше генерацию по тексту с:\n",
    "* Температурой - 0.9\n",
    "* Top-K - 20\n",
    "* Repetition Penalty (Frequency Penalty) - 1.2\n",
    "* максимальное число новых токенов - 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b62dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is still a sample text, but\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "results: list[str] = []\n",
    "for i in range(10):\n",
    "    gens: torch.Tensor = model.generate(**inputs, top_k=20, temperature=0.9, repetition_penalty=1.2, max_new_tokens=10, do_sample=True)\n",
    "    generation: str = tokenizer.decode(gens[0]) # сгенерированный текст\n",
    "    results.append(generation)\n",
    "\n",
    "assert len(set(results)) > 1, \"Все генерации получились одинаковыми, проверьте опции генерации и флаг do_sample!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7",
   "metadata": {},
   "source": [
    "## Generate Batched - 5\n",
    "Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n",
    "\n",
    "Когда появляется несколько текстов разной длины, то появляются и паддинги.\n",
    "\n",
    "Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n",
    "\n",
    "Тогда \n",
    "\n",
    "```python\n",
    "input_ids = [\n",
    "    [3, 2, -1, -1, -1]\n",
    "    [5, 6,  7,  1,  2]\n",
    "]\n",
    "attention_mask = [\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "Представим, что мы сгенерировали еще один токен, тогда\n",
    "\n",
    "```python\n",
    "input_ids = [\n",
    "    [3, 2, -1, -1, -1, 7]\n",
    "    [5, 6,  7,  1,  2, 8]\n",
    "]\n",
    "attention_mask = [\n",
    "    [1, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n",
    "Тогда исходная последовательность будет:\n",
    "\n",
    "```python\n",
    "input_ids = [\n",
    "    [-1, -1, -1, 3, 2]\n",
    "    [ 5,  6,  7, 1, 2]\n",
    "]\n",
    "attention_mask = [\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "и после генерации следующего токена\n",
    "\n",
    "```python\n",
    "input_ids = [\n",
    "    [-1, -1, -1, 3, 2, 7]\n",
    "    [ 5,  6,  7, 1, 2, 8]\n",
    "]\n",
    "attention_mask = [\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n",
    "\n",
    "Для этого нам придется использовать параметр padding_side в конструкторе токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # ваш код здесь\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n",
    "\n",
    "# Внимание! В данном задании нужна жадная генерация!\n",
    "\n",
    "# Соберите оба текста в один батч и положите результаты генерации в \n",
    "# batched_generations\n",
    "batched_generations: List[str] = []\n",
    "\n",
    "batch = tokenizer(texts, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    gens_tokens = model.generate(**batch)\n",
    "\n",
    "gens_decoded = tokenizer.batch_decode(gens_tokens, skip_special_tokens=True)\n",
    "batched_generations += gens_decoded\n",
    "\n",
    "# Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации \n",
    "# в single_generations\n",
    "single_generations: List[str] = []\n",
    "\n",
    "for text in texts:\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model.generate(**tokenized)\n",
    "\n",
    "    gen_decoded = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "    single_generations.append(gen_decoded)\n",
    "\n",
    "assert len(batched_generations) == 2 and len(single_generations) == 2\n",
    "for s, b in zip(batched_generations, single_generations):\n",
    "    assert s == b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da008c-3653-40d5-89ba-cd831352fd3d",
   "metadata": {},
   "source": [
    "# Скоринг, Perplexity - 10 баллов\n",
    "\n",
    "Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n",
    "\n",
    "Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n",
    "\n",
    "Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n",
    "\n",
    "```\n",
    "P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n",
    "```\n",
    "\n",
    "Т.е. это вероятность слова при условии его левого контекста.\n",
    "Зачастую ее обозначают как $P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n",
    "Эти вероятности можно взять из выходного вектора!\n",
    "\n",
    "Давайте попробуем подсчитать вероятность и perplexity текстов!\n",
    "perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n",
    "\n",
    "$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n",
    "\n",
    "В этом задании нужно:\n",
    "1. Посчитать вероятность **text**\n",
    "2. Посчитать перплексию **text**\n",
    "\n",
    "Еще одна важная деталь:\n",
    "работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n",
    "Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n",
    "\n",
    "$$ p = p_1 * p_2 * p_3 $$\n",
    "$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n",
    "$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n",
    "\n",
    "В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of sentence (BOS) token = `<|endoftext|>`\n",
      "End of sentence (EOS) token  = `<|endoftext|>`\n"
     ]
    }
   ],
   "source": [
    "def calc_probs_simple(input_ids: torch.Tensor, log_probs: torch.Tensor) -> torch.Tensor:\n",
    "    # сделал неэффективное, но верное решение в лоб для подбора параметров для torch.gather\n",
    "    result = []\n",
    "\n",
    "    for batch_idx in range(input_ids.shape[0]):\n",
    "        inp = input_ids[batch_idx][1:]\n",
    "        lp = log_probs[batch_idx]\n",
    "        \n",
    "        batch_log_probs = []\n",
    "        for idx, item in enumerate(inp):\n",
    "            batch_log_probs.append(lp[idx][item].item())\n",
    "\n",
    "        result.append(batch_log_probs)\n",
    "\n",
    "    return torch.tensor(result)\n",
    "\n",
    "\n",
    "print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n",
    "print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n",
    "text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    # 1. Нужно обрезать logits по длине, т.к. для предсказаний по последнему токену нечего считать\n",
    "    logits_trunc = logits[:, :-1, :]\n",
    "\n",
    "    # 2. Превращаем logits в log_probs\n",
    "    log_probs = torch.log_softmax(logits_trunc, dim=-1)\n",
    "\n",
    "    # 3. Берем вероятности следующих токенов, т.к. по вектору i-й позиции мы предсказываем токен на позиции (i + 1)\n",
    "    # для этого нам поможет torch.gather\n",
    "    expected_log_probs = calc_probs_simple(inputs[\"input_ids\"], log_probs)\n",
    "\n",
    "    # !вопрос! это такой сложной конструкцией должно оформляться? или есть более элегантное и простое решение?\n",
    "    index = inputs[\"input_ids\"][:, 1:].unsqueeze(-1)\n",
    "    next_token_log_probs = torch.gather(input=log_probs, dim=2, index=index).squeeze(-1) # batch_size x seq_len - 1\n",
    "\n",
    "    assert torch.all(expected_log_probs==next_token_log_probs)\n",
    "\n",
    "    # 4. Считаем вероятности и perplexity!\n",
    "    prob = torch.exp(next_token_log_probs.sum())\n",
    "    ppl = torch.exp((-1 / next_token_log_probs.shape[-1]) * next_token_log_probs.sum())\n",
    "\n",
    "    # должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl\n",
    "    expected_prob = torch.tensor(2.1783e-14)\n",
    "    expected_ppl = torch.tensor(51.0)\n",
    "\n",
    "    assert torch.isclose(prob, expected_prob)\n",
    "    assert torch.isclose(ppl, expected_ppl, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f244eac-7cb1-4689-8adc-46662891e657",
   "metadata": {},
   "source": [
    "# Вопросы - 5 баллов\n",
    "\n",
    "**Ответьте на вопрсоы текстом прямо здесь!**\n",
    "\n",
    "\n",
    "1. Какое значение P(X) вероятности текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n",
    "\n",
    "    Ответ: 1 - максимальное значение вероятности [0..1]. Чем ближе к 1 тем более модель уверена в ответе\n",
    "\n",
    "2. Какое значение перплексии текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n",
    "\n",
    "    Ответ: тоже 1, но это минимальное значение Perplexity. Чем выше значение, тем больше \"удивлена\" модель заданной генерации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd5038-620b-48bb-bbc1-db3729141d78",
   "metadata": {},
   "source": [
    "# Chat-Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c7530-a7ce-4d23-abaf-2b0cec87301e",
   "metadata": {},
   "source": [
    "# Формат - 5 баллов\n",
    "Как мы обсуждали на лекции, все chat-модели принимают входы в своем особом формате.\n",
    "Он может быть описан текстом, а может быть заложен в шаблон, который доступен через `tokenizer.apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52fdd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    load_dotenv(\"../.env\")  # load HF_TOKEN for downloading google/gemma-2b-it\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5fe593-63a8-406d-9678-6d805c180670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411ce2c2374447c98c996c21ba72d75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "model_name = \"google/gemma-2b-it\" # smaller model to fit 16GB\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.half).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8dad11-6811-49ab-ad3d-8ac7de103828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(d):\n",
    "    for k, v in d.items():\n",
    "        d[k] = v.to(device)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f15fd-576a-4e8e-917a-74df01a944f4",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как chat модель отработает на обычном тексте. Используйте для генерации сэмплинг и kv cache, выведите 5 результатов генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you doing today?\n",
      "\n",
      "As a large language model, I do not experience time in the same way that\n",
      "============\n",
      "hello how are you doing today?\n",
      "\n",
      "I am doing well, thank you for asking!  I hope you are having\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "text = \"hello how are you\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# !вопрос! как включить kv cache?\n",
    "\n",
    "for i in range(5):\n",
    "    generated_tokens = model.generate(**move_to_device(inputs), use_cache=True, do_sample=True)\n",
    "    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print(\"====\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc",
   "metadata": {},
   "source": [
    "Видим, что текст зачастую выходит мусорный. Это потому что формат входных данных сильно отличается от того, что модель видела на обучении.\n",
    "Как мы уже обсуждали, у всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели.\n",
    "\n",
    "Не забудьте про опцию add_generation_prefix - она добавляет часть формата, после которой ожидается ответ модели!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hello\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n",
    "    {\"role\": \"user\", \"content\": \"I love you\"},\n",
    "]\n",
    "\n",
    "prefix = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "reference_llama = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "reference_gemma = \"\"\"<bos><start_of_turn>user\n",
    "hello<end_of_turn>\n",
    "<start_of_turn>model\n",
    "I'm good. How can I help you today<end_of_turn>\n",
    "<start_of_turn>user\n",
    "I love you<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "reference = reference_llama if \"Llama\" in model.name_or_path else reference_gemma\n",
    "assert prefix.strip() == reference.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b8882-bec0-4bf4-b6fb-30e727d095c6",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что нам ответит модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "hello\n",
      "model\n",
      "I'm good. How can I help you today\n",
      "user\n",
      "I love you\n",
      "model\n",
      "I'm happy to hear that! How can I help you today? Is there anything I can\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
    "gen_tokens = model.generate(**inputs)\n",
    "gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72482f3-c296-46f3-851c-57b4f91a717b",
   "metadata": {},
   "source": [
    "## Benchmark - 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6",
   "metadata": {},
   "source": [
    "Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n",
    "* question - вопрос\n",
    "* choices - варианты ответа\n",
    "* answer - номер правильного ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices?',\n",
       " 'subject': 'global_facts',\n",
       " 'choices': ['About $300', 'About $3k', 'About $8k', 'About $15k'],\n",
       " 'answer': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n",
    "mmlu[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca61a91-5784-44f3-af9b-72250f8d58a4",
   "metadata": {},
   "source": [
    "Наша задача здесь решить задачу многоклассовой классификации.\n",
    "Для этого нужно посчитать \n",
    "$$P(choices_i | question)$$\n",
    "т.е. для посчитать вероятность каждого варианта ответа для вопроса. Мы это уже делали кодом выше!\n",
    "\n",
    "После этого давайте брать самый вероятный ответ и считать, что модель его выбрала.\n",
    "После этого давайте посчитаем accurracy, т.е. долю правильных ответов.\n",
    "Вместо вероятностей для подсчета лучше использовать logprobs.\n",
    "\n",
    "Итого, что нужно сделать:\n",
    "1. Пройтись по датасету, для каждого question и каждого из соответствующих choices получить самый вероятный ответ.\n",
    "2. Посчитать итоговый accuracy\n",
    "\n",
    "**Важно**\n",
    "1. Выше мы уже написали скоринг текста с помощью LLM, для этого задания можно адаптировать функцию.\n",
    "2. Если делаете варианты с батчеванием помните: длины choices могут быть разными! Нужно не считать вероятности по паддингам. В этом нам помогут attention_masks из выходов `tokenizer()`\n",
    "3. В данном задании для простоты мы игнорируем формат ответа llama3 и делаем скоринг по f\"{question} {answer}\"\n",
    "\n",
    "\n",
    "Попробуйте для начала написать вариант со скорингом для батча размера 1, а потом для батча размера 3 или 5. Код должен корректно работать для батча любого размера и выдавать одинаковую итоговую точность.\n",
    "\n",
    "За задание, в котором код работает только с батчом размера 1, 2, 4 можно получить **только 10 баллов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a4126e2-c463-404d-a3b5-5361f744242e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a4d2f4594248328e024174825ca554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a086fad8f344a892c7cd69c47004f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting dataset accuracy: 0.27\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def sample_to_texts(sample):\n",
    "    return [sample[\"question\"] + \" \" + answer for answer in sample[\"choices\"]]\n",
    "\n",
    "all_samples_formatted = sum([sample_to_texts(sample) for sample in mmlu], [])\n",
    "#print(*all_samples_formatted[2:6], sep=\"\\n\")\n",
    "# ваш код здесь!\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_texts_log_probs(texts: list[str]) -> list[float]:\n",
    "    # Задача - найти самый вероятный ответ, то есть сравнить вероятность каждой из опций и выбрать наибольшую\n",
    "    # Считать вероятности надо только по токенам ответа, т.к. вопрос считается уже заданным и вероятность генерации текста вопроса нам не нужна\n",
    "    # Но т.к. вероятности по токенам вопроса будут неизменной, можно посчитать по целой строке \"вопрос вариант\" и выбрать наибольшую\n",
    "    # Вероятности крайне малые будут, так что опираемся на логарифмы вероятностей log_probs\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, padding_side=\"right\")\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "    logits_trunc = logits[:, :-1, :]\n",
    "    log_probs = torch.log_softmax(logits_trunc, dim=-1)\n",
    "    index = inputs[\"input_ids\"][:, 1:].unsqueeze(-1)\n",
    "    next_token_log_probs = torch.gather(input=log_probs, dim=2, index=index).squeeze(-1) # batch_size x seq_len - 1\n",
    "\n",
    "    # у маски как и у инпутов надо убрать первый элемент, т.к. по первому токену предсказаний нет\n",
    "    shifted_mask = inputs[\"attention_mask\"][:, 1:]\n",
    "    next_token_log_probs_masked = next_token_log_probs.masked_fill(~shifted_mask.bool(), 0)\n",
    "\n",
    "    texts_log_probs = next_token_log_probs_masked.sum(dim=-1)\n",
    "\n",
    "    return texts_log_probs\n",
    "\n",
    "\n",
    "class DatasetEvaluator:\n",
    "    def __init__(self, batch_size: int):\n",
    "        assert batch_size >= 1\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self._accuracy = 0.0\n",
    "        self._num_questions_answered = 0\n",
    "        self._num_correct_answers = 0\n",
    "        self._questions = []\n",
    "        self._batch_texts = []\n",
    "        self._predicted_answers = []\n",
    "\n",
    "    def _add_to_batch(self, text: str) -> None:\n",
    "        self._batch_texts.append(text)\n",
    "\n",
    "        if len(self._batch_texts) == self.batch_size:\n",
    "            self._predict_batch()\n",
    "\n",
    "    def _predict_batch(self):\n",
    "        if self._batch_texts:\n",
    "            log_probs = calc_texts_log_probs(self._batch_texts)\n",
    "            self._predicted_answers += log_probs\n",
    "            self._process_answers()\n",
    "\n",
    "            self._batch_texts.clear()\n",
    "\n",
    "    def _process_answers(self):\n",
    "        while self._questions:\n",
    "            question = self._questions[0]\n",
    "            num_answers = len(question[\"choices\"])\n",
    "            correct_answer_idx = question[\"answer\"] - 1\n",
    "\n",
    "            # not all options for a question were calculated\n",
    "            if len(self._predicted_answers) < num_answers:\n",
    "                break\n",
    "\n",
    "            predicted_answers_logprobs = torch.tensor(self._predicted_answers[:num_answers])\n",
    "            is_prediction_correct = torch.argmax(predicted_answers_logprobs) == correct_answer_idx\n",
    "            self._accept_answer(is_prediction_correct)\n",
    "\n",
    "            # remove question and answers from buffer\n",
    "            self._questions.pop(0)\n",
    "            self._predicted_answers = self._predicted_answers[num_answers:]\n",
    "\n",
    "    def _accept_answer(self, correct: bool) -> None:\n",
    "        self._num_questions_answered += 1\n",
    "\n",
    "        if correct:\n",
    "            self._num_correct_answers += 1\n",
    "\n",
    "        self._accuracy = self._num_correct_answers / self._num_questions_answered\n",
    "\n",
    "    # Can be called multiple times to accumulate accuracy\n",
    "    def calculate_dataset_accuracy(self, dataset: Dataset) -> float:\n",
    "        ds_iterable = tqdm(dataset)\n",
    "\n",
    "        for entry in ds_iterable:\n",
    "            # display current accuracy\n",
    "            ds_iterable.set_description(f\"Accuracy: {self._accuracy:.2f}\")\n",
    "\n",
    "            self._questions.append(entry)\n",
    "            for text in sample_to_texts(entry):\n",
    "                self._add_to_batch(text)\n",
    "\n",
    "        self._predict_batch()\n",
    "\n",
    "        return self._accuracy\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self._accuracy = 0.0\n",
    "        self._num_questions_answered = 0\n",
    "        self._num_correct_answers = 0\n",
    "\n",
    "    # for debugging\n",
    "    def print_results(self) -> None:\n",
    "        print(f\"Batch Size: {self.batch_size} Accuracy: {self._accuracy} Total Questions: {self._num_questions_answered} Correct Answers: {self._num_correct_answers}\")\n",
    "\n",
    "\n",
    "dataset = mmlu.select(range(100))\n",
    "\n",
    "ds_eval_batch_1 = DatasetEvaluator(batch_size=1)\n",
    "ds_eval_batch_7 = DatasetEvaluator(batch_size=7)\n",
    "\n",
    "ds_accuracy_batch_1 = ds_eval_batch_1.calculate_dataset_accuracy(dataset)\n",
    "ds_accuracy_batch_7 = ds_eval_batch_7.calculate_dataset_accuracy(dataset)\n",
    "\n",
    "assert ds_accuracy_batch_1 == ds_accuracy_batch_7\n",
    "\n",
    "print(f\"Resulting dataset accuracy: {ds_accuracy_batch_1}\") # 0.27 for google/gemma-2b-it, only 0.02 better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c50f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 1 Accuracy: 0.27 Total Questions: 100 Correct Answers: 27\n",
      "Batch Size: 7 Accuracy: 0.27 Total Questions: 100 Correct Answers: 27\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "ds_eval_batch_1.print_results()\n",
    "ds_eval_batch_7.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86188a",
   "metadata": {},
   "source": [
    "# Вопросы - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c440af-dfc0-460d-a113-3fab3fefa361",
   "metadata": {},
   "source": [
    "**Ответьте на следующие вопросы (5 баллов в сумме)**:\n",
    "1. Как влияет длина ответа на вероятность ответа при скоринге? Если есть какие-либо проблемы, как бы вы с этим боролись.\n",
    "\n",
    "Ответ:\n",
    "\n",
    "Чем длинее ответ, тем меньше будет его вероятность, т.к. даже в идеальной модели вероятность следующего токена будет меньше 1 ввиду энтропии естественного языка. Значит наиболее репрезентативно сравнивать ответы одинаковой или близкой длины\n",
    "\n",
    "Из проблем можно отметить то, что вероятность целого ответа - очень маленькое число, выходящее за точность float и работать надо с логарифмом\n",
    "\n",
    "2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже?\n",
    "Стоит ли по-вашему добавлять эти метки?\n",
    "\n",
    "Ответ: если мы эвалюируем ответы методом подсчета вероятностей как в задании выше, то добавление меток скорее ухудшит качество ответов.\n",
    "\n",
    "Причины: \n",
    "- Чтобы качество было хорошее при наличии меток, тренировочные данные должны содержать этот же тест с вариантами ответа жестко привязанными к меткам, другими словами метка становится частью ответа. Смена порядка ответов при эвалюации будет влиять на качество (префиксы остаются A B C D, а варианты за ними могут быть в любом порядке)\n",
    "- Добавление префиксов добавляет больше токенов и это уменьшает вероятность каждого из ответов\n",
    "\n",
    "По вышеуказанным причинам префиксы лучше не добавлять\n",
    "\n",
    "С другой стороны, если делать эвалюацию более мощных моделей в промпте указывая, что нужно вывести номер\\префикс правильного ответа, то предположу, что качество не измениться, а может и улучшиться, за счет того, что генерироваться будет только один токен правильного ответа (уменьшится шанс геренации ответа, которого не было в списке вариантов)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
