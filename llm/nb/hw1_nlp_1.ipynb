{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Домашнее задание (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В этом домашнем задании вы познакомитесь с основами NLP, научитесь обрабатывать тексты.\n",
    "\n",
    "В местах, где используется `...` (elipsis), требуется заменить его на код.\n",
    "\n",
    "Установим необходимые зависимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T19:21:30.570036Z",
     "start_time": "2024-11-27T19:21:27.452197Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (24.3.1)\n",
      "Requirement already satisfied: nltk in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: seqeval in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: datasets in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: click in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install nltk tqdm seqeval scikit-learn datasets numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T19:21:40.162946Z",
     "start_time": "2024-11-27T19:21:40.157819Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Токенизация (15 баллов)\n",
    "\n",
    "Токенизация - это процесс преобразования текста в набор токенов.\n",
    "Наивная реализация разбивает текст по пробелам. Более умные реализации учитывают пунктуацию.\n",
    "\n",
    "### Библиотека NLTK (2 балла)\n",
    "\n",
    "Научимся работать с токенизацией NLTK, где уже [реализована](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize) работа с пунктуацией.\n",
    "\n",
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vkuznetsov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vkuznetsov/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vkuznetsov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# https://www.nltk.org/nltk_data/\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def tokenize(text: str, language: str = \"english\") -> List[str]:\n",
    "    return word_tokenize(text, language='english')\n",
    "\n",
    "assert tokenize(\"\") == []\n",
    "assert tokenize(\"Hello, world!\") == [\"Hello\", \",\", \"world\", \"!\"]\n",
    "assert tokenize(\"EU rejects German call to boycott British lamb.\") == [\"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация (3 балла)\n",
    "\n",
    "Добавим нормализацию после токенизации. Пробуем [лемматизацию](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.wordnet.WordNetLemmatizer) , [стемминг](https://www.nltk.org/api/nltk.stem.snowball.html#nltk.stem.snowball.EnglishStemmer) и [юникод](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize) нормализацию. Напишем функцию, которая будет принимать на вход токен после токенизации, нормализовать в NFC юникод форму, переводит в нижний регистр, лемматизирует слово и, если слово не изменилось после лемматизации, применяет стемминг.\n",
    "\n",
    "\n",
    "Создайте функцию `normalize`:\n",
    "   - Функция `normalize` должна принимать строку `token` и возвращать нормализованный токен.\n",
    "   - Примените к токену Unicode нормализацию с помощью `unicode_nfc_normalizer`.\n",
    "   - Преобразуйте токен в нижний регистр.\n",
    "   - Примените лемматизацию с помощью `lemmatizer`.\n",
    "   - Если лемматизированный токен отличается от исходного, верните его. В противном случае, примените стемминг с помощью `stemmer` и верните результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "from functools import partial\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "unicode_nfc_normalizer = partial(unicodedata.normalize, \"NFC\")\n",
    "\n",
    "\n",
    "def normalize(token: str) -> str:\n",
    "  \"\"\"\n",
    "  Нормализует токен, применяя Unicode нормализацию, преобразование в нижний регистр,\n",
    "  лемматизацию и стемминг при необходимости.\n",
    "\n",
    "  :param token: Токен для нормализации\n",
    "  :return: Нормализованный токен\n",
    "  \"\"\"\n",
    "\n",
    "  normalized = unicode_nfc_normalizer(token)\n",
    "  normalized = normalized.lower()\n",
    "\n",
    "  lemmatized = lemmatizer.lemmatize(normalized)\n",
    "  if lemmatized != normalized:\n",
    "    return lemmatized\n",
    "\n",
    "  stemmed = stemmer.stem(normalized)\n",
    "  return stemmed\n",
    "\n",
    "\n",
    "test_tokens = [\"Worlds\", \"churches\", \"Helping\"]\n",
    "assert [normalize(token) for token in test_tokens] == [\"world\", \"church\", \"help\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавляем Словарь (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Современные токенайзеры не только разбивают строки на токены, но и преобразуют последовательность токенов в последовательность числел. Объединим функцию токенизации, нормализации и отображения из токенов в индексы в один объект токенайзера.\n",
    "\n",
    "Напишите класс `Tokenizer` для токенизации и нормализации текста.\n",
    "\n",
    "Построение словаря:\n",
    "   - Создайте метод `_build_vocabulary`, который принимает список текстов `texts` и обновляет словарь токенов.\n",
    "   - Для каждого текста:\n",
    "     - Токенизируйте и нормализуйте текст.\n",
    "     - Обновите счетчик вхождений слов.\n",
    "   - Для каждого слова, которое встречается не менее `min_count` раз, добавьте слово в словарь `word2idx` и список `idx2word`.\n",
    "\n",
    "Кодирование и декодирование:\n",
    "   - Создайте метод `encode_word`, который принимает слово `word` и возвращает его индекс с применением нормализации.\n",
    "   - Создайте метод `encode`, который принимает текст `text` и возвращает список индексов токенов.\n",
    "   - Создайте метод `decode`, который принимает список индексов `input_ids` и возвращает текст, вставляя пробелы между токенами.\n",
    "\n",
    "> Note: для функций, которые могут долго исполнятся (`_build_vocab`), рекомендуется использовать библиотеку tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            texts: List[str],\n",
    "            tokenize_fn: Callable[[str], List[str]] = tokenize,\n",
    "            normalize_fn: Callable[[str], str] = lambda token: token,\n",
    "            min_count: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Инициализация токенизатора.\n",
    "\n",
    "        :param texts: список текстов для построения словаря\n",
    "        :param tokenize_fn: функция для токенизации текста\n",
    "        :param normalize_fn: функция для нормализации токенов\n",
    "        :param min_count: минимальное количество вхождений слова для включения в словарь\n",
    "        \"\"\"\n",
    "        self.min_count = min_count\n",
    "        self.tokenize = tokenize_fn\n",
    "        self.normalize = normalize_fn\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.unk_token_id = 3\n",
    "        self.idx2word = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "        self.word2count = Counter()\n",
    "        self._build_vocabulary(texts)\n",
    "\n",
    "        # for ngram\n",
    "        self.bos_token = 1\n",
    "        self.eos_token = 2\n",
    "        self.special_token_ids = (self.bos_token, self.eos_token)\n",
    "\n",
    "    def _build_vocabulary(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Построение словаря на основе списка текстов.\n",
    "\n",
    "        :param texts: список текстов\n",
    "        \"\"\"\n",
    "\n",
    "        for text in tqdm(texts):\n",
    "          for token in self.tokenize(text):\n",
    "            # normalized token\n",
    "            nt = self.normalize(token)\n",
    "            self.word2count[nt] += 1\n",
    "\n",
    "            if self.word2count[nt] >= self.min_count and nt not in self:\n",
    "              # add token\n",
    "              self.idx2word.append(nt)\n",
    "              self.word2idx[nt] = len(self.idx2word) - 1\n",
    "\n",
    "    def encode_word(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Кодирование слова в индекс с применением нормализации.\n",
    "\n",
    "        :param text: слово\n",
    "        :return: индекс слова\n",
    "        \"\"\"\n",
    "        nt = self.normalize(text)\n",
    "        encoded = self.word2idx.get(nt, self.unk_token_id)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Кодирование текста в набор индексов.\n",
    "\n",
    "        :param text: текст\n",
    "        :return: набор индексов токенов\n",
    "        \"\"\"\n",
    "        encoded = []\n",
    "\n",
    "        for token in self.tokenize(text):\n",
    "          token_enc = self.encode_word(token)\n",
    "          encoded.append(token_enc)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, input_ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n",
    "\n",
    "        :param input_ids: набор индексов токенов\n",
    "        :return: текст\n",
    "        \"\"\"\n",
    "        result = []\n",
    "\n",
    "        for idx in input_ids:\n",
    "          # Handle incorrect indexes\n",
    "          if idx < 0 or idx >= len(self.idx2word):\n",
    "            idx = self.unk_token_id\n",
    "\n",
    "          result.append(self.idx2word[idx])\n",
    "\n",
    "        return \" \".join(result)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает количество уникальных токенов в словаре.\n",
    "\n",
    "        :return: количество уникальных токенов\n",
    "        \"\"\"\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def __contains__(self, item: str) -> bool:\n",
    "        \"\"\"\n",
    "        Проверяет, содержится ли слово в словаре.\n",
    "\n",
    "        :param item: слово\n",
    "        :return: True, если слово содержится в словаре, иначе False\n",
    "        \"\"\"\n",
    "        return item in self.word2idx\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Возвращает строковое представление словаря.\n",
    "\n",
    "        :return: строковое представление словаря\n",
    "        \"\"\"\n",
    "        return str(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977cf06bcb2346d49e3092fe9cc324f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e83f9b088b4c14bf81dd9340bd5377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = [\"Hello, world!\", \"I love Python!\"]\n",
    "\n",
    "tokenizer = Tokenizer(corpus, min_count=1)\n",
    "encoded = tokenizer.encode(\"Hello, Python! I love you\")\n",
    "assert tokenizer.decode(encoded) == \"Hello , Python ! I love <UNK>\"\n",
    "\n",
    "tokenizer = Tokenizer(corpus, normalize_fn=normalize)\n",
    "encoded = tokenizer.encode(\"Hello, Python! I loved you\")\n",
    "assert tokenizer.decode(encoded) == \"hello , python ! i love <UNK>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (20 баллов)\n",
    "\n",
    "\n",
    "### Класс TFIDF (10 баллов)\n",
    "\n",
    "Создайте класс `TFIDF` для вычисления TF-IDF значений. Формулы для подсчёта TF и IDF можно выбрать [тут](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Обучение модели должно осуществляться с помощью метода `fit`, который принимает список строк `docs` и обучает модель на этом корпусе, вызывая метод `add_doc` для каждого документа.\n",
    "\n",
    "Предсказание TF-IDF значений:\n",
    "   - Создайте метод `predict`, который принимает список строк `docs` и возвращает матрицу TF-IDF значений.\n",
    "   - Для каждого документа:\n",
    "     - Токенизируйте документ.\n",
    "     - Вычислите TF для каждого термина.\n",
    "     - Вычислите IDF для каждого термина.\n",
    "     - Заполните матрицу TF-IDF значений.\n",
    "   - Нормализуйте строки матрицы, чтобы сумма значений в каждой строке была равна 1.\n",
    "\n",
    "> Важно! Не забудьте убрать `<UNK>` токен во  время подсчёта TF-IDF \n",
    "\n",
    "Для функций, которые могут долго исполнятся (`fit`, `predict`), рекомендуется использовать библиотеку tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class TFIDF:\n",
    "    def __init__(self, tokenizer: Tokenizer, default_idf = 1.0) -> None:\n",
    "        \"\"\"\n",
    "        Инициализация TFIDF.\n",
    "\n",
    "        :param tokenizer: токенизатор для преобразования текста в токены\n",
    "        :param default_idf: значение IDF для неизвестных токенов\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.default_idf = default_idf\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.num_docs = 0\n",
    "        self.term2num_docs = [0 for _ in self.tokenizer.word2idx]  # для подсчёта IDF        \n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает размер словаря.\n",
    "\n",
    "        :return: размер словаря\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer)\n",
    "\n",
    "    def add_doc(self, doc: str) -> None:\n",
    "        \"\"\"\n",
    "        Добавляет документ в модель TFIDF.\n",
    "\n",
    "        :param doc: документ для добавления\n",
    "        \"\"\"\n",
    "        self.num_docs += 1\n",
    "        \n",
    "        # Calculate term presence in doc\n",
    "        terms_added = set(self.tokenizer.encode(doc))\n",
    "        for term in terms_added:\n",
    "            self.term2num_docs[term] += 1\n",
    "        \n",
    "    def fit(self, docs: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Обучает модель TFIDF на корпусе docs.\n",
    "\n",
    "        :param docs: корпус для обучения\n",
    "        \"\"\"\n",
    "        # reset before fit, easier to debug\n",
    "        self.reset()\n",
    "\n",
    "        for doc in tqdm(docs):\n",
    "            self.add_doc(doc)\n",
    "\n",
    "    def predict(self, docs: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Предсказывает TFIDF значения для списка документов.\n",
    "\n",
    "        :param docs: список документов\n",
    "        :return: матрица TFIDF значений\n",
    "        \"\"\"\n",
    "        idf = np.array([self.idf(i) for i in range(len(self.term2num_docs))])\n",
    "        matrix = np.zeros(shape=(len(docs), len(self.term2num_docs)))\n",
    "\n",
    "        doc_lengths = []\n",
    "\n",
    "        for doc_idx, doc in enumerate(tqdm(docs)):\n",
    "            # get tokens except unknown\n",
    "            doc_tokens = [t for t in self.tokenizer.encode(doc) if t != self.tokenizer.unk_token_id]\n",
    "\n",
    "            doc_lengths.append(len(doc_tokens))\n",
    "            \n",
    "            # skip empty doc\n",
    "            if not doc_tokens:\n",
    "                continue\n",
    "\n",
    "            tf_idf = matrix[doc_idx, :]\n",
    "            \n",
    "            # calculate token occurrence\n",
    "            cnt = Counter(doc_tokens)\n",
    "            for term, count in cnt.items():\n",
    "                tf_idf[term] = count\n",
    "\n",
    "        # for zero doc length, set doc length to 1 to avoid div by zero\n",
    "        # it will not affect the results as all values in the row will be 0s\n",
    "        doc_lengths_updated = [dl if dl > 0 else 1 for dl in doc_lengths]\n",
    "        \n",
    "        # calculate term frequency for a doc\n",
    "        matrix /= np.array(doc_lengths_updated).reshape(-1,1)\n",
    "            \n",
    "        # calculate tf-idf for a doc\n",
    "        matrix *= idf\n",
    "\n",
    "        # normalize\n",
    "        sum_per_doc = matrix.sum(axis=1,keepdims=True)\n",
    "        \n",
    "        # for zero rows - avoid div by zero\n",
    "        sum_per_doc[sum_per_doc == 0] = 1\n",
    "        matrix /= sum_per_doc\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def idf(self, term: int) -> float:\n",
    "        \"\"\"\n",
    "        Вычисляет IDF (обратную частоту документа) для термина.\n",
    "\n",
    "        :param term: термин\n",
    "        :return: IDF значение\n",
    "        \"\"\"\n",
    "        if term == self.tokenizer.unk_token_id:\n",
    "            return self.default_idf\n",
    "\n",
    "        df = self.term2num_docs[term]\n",
    "        idf = math.log(self.num_docs / (df + 1)) + 1\n",
    "\n",
    "        return idf\n",
    "\n",
    "        # тестовый корпус из ячейки ниже содержит всего 2 текста\n",
    "        # токен \"test\" появляется в обоих и в классической формуле\n",
    "        # idf(\"test\") == log(2 / 2) == 0\n",
    "        # поэтому в домашнем задании рекомендуется использовать\n",
    "        # сглаженный вариант подсчёта inverse document frequency smooth:\n",
    "        # log(N / (n + 1)) + 1\n",
    "        # https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency\n",
    "        ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тесты были проверены для такой комбинации формул:\n",
    "\n",
    "$$\n",
    "\\text{TF} = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}\n",
    "$$\n",
    "$$\n",
    "\\text{IDF} = \\log{\\frac{N}{1 + n_t}} + 1\n",
    "$$\n",
    "\n",
    "Если вы выбрали другие формулы для подсчёта, то можно поправить тесты соответственно.\n",
    "\n",
    "> Можно расширить расширить класс для удобства и поэксперементировать с различными формулами для подсчёта TF и IDF при классификации IMDB датасета ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e9b9866a9f4f51976684a3beeec83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b59fdc729f4e04bf0d94ce925ca644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8347a1550141028636549c559ab234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bea80908869419c9d45a836c8184c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4d27fd85a04a0db50e7af9868f2f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = [\"test test\", \"not a test\"]\n",
    "tokenizer = Tokenizer(corpus)\n",
    "tfidf = TFIDF(tokenizer)\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "assert tfidf.vocab_size == 4 + 3\n",
    "# 3 токена, один из которых <UNK>\n",
    "vector = tfidf.predict([\"a test string\"])[0]\n",
    "# tf(\"a\") == tf(\"test\") and idf(\"a\") > idf(\"test\")\n",
    "assert vector[tfidf.tokenizer.word2idx[\"a\"]] > vector[tfidf.tokenizer.word2idx[\"test\"]]\n",
    "\n",
    "vector = tfidf.predict([\"not a test a string\"])[0]\n",
    "assert vector[tfidf.tokenizer.word2idx[\"a\"]] > 2 * vector[tfidf.tokenizer.word2idx[\"test\"]]\n",
    "assert vector[tfidf.tokenizer.word2idx[\"a\"]] == 2 * vector[tfidf.tokenizer.word2idx[\"not\"]]\n",
    "\n",
    "assert not np.any(tfidf.predict([\"all tokens abscent from vocab should be zeros vector\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет (1 балл)\n",
    "\n",
    "В качестве датасета вёзмём популярный [набор отзывов на фильмы с сайта IMDB](https://huggingface.co/datasets/stanfordnlp/imdb). Нужно предсказать является ли отзыв позитивным или негативным. Чтобы скачать датасет воспользуемся библиотекой `datasets` из экосистемы `HuggingFace`. Интерфейс датасета похож на словарь, доступ к разным частям осуществляется по названию ключа:\n",
    "\n",
    "1. Тренировочная часть датасета: `imdb[\"train\"]`\n",
    "2. Тексты для тренировки: `imdb[\"train\"][\"text\"]`\n",
    "3. Лейблы для тренировки: `imdb[\"train\"][\"label\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "imdb = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренируем Токенайзер (2 балла)\n",
    "\n",
    "Используя `\"train\"` часть датасета, инициализируйте два токеназйера:\n",
    "1. Не использующий нормализацию\n",
    "2. Использующий функцию `normalize`, определённую выше\n",
    "\n",
    "Сравним размер полученного словаря в обоих случаях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7433e2990bfd49948f1fb244f543b885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a92c550ec3414aae89b05be5e5ead1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_without_norm = Tokenizer(imdb[\"train\"][\"text\"])\n",
    "tokenizer_with_norm = Tokenizer(imdb[\"train\"][\"text\"], normalize_fn=normalize)\n",
    "\n",
    "assert len(tokenizer_without_norm) > len(tokenizer_with_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренируем TF-IDF Модель (2 балла)\n",
    "\n",
    "Теперь мы можем натренировать модель на датасете. Обучим две модели, которые будут использовать разные токенайзеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc426643fe164b609611b4e6bd1df6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_without_norm = TFIDF(tokenizer_without_norm)\n",
    "tfidf_without_norm.fit(imdb[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b82b079c6447e983bb49ce3af2ea92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_with_norm = TFIDF(tokenizer_with_norm)\n",
    "tfidf_with_norm.fit(imdb[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим Логистическую Регрессию (5 баллов)\n",
    "\n",
    "В качестве входов в модель нужно использовать TF-IDF представления документов (`X_train`), в качестве лейблов - 0 и 1, обозначающие нужный класс (`Y_train`). Начнём с модели, которая использует нормализацию.\n",
    "\n",
    "Используюя тестовый датасет и `logreg.predict` проверьте предсказания модели, вычислив accuracy - количество правильных предсказаний, делённое на количество входных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e542edc7ae84628aebf2506c905dc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff2648341ce402d836b81fe3e06eeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.78052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "X_train = tfidf_with_norm.predict(imdb[\"train\"][\"text\"])\n",
    "Y_train = imdb[\"train\"][\"label\"]\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "X_test = tfidf_with_norm.predict(imdb[\"test\"][\"text\"])\n",
    "Y_test = imdb[\"test\"][\"label\"]\n",
    "\n",
    "preds = logreg.predict(X_test)\n",
    "accuracy = (preds == Y_test).sum() / len(X_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим логистическую регрессию со второй TF-IDF моделью и сравним результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140ceb2553524d9aa816723c85a5fa95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4179886528d341559845a19948f5f1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.76624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "X_train = tfidf_without_norm.predict(imdb[\"train\"][\"text\"])\n",
    "Y_train = imdb[\"train\"][\"label\"]\n",
    "\n",
    "logreg_without_norm = LogisticRegression()\n",
    "logreg_without_norm.fit(X_train, Y_train) \n",
    "\n",
    "X_test = tfidf_without_norm.predict(imdb[\"test\"][\"text\"])\n",
    "Y_test = imdb[\"test\"][\"label\"]\n",
    "\n",
    "preds = logreg_without_norm.predict(X_test)\n",
    "accuracy = (preds == Y_test).sum() / 25000\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Опционально) TfidfVectorizer\n",
    "\n",
    "Можете изучть класс [TfidfVectorizer](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) из библиотеки scikit-learn и сравнить его со своей имплементацией, обучив логистическую регрессию с его помощью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.88292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(imdb[\"train\"][\"text\"])\n",
    "Y_train = imdb[\"train\"][\"label\"]\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "X_test = tfidf_vectorizer.transform(imdb[\"test\"][\"text\"])\n",
    "Y_test = imdb[\"test\"][\"label\"]\n",
    "\n",
    "preds = logreg.predict(X_test)\n",
    "accuracy = (preds == Y_test).sum() / X_test.shape[0]\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $n$-граммные языковые модели (15 баллов)\n",
    "\n",
    "### Расширяем Токенайзер (3 балла)\n",
    "\n",
    "Перед созданием языковой модели, расширим токенизационный класс. Добавим два флага в сигнатуру метода `encode`, чтобы управлять добавлением служебных токенов во время токенизации. Существующий метод `decode` уже пропускает `<PAD>` токен, добавим флаг `skip_special_tokens` для пропуска всех специальных токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoSTokenizerEoS(Tokenizer):\n",
    "    def encode(self, text: str, add_bos: bool = True, add_eos: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        Кодирование текста в набор индексов.\n",
    "\n",
    "        :param text: текст\n",
    "        :param add_bos: добавление begin-of-sentence токена в начало\n",
    "        :param add_eos: добавление end-of-sentence токена в конец\n",
    "        :return: набор индексов токенов\n",
    "        \"\"\"\n",
    "        encoded = super().encode(text)\n",
    "\n",
    "        if add_bos:\n",
    "            encoded.insert(0, self.bos_token)\n",
    "        \n",
    "        if add_eos:\n",
    "            encoded.append(self.eos_token)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, input_ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n",
    "\n",
    "        :param input_ids: набор индексов токенов\n",
    "        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
    "        :return: текст\n",
    "        \"\"\"\n",
    "        if skip_special_tokens:\n",
    "            input_ids = [i for i in input_ids if i not in self.special_token_ids]\n",
    "\n",
    "        return super().decode(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаём NGram Модель (12 баллов)\n",
    "\n",
    "Создайте класс `NGramLanguageModel` для построения n-граммной языковой модели. В этом задании вы можете как опираться на предложенную структуру модели, так и сделать свою имплементацию.\n",
    "\n",
    "Построение модели:\n",
    "   - Создайте метод `_build_model`, который принимает список текстов `texts` и обновляет частоты n-грамм.\n",
    "   - Для каждого текста:\n",
    "     - Токенизируйте текст и добавьте токен `\"<EOS>\"` в конец.\n",
    "     - Для каждого токена:\n",
    "       - Определите префикс длиной `n-1`.\n",
    "       - Обновите частоты n-грамм и частоты префиксов.\n",
    "\n",
    "Генерация следующего токена:\n",
    "   - Создайте метод `generate_next_token`, который принимает префикс `prefix` и возвращает следующий токен.\n",
    "   - Преобразуйте префикс в кортеж.\n",
    "   - Получите распределение частот для префикса.\n",
    "   - Если распределение пустое, верните токен `\"<UNK>\"`.\n",
    "   - Верните токен с наибольшей частотой.\n",
    "\n",
    "Автодополнение текста:\n",
    "   - Создайте метод `autocomplete`, который принимает текст `text` и максимальную длину `max_len`, и возвращает завершенный текст.\n",
    "   - Токенизируйте текст.\n",
    "   - Пока длина токенов меньше `max_len`:\n",
    "     - Определите префикс длиной `n-1`.\n",
    "     - Сгенерируйте следующий токен.\n",
    "     - Добавьте токен в список токенов.\n",
    "     - Если токен равен `\"<EOS>\"`, завершите генерацию.\n",
    "   - Декодируйте и верните текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n: int, tokenizer: BoSTokenizerEoS, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Создание n-граммной языковой модели.\n",
    "\n",
    "        :param n: порядок n-грамм\n",
    "        :param vocabulary: словарь\n",
    "        \"\"\"\n",
    "        assert n >= 2\n",
    "        self.n = n\n",
    "        self._context_length = self.n - 1 # added for convenience\n",
    "        self.tokenizer = tokenizer\n",
    "        self.frequencies = defaultdict(lambda: Counter())  # частота n-грамм\n",
    "        self.frequencies_of_prefixes = Counter()  # сумма частот n-грамм для префиксов\n",
    "        self._build_model(texts)\n",
    "\n",
    "\n",
    "    def _build_model(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Построение модели на основе списка текстов.\n",
    "\n",
    "        :param texts: список текстов\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            current_tokens = [self.tokenizer.bos_token] * self._context_length\n",
    "            tokenized = self.tokenizer.encode(text, add_bos=False, add_eos=True) # Expected output: \"some sentence here <eos>\"\n",
    "\n",
    "            # empty doc\n",
    "            if not tokenized:\n",
    "                continue\n",
    "\n",
    "            for token in tokenized:\n",
    "                prefix = tuple(current_tokens)\n",
    "\n",
    "                # increase count of token for specified prefix\n",
    "                self.frequencies[prefix][token] += 1\n",
    "\n",
    "                # add token to context\n",
    "                current_tokens.append(token)\n",
    "                current_tokens = current_tokens[1:]\n",
    "\n",
    "    def generate_next_token(self, prefix: List[int]) -> int:\n",
    "        \"\"\"\n",
    "        Жадная генерация следующего токена по префиксу.\n",
    "\n",
    "        :param prefix: префикс\n",
    "        :return: следующий токен\n",
    "        \"\"\"\n",
    "        # extend smaller context to n - 1\n",
    "        pr = prefix.copy()\n",
    "        while len(pr) < self._context_length:\n",
    "            pr.insert(0, self.tokenizer.bos_token)\n",
    "\n",
    "        # trim larger context to n - 1\n",
    "        if len(pr) > self._context_length:\n",
    "            pr = pr[-self._context_length:]\n",
    "\n",
    "        token_freqs = self.frequencies.get(tuple(pr), None)\n",
    "        if not token_freqs:\n",
    "            return self.tokenizer.unk_token_id\n",
    "        \n",
    "        most_common_next_token, _ = token_freqs.most_common(1)[0]\n",
    "        \n",
    "        return most_common_next_token\n",
    "\n",
    "    def autocomplete(self, text: str, max_len: int = 32, skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Автоматическое дополнение текста.\n",
    "\n",
    "        :param text: текст\n",
    "        :param max_len: максимальная длина текста\n",
    "        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
    "        :return: завершенный текст\n",
    "        \"\"\"\n",
    "        context = self.tokenizer.encode(text, add_bos=True)\n",
    "\n",
    "        # last token will be EOS, so generate 1 token less than max to end with <EOS>\n",
    "        max_tokens_to_generate = max_len - len(context) - 1\n",
    "        \n",
    "        # generate next tokens\n",
    "        while max_tokens_to_generate > 0:\n",
    "            next_token = self.generate_next_token(context)\n",
    "            if next_token == self.tokenizer.eos_token:\n",
    "                break\n",
    "            \n",
    "            max_tokens_to_generate -= 1\n",
    "            context.append(next_token)\n",
    "        \n",
    "        # add EOS\n",
    "        context.append(self.tokenizer.eos_token)\n",
    "        result = self.tokenizer.decode(context, skip_special_tokens)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646501906af348f786090763b87e88ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\"]\n",
    "tokenizer = BoSTokenizerEoS(corpus, min_count=1)\n",
    "ngram_lm = NGramLanguageModel(2, tokenizer, corpus)\n",
    "\n",
    "assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10) == \"Hello , Python !\"\n",
    "assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10, skip_special_tokens=False) == \"<BOS> Hello , Python ! <EOS>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Комментарии\n",
    "\n",
    "Если остались вопросы, на которые хочется получить ответ при ревью, это место для них:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вопросы\n",
    "TfIdf. То, что своя имплементация медленнее, не удивительно, но почему accuracy так отличается?\n",
    "- tfidf_without_norm      0.77\n",
    "- tfidf_with_norm         0.78\n",
    "- sclearn TfidfVectorizer 0.88\n",
    "\n",
    "если алгоритм аналогичный не должно быть таких отличий\n",
    "\n",
    "\n",
    "# Предложения\n",
    "Я бы внес 2 правки в формулировки заданий\n",
    "\n",
    "1. Обновить описание задания для TF-IDF, предложенная версия:\n",
    "\n",
    "Создайте класс `TFIDF` для вычисления TF-IDF значений. Формулы для подсчёта TF и IDF можно выбрать [тут](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Обучение модели должно осуществляться с помощью метода `fit`:\n",
    "  - метод принимает список строк `docs` и обучает модель на этом корпусе, вызывая метод `add_doc` для каждого документа.\n",
    "  - результатом тренировки является заполняется массива df (self.term2num_docs)\n",
    "\n",
    "Предсказание TF-IDF значений:\n",
    "   - Создайте метод `predict`, который принимает список строк `docs` и возвращает матрицу TF-IDF значений.\n",
    "   - Для каждого документа:\n",
    "     - Токенизируйте документ.\n",
    "     - Вычислите TF для каждого термина.\n",
    "     - Вычислите IDF для каждого термина.\n",
    "     - Заполните матрицу TF-IDF значений.\n",
    "   - Нормализуйте строки матрицы, чтобы сумма значений в каждой строке была равна 1.\n",
    "   - Метод predict обрабатывает каждый переданный документ отдельно, без учета других документов. При этом df рассчитывается исключительно на основе документов, которые были обработаны методом fit.\n",
    "\n",
    "> Важно! Не забудьте убрать `<UNK>` токен во  время подсчёта TF-IDF \n",
    "\n",
    "Для функций, которые могут долго исполнятся (`fit`, `predict`), рекомендуется использовать библиотеку tqdm.\n",
    "\n",
    "\n",
    "2. BoSTokenizerEoS.decode\n",
    ":param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
    "не описано, какие токены специальные. Указать явно надо : EOS + BOS\n",
    "\n",
    "Если промотать вниз то понятно становится, но для выполнения задания не должно требоваться пролистать ноутбук вниз"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
