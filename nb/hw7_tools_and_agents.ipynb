{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c44059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vkuznetsov/my/deepschool_llm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "from together import Together\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f4847",
   "metadata": {},
   "source": [
    "# Предобработка входных данных\n",
    "\n",
    "В данном задании мы будем ходить в онлайн модель. Предлагается все также ходить в together.ai, т.к. они дают $5 кредита при регистрации.\n",
    "\n",
    "Вначале давайте руками поиграемся с API, посмотрим, как походы в API соотносятся с тем, что мы делали в домашнем задании \"Доступные LLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ca6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3.1-70B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c702b9d",
   "metadata": {},
   "source": [
    "## Ручное форматирование промпта - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad05b7",
   "metadata": {},
   "source": [
    "Давайте попробуем собрать вход для llama3.1 руками, для этого допишем функцию `format_messages_to_prompt`.\n",
    "Она принимает messages - массив словарей, где указаны роли и текст сообщений, а возвращает она текст в формате, который нужно подать модели.\n",
    "\n",
    "Например для истории сообщений\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "```\n",
    "\n",
    "должен выдаваться итоговый промпт\n",
    "\n",
    "```text\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\n",
    "```\n",
    "\n",
    "Что важно:\n",
    "1. Текст начинается со спецтокена bos\n",
    "2. Дальше идет заголовок start_header_id + end_header_id, которые содержат роль\n",
    "3. Дальше после \\n\\n идет текст, заканчивающийся на eot_id\n",
    "4. Дальше следующий заголовок с новой ролью и т.д.\n",
    "\n",
    "**Важно** - в данной функции нельзя использовать `tokenizer.apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fb161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN_OF_TEXT_TOKEN = \"<|begin_of_text|>\"\n",
    "END_OF_TEXT_TOKEN = \"<|eot_id|>\"\n",
    "START_HEADER_ID_TOKEN = \"<|start_header_id|>\"\n",
    "END_HEADER_ID_TOKEN = \"<|end_header_id|>\\n\\n\"\n",
    "\n",
    "def format_messages_to_prompt(messages: List[Dict[str, str]]) -> str:\n",
    "    def wrap_role(role: str) -> str:\n",
    "        return START_HEADER_ID_TOKEN + role + END_HEADER_ID_TOKEN\n",
    "    \n",
    "    def wrap_message(content: str) -> str:\n",
    "        return content + END_OF_TEXT_TOKEN\n",
    "\n",
    "    result = BEGIN_OF_TEXT_TOKEN\n",
    "\n",
    "    for message in messages:\n",
    "        role: str = message[\"role\"]\n",
    "        content: str = message[\"content\"]\n",
    "\n",
    "        result += wrap_role(role)\n",
    "        result += wrap_message(content)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "reference_text = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\"\"\"\n",
    "\n",
    "\n",
    "assert format_messages_to_prompt(messages) == reference_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a536107",
   "metadata": {},
   "source": [
    "Мы также помним, что раньше у нас была `tokenizer.apply_chat_template`. Т.к. у нас неофициальный форк llama3.1, то chat_template в токенайзер нам не завезли, поэтому придется добавить его руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5db895",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "{{- bos_token }}\n",
    "{%- if custom_tools is defined %}\n",
    "    {%- set tools = custom_tools %}\n",
    "{%- endif %}\n",
    "{%- if not tools_in_user_message is defined %}\n",
    "    {%- set tools_in_user_message = true %}\n",
    "{%- endif %}\n",
    "{%- if not date_string is defined %}\n",
    "    {%- set date_string = \"26 Jul 2024\" %}\n",
    "{%- endif %}\n",
    "{%- if not tools is defined %}\n",
    "    {%- set tools = none %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
    "{%- if messages[0]['role'] == 'system' %}\n",
    "    {%- set system_message = messages[0]['content']|trim %}\n",
    "    {%- set messages = messages[1:] %}\n",
    "{%- else %}\n",
    "    {%- set system_message = \"\" %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- System message + builtin tools #}\n",
    "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
    "{%- if builtin_tools is defined or tools is not none %}\n",
    "    {{- \"Environment: ipython\\n\" }}\n",
    "{%- endif %}\n",
    "{%- if builtin_tools is defined %}\n",
    "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
    "{%- endif %}\n",
    "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
    "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
    "{%- if tools is not none and not tools_in_user_message %}\n",
    "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
    "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
    "    {{- \"Do not use variables.\\n\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \"\\n\\n\" }}\n",
    "    {%- endfor %}\n",
    "{%- endif %}\n",
    "{{- system_message }}\n",
    "{{- \"<|eot_id|>\" }}\n",
    "\n",
    "{#- Custom tools are passed in a user message with some extra guidance #}\n",
    "{%- if tools_in_user_message and not tools is none %}\n",
    "    {#- Extract the first user message so we can plug it in here #}\n",
    "    {%- if messages | length != 0 %}\n",
    "        {%- set first_user_message = messages[0]['content']|trim %}\n",
    "        {%- set messages = messages[1:] %}\n",
    "    {%- else %}\n",
    "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
    "{%- endif %}\n",
    "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
    "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
    "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
    "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
    "    {{- \"Do not use variables.\\n\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \"\\n\\n\" }}\n",
    "    {%- endfor %}\n",
    "    {{- first_user_message + \"<|eot_id|>\"}}\n",
    "{%- endif %}\n",
    "\n",
    "{%- for message in messages %}\n",
    "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
    "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
    "    {%- elif 'tool_calls' in message %}\n",
    "        {%- if not message.tool_calls|length == 1 %}\n",
    "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
    "        {%- endif %}\n",
    "        {%- set tool_call = message.tool_calls[0].function %}\n",
    "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
    "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
    "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
    "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
    "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
    "                {%- if not loop.last %}\n",
    "                    {{- \", \" }}\n",
    "                {%- endif %}\n",
    "                {%- endfor %}\n",
    "            {{- \")\" }}\n",
    "        {%- else  %}\n",
    "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
    "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
    "            {{- '\"parameters\": ' }}\n",
    "            {{- tool_call.arguments | tojson }}\n",
    "            {{- \"}\" }}\n",
    "        {%- endif %}\n",
    "        {%- if builtin_tools is defined %}\n",
    "            {#- This means we're in ipython mode #}\n",
    "            {{- \"<|eom_id|>\" }}\n",
    "        {%- else %}\n",
    "            {{- \"<|eot_id|>\" }}\n",
    "        {%- endif %}\n",
    "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
    "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
    "        {%- if message.content is mapping or message.content is iterable %}\n",
    "            {{- message.content | tojson }}\n",
    "        {%- else %}\n",
    "            {{- message.content }}\n",
    "        {%- endif %}\n",
    "        {{- \"<|eot_id|>\" }}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "{%- endif %}\n",
    "\"\"\".strip()\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea72a3",
   "metadata": {},
   "source": [
    "## Автоматическая сборка промпта - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe9b9f",
   "metadata": {},
   "source": [
    "Давайте вспомним теперь на деле, как используется chat_template! Попробуем использовать функцию `tokenizer.apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e493aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nSome system message<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThis is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nthis is a mesage from the assistant<|eot_id|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee33f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 26 Jul 2024\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\"\"\"\n",
    "\n",
    "assert prompt == reference_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed104b62",
   "metadata": {},
   "source": [
    "Обратите внимание, что в заданном chat_template указаны Cutting Knowledge Date, т.е. до данные до какого периода видела модели, и Today Date - захардкоженная дата текущего диалога.\n",
    "\n",
    "**Вопрос, обязательно напишите свой ответ здесь!**\n",
    "На что влияет аргумент `add_generation_prompt` в функции `tokenizer.apply_chat_template`? Зачем его использовать?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab24edd",
   "metadata": {},
   "source": [
    "## Походы в API - 10 баллов\n",
    "\n",
    "Теперь давайте посмотрим, как можно ходить в API. Для примера мы будем ходить в together.ai, который щедро предоставляет $5 всем зарегистрировавшимся. Вообще говоря различных провайдеров много, API у них у всех очень похожий, т.к. все мимикрируют под OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f3931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вставьте свой ключ из https://api.together.ai/\n",
    "API_KEY = \"PASTE YOUR KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af82751b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7e4d7a84a878f57edba454784f16fc4aa46fac127ae9194fb64383234fd50dea'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df9d33",
   "metadata": {},
   "source": [
    "Есть несколько способов сходить в API. Можно ходить напрямую через библиотеку **requests**. Допишите post запрос в `url` с данными `data` и заголовками `headers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9f97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Authorization': 'Bearer ' + API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "url = \"https://api.together.ai/v1/chat/completions\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"messages\": messages\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "model_answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "assert \"london\" in model_answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efb521",
   "metadata": {},
   "source": [
    "Мы подали messages, дальше они каким-то образом собрались в promt и подались модели. Мы не знаем, какой промпт используется на стороне провайдера. Вспомним про Today Date из предыдущего пункта задания - использует ли его together? Обновляют ли они его сегодняшним днем или оставляют 26 июля? Если обновляют, то по какому часовому поясу?\n",
    "\n",
    "Чтобы ответы на эти и многие другие вопросы не мучали нас по ночам, можно использовать prompt формат, а именно подать модели текст напрямую на генерацию. Давайте для этого используем `tokenizer.apply_chat_template`. Модель будет принимать текст ровно так, как вы его подадите, без каких-либо предобработок. Подумайте, нужно ли вам использовать аргумент `add_generation_prompt`?\n",
    "\n",
    "Чтобы послать запрос напрямую, нужно в предыдущем запросе убрать messages, который представляет из себя список словарей, и послать поле prompt - строку с промптом для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772bd506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ответ\n",
    "# Флаг add_generation_prompt добавляет тег начала ответа assistant, по идее его надо добавить иначе модель может продолжать промпт вопроса юзера\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ' + API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "url = \"https://api.together.ai/v1/chat/completions\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"messages\": messages\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "model_answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "assert \"london\" in model_answer.lower() and \"assistant\" not in model_answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789bff9",
   "metadata": {},
   "source": [
    "## Клиент - 5 баллов\n",
    "\n",
    "Теперь мы понимаем общую схему взаимодействия с провайдером - они предоставляют апи, куда можно посылать или промтп или историю диалога. При посылке промпта вся ответственность за формат ложится на нас, при посылке messages форматтинг происходит на стороне провайдера, но мы не всегда представляем, как он работает. Выбор в пользу того или иного варианта всегда остается на вас.\n",
    "\n",
    "Мы использовали выше библиотеку requests, чтобы послать HTTP-запрос на сервера together, однако есть способ и проще - python client. Давайте познакомимся с ним поближе. Для этого давайте используем функцию `client.chat.completions.create`. Также давайте добавим опции сэмплинга, которые в этой функции поддержаны. Их можно посылать и в запросах через requests, но мы здесь и далее будем пользоваться клиентом.\n",
    "* top_k = 100\n",
    "* temperature = 0.5\n",
    "* top_p = 0.9\n",
    "* repetition_penalty = 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88dc9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Together(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88e3ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Britain, also known as the United Kingdom (UK), is London.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    top_k=100,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.05,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "response_text = response.choices[0].message.content\n",
    "assert \"london\" in response_text.lower()\n",
    "response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c4a4c",
   "metadata": {},
   "source": [
    "Аналогично посылать просто prompt можно через `client.completions.create`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5758f",
   "metadata": {},
   "source": [
    "## Tools - 5 баллов\n",
    "\n",
    "Давайте теперь посмотрим, как можно использовать tools в связке с моделями. У нас есть функция, которая входит в базу данных и получает информацию о юзере. Базы данных, конечно же, у нас никакой нет, но у нас есть некоторая функция, которая эмулирует это поведение, так что давайте попробуем ее описать.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd5090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job': 'DeepSchool Founder', 'city': 'Novosibirsk'}\n"
     ]
    }
   ],
   "source": [
    "def get_user_info_from_db(person_name: str) -> Dict[str, str]:\n",
    "    database = {\n",
    "        \"ilya\": {\n",
    "            \"job\": \"Software Developer\",\n",
    "            \"pets\": \"dog\",\n",
    "        },\n",
    "        \"farruh\": {\n",
    "            \"job\": \"Senior Data & Solution Architect\",\n",
    "            \"hobby\": \"travelling, hiking\",\n",
    "        },\n",
    "        \"timur\": {\n",
    "            \"job\": \"DeepSchool Founder\",\n",
    "            \"city\": \"Novosibirsk\",\n",
    "        }\n",
    "    }\n",
    "    no_info = {\"err\": f\"No info about {person_name}\"}\n",
    "    return database.get(person_name.lower(), no_info)\n",
    "\n",
    "print(get_user_info_from_db(\"Timur\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e8052",
   "metadata": {},
   "source": [
    "Давайте попробуем описать эту функцию в формате json, чтобы модель могла ее увидеть!\n",
    "Заполните поля в определении дальше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb3a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_info_from_db_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_user_info_from_db\",\n",
    "        \"description\": \"function returns available user info such as job title, city of residence or hobbies\", # Напишите, что функция делает своими словами\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"person_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"User first name\"# Опишите смысл аргумента\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"person_name\"] # укажите обязательные аргументы для функции\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f5bd6",
   "metadata": {},
   "source": [
    "Теперь давайте подадим это описание в `tokenizer.apply_chat_template`. Обратите внимание на его аргумент `tools`! Не забудьте `add_generation_prompt`, если он нужен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ad9aaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"get_user_info_from_db\",\n",
      "        \"description\": \"function returns available user info such as job title, city of residence or hobbies\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"person_name\": {\n",
      "                    \"type\": \"string\",\n",
      "                    \"description\": \"User first name\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\n",
      "                \"person_name\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "What do you know about Ilya?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(conversation=messages, tokenize=False, add_generation_prompt=True, tools=[get_user_info_from_db_tool])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980a9be",
   "metadata": {},
   "source": [
    "Давайте пошлем наш запрос в модель. На выбор 2 модели, если не будет работать с 8b, то предлагается посылать в 70b.\n",
    "Для данного запроса для 8b был подобран работающий `seed=9706540181089681000`, который можно подать в функцию.\n",
    "\n",
    "Давайте воспользуемся `client.completions.create` для генерации ответа от модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25e080b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8b = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "model_70b = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2cdaebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\n",
      "{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\n"
     ]
    }
   ],
   "source": [
    "seed=9706540181089681000\n",
    "\n",
    "response_8b = client.completions.create(prompt=prompt, model=model_8b, seed=seed) # Ваш код здесь\n",
    "response_70b = client.completions.create(prompt=prompt, model=model_70b) # Ваш код здесь\n",
    "\n",
    "print(response_8b.choices[0].text)\n",
    "print(response_70b.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b870a67",
   "metadata": {},
   "source": [
    "Если все хорошо, то мы получили ответ от модели, который выглядит как некоторый структурированный вывод, который можно использовать для вызова модели. Давайте попробуем написать функцию, которая принимает ответ модели в \"сыром виде\", выбирает, какую функцию с какими аргументами вызвать. \n",
    "\n",
    "Здесь нам поможет FUNCTION_REGISTRY и то, что параметры в функцию можно передавать как словарь, например так\n",
    "```python\n",
    "def foo(a, b, c):\n",
    "    print(a, b, c)\n",
    "\n",
    "obj = {'b':10, 'c':'lee'}\n",
    "\n",
    "foo(100, **obj)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92feaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_REGISTRY = {\"get_user_info_from_db\": get_user_info_from_db}\n",
    "# На случай, если модель не генерит function call\n",
    "reference_answer = \"\"\"{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\"\"\"\n",
    "\n",
    "\n",
    "def parse_function_call(model_answer: str) -> tuple[bool, str]:\n",
    "    # Check valid JSON returned\n",
    "    try:\n",
    "        data = json.loads(model_answer)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, model_answer\n",
    "    \n",
    "    # Check if unexpected JSON returned\n",
    "    if not all(key in data for key in [\"name\", \"parameters\"]):\n",
    "        return None, model_answer\n",
    "    \n",
    "    func_name = data[\"name\"]\n",
    "    func_args = data[\"parameters\"]\n",
    "\n",
    "    # Unexpected function called\n",
    "    if func_name not in FUNCTION_REGISTRY.keys():\n",
    "        return None, model_answer\n",
    "    \n",
    "    func = FUNCTION_REGISTRY[func_name]\n",
    "    result = func(**func_args)\n",
    "    return func_name, result\n",
    "    \n",
    "\n",
    "assert parse_function_call(reference_answer)[1] == get_user_info_from_db(\"Ilya\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed947f2",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем объединить все это в историю диалога и сгенерировать моделью финальный ответ.\n",
    "Для этого в messages, где хранится наша история диалога нужно добавить\n",
    "1. Вызов function call моделью с ролью ХХХ (это часть задания, напишите сами)\n",
    "2. Ответ function call с ролью tool\n",
    "\n",
    "После этого данный промпт нужно послать модели снова, чтобы получить финальный ответ.\n",
    "Для этого опять используем `tokenizer.apply_chat_template` и `client.completions.create`.\n",
    "\n",
    "В зависимости от модели может понадобиться убрать tools (на 8b, 70b должна справиться). Для 8b опять же подобран seed=2017684582943914000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e7e6d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "It seems like I have some information about a person named Ilya. \n",
      "\n",
      "Ilya is a software developer, and he has a dog as a pet. However, I don't have any more information about him. If you could provide more context or details about Ilya, I might be able to help you better.\n",
      "\n",
      "Here are a few possibilities:\n",
      "\n",
      "1. Ilya Prigogine: He was a Russian-born Belgian chemist and Nobel laureate who made significant contributions to the field of thermodynamics and the theory of dissipative structures.\n",
      "2. Ilya Kovalchuk: He is a Russian professional ice hockey player who has played in the National Hockey League (NHL).\n",
      "3. Ilya Muromets: He was a legendary Russian warrior and prince who lived in the 12th century and was known for his bravery and strength.\n",
      "\n",
      "If none of these are the Ilya you are looking for, please provide more context or details, and I'll try to help you further.\n",
      "---\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
      "Ilya is a software developer and has a dog.\n"
     ]
    }
   ],
   "source": [
    "seed = None #2017684582943914000\n",
    "\n",
    "def chat_with_tools_manual(model: str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "    ]\n",
    "\n",
    "    # Add tool\n",
    "    prompt = tokenizer.apply_chat_template(conversation=messages, tokenize=False, add_generation_prompt=True, tools=[get_user_info_from_db_tool])\n",
    "\n",
    "    # Call model\n",
    "    response = client.completions.create(model=model, prompt=prompt, seed=seed)\n",
    "    response_text = response.choices[0].text\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "    # If tool call requested - call tool\n",
    "    function_name, result = parse_function_call(response_text)\n",
    "    \n",
    "    if function_name:\n",
    "        messages.append({\"role\": \"tool\", \"name\": function_name, \"content\": result})\n",
    "        prompt = tokenizer.apply_chat_template(conversation=messages, tokenize=False, add_generation_prompt=True)\n",
    "        a = 5 \n",
    "        result = client.completions.create(model=model, prompt=prompt, seed=seed).choices[0].text\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(f\"{model_8b}\\n{chat_with_tools_manual(model_8b)}\\n---\")\n",
    "print(f\"{model_70b}\\n{chat_with_tools_manual(model_70b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce7746",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на chat-API, как обрабатываются function calls там?\n",
    "Используем для этого уже знакомый `client.chat.completions.create`, обратим внимание на аргумент tools внутри него. Здесь рекомендуется использовать 70b модель. На всякий случай работающий seed=14157400267283583000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bae78906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "It seems like there are multiple individuals named Ilya. Here are a few notable ones:\n",
      "\n",
      "1. Ilya Kovalchuk: A Russian professional ice hockey player who played in the National Hockey League (NHL) for the Atlanta Thrashers, New Jersey Devils, and Los Angeles Kings.\n",
      "\n",
      "2. Ilya Prigogine: A Russian-born Belgian chemist who was awarded the Nobel Prize in Chemistry in 1977 for his work on non-equilibrium thermodynamics.\n",
      "\n",
      "3. Ilya Repin: A Russian painter who was a prominent figure in the development of Russian realism.\n",
      "\n",
      "4. Ilya Salkind: A Russian-born American film producer who is best known for producing the Superman film series.\n",
      "\n",
      "5. Ilya Yablonovsky: A Russian politician who served as the Minister of Culture of the Russian Federation from 1992 to 1993.\n",
      "\n",
      "6. Ilya Ilyich Mechnikov: A Russian biologist who was awarded the Nobel Prize in Physiology or Medicine in 1908 for his work on the immune system.\n",
      "\n",
      "7. Ilya Kramnik: A Russian chess grandmaster who was the World Chess Champion from 2000 to 2007.\n",
      "\n",
      "8. Ilya Grigoriyevich Frank: A Russian physicist who was awarded the Nobel Prize in Physics in 1921 for his work on the photoelectric effect.\n",
      "\n",
      "9. Ilya Ehrenburg: A Russian writer and journalist who was a prominent figure in the Soviet literary scene.\n",
      "\n",
      "10. Ilya Muromets: A legendary Russian hero who was said to have been a powerful warrior and a wise leader.\n",
      "\n",
      "These are just a few examples of notable individuals named Ilya. There may be many others who are not as well-known.\n",
      "---\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
      "Ilya is a software developer and has a dog as a pet.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def chat_with_tools_client(model: str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(messages=messages, model=model, tools=[get_user_info_from_db_tool])\n",
    "    tool_calls = response.choices[0].message.tool_calls\n",
    "\n",
    "    if tool_calls:\n",
    "        json_message = {\"tool_calls\": [t.model_dump() for t in tool_calls]}\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(json_message)})\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        if function_name not in FUNCTION_REGISTRY.keys():\n",
    "            raise ValueError(f\"Function {function_name} not defined\")\n",
    "        \n",
    "        func = FUNCTION_REGISTRY[function_name]\n",
    "        function_response = func(**function_args)\n",
    "        function_response = json.dumps(function_response)\n",
    "\n",
    "        messages.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    if tool_calls:\n",
    "        # Если еще раз передать тулзы в промпт, 8b модель отвечает хуже - вызывает ещё раз tool call\n",
    "        # response = client.chat.completions.create(messages=messages, model=model, tools=[get_user_info_from_db_tool])\n",
    "        response = client.chat.completions.create(messages=messages, model=model)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(f\"{model_8b}\\n{chat_with_tools_client(model_8b)}\\n---\")\n",
    "print(f\"{model_70b}\\n{chat_with_tools_client(model_70b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2dd36",
   "metadata": {},
   "source": [
    "Мы можем видеть, что у нас не работает предыдущий подход с полем `content`, однако должно было появиться поле `tool_calls`, которое содержит в себе информацию о вызове инструмента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13c84279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907c917",
   "metadata": {},
   "source": [
    "# Использование библиотек\n",
    "\n",
    "Теперь, когда мы руками прошли весь пути обработки function call можно посмотреть уже на готовые инструменты.\n",
    "Мы много чего сделали руками:\n",
    "1. Писали описание функции\n",
    "2. Обрабатывали ответ\n",
    "3. Вызывали функцию\n",
    "4. Возвращали все это в модель\n",
    "\n",
    "Давайте теперь посмотрим, как оно работает в библиотеках!\n",
    "\n",
    "**NB** - библиотеки развиваются и вполне, возможно, что к концу курса те интерфейсы, которые мы используем в этом домашнем задании будут уже неактуальны, но я уверен, что знаний и принципов, полученных из этих заданий хватит, чтобы адаптироваться к будущим вызовам!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d89c627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain==0.2.16 llama-index-core==v0.11.16 langchain-together==0.2.0 llama-index-llms-together==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b964f9",
   "metadata": {},
   "source": [
    "# LangChain - 5 баллов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ef83e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b08ae",
   "metadata": {},
   "source": [
    "Давайте ознакомимся с langchain-интеграцией together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7f75658",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOGETHER_API_KEY\"] = API_KEY\n",
    "\n",
    "\n",
    "llm = ChatTogether(\n",
    "    model=model_70b\n",
    ")# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d548c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilya can refer to several individuals or concepts. Here are a few possibilities:\n",
      "\n",
      "1. Ilya Kovalchuk: Ilya Kovalchuk is a Russian professional ice hockey player who has played in the National Hockey League (NHL) for teams such as the Atlanta Thrashers, New Jersey Devils, and Los Angeles Kings.\n",
      "2. Ilya Repin: Ilya Repin was a Russian painter and artist who was active in the late 19th and early 20th centuries. He is known for his realistic and detailed portraits, as well as his landscapes and historical scenes.\n",
      "3. Ilya Prigogine: Ilya Prigogine was a Russian-born Belgian physical chemist and Nobel laureate who made significant contributions to the field of thermodynamics and the study of complex systems.\n",
      "4. Ilya Muromets: Ilya Muromets is a legendary hero in Russian folklore, known for his bravery and strength. He is often depicted as a bogatyr, a type of warrior or knight, and is said to have performed many heroic deeds.\n",
      "5. Ilya (given name): Ilya is also a common given name in Russia and other Slavic countries, derived from the Hebrew name Elijah.\n",
      "\n",
      "If you could provide more context or information about the Ilya you are referring to, I would be happy to try and provide more specific information.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d698905",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрались, как базово работать с langchain, давайте попробуем добавить инструментов. Чтобы нам было не так скучно, давайте напишем новую функцию, которая считает \"волшебную операцию\".\n",
    "\n",
    "Эта функция принимает 2 строки, возвращает строку строку b в обратном порядке, сконкатенированную со строкой a. Допишите эту функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb6976bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_operation(a: str, b: str) -> str:\n",
    "    return b[::-1] + a\n",
    "\n",
    "assert magic_operation(\"456\", \"321\") == \"123456\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89726ec1",
   "metadata": {},
   "source": [
    "Теперь давайте обернем эту функцию в декоратор tool из langchain, аннотируем типы и допишем docstring. После этого можно будет автоматически сгенерировать описание функции в function call формате!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "482c8b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Executes magic operation on 2 strings and returns unified result', 'properties': {'a': {'title': 'A', 'type': 'string'}, 'b': {'title': 'B', 'type': 'string'}}, 'required': ['a', 'b'], 'title': 'magic_operation_tool', 'type': 'object'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/733p_9w13nqf8l5ss3r78ybw0000gp/T/ipykernel_8011/3530015177.py:6: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  print(magic_operation_tool.args_schema.schema())\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def magic_operation_tool(a: str, b: str) -> str: # аннотации типов\n",
    "    \"\"\"Executes magic operation on 2 strings and returns unified result\"\"\"\n",
    "    return magic_operation(a, b)\n",
    "\n",
    "print(magic_operation_tool.args_schema.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a60cf8b",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем подать запрос в нашу LLM и обогатить ее нашим function_call. Для этого нужна функция `llm.bind_tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6c465f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [magic_operation_tool]\n",
    "TOOLS_REGISTRY = {\n",
    "    t.name: t.func for t in tools_list\n",
    "}\n",
    "llm_with_tools = llm.bind_tools([magic_operation_tool])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db362d",
   "metadata": {},
   "source": [
    "Теперь давайте как и раньше:\n",
    "1. Сгенерируем ответ на messages\n",
    "2. Проверим в ответе resp.tool_calls, вызовем нужный инструмент\n",
    "3. Расширим messages ответом модели и ответом инструмента, сгенерируем финальный ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b15a3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"}\n",
    "]\n",
    "resp = llm_with_tools.invoke(messages)\n",
    "\n",
    "if resp.tool_calls:\n",
    "    # Add tool call to history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": json.dumps({\"tool_calls\": resp.tool_calls})})\n",
    "    \n",
    "    # Call tool\n",
    "    for tool_call in resp.tool_calls:\n",
    "        func = TOOLS_REGISTRY[tool_call[\"name\"]]\n",
    "        result = func(**tool_call[\"args\"])\n",
    "\n",
    "        messages.append({\n",
    "                \"tool_call_id\": tool_call[\"id\"],\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": tool_call[\"name\"],\n",
    "                \"content\": result,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9a20e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(messages) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d462fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(messages).content\n",
    "assert \"123456\" in res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb1a5b",
   "metadata": {},
   "source": [
    "# LlamaIndex - 5 баллов\n",
    "\n",
    "Аналогичный инструмент LlamaIndex. В ней не так хороша поддержка function calls не для OpenAI, поэтому придется забежать вперед и использовать ReActAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e6485db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.together import TogetherLLM\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfef1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = TogetherLLM(model=model_70b, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea9b3e",
   "metadata": {},
   "source": [
    "Скопируйте magic_operation_tool из части с langchain сюда,  но без декоратора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b6a47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_operation_tool(a: str, b: str): # аннотации типов\n",
    "    \"\"\"Executes magic operation on 2 strings and returns unified result\"\"\"\n",
    "    print(\"INSIDE FUNCTION CALL\")\n",
    "    return magic_operation(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc72e3",
   "metadata": {},
   "source": [
    "Мы можем аналогично создать инструмент с помощью `FunctionTool.from_defaults`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b50ff121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='magic_operation_tool(a: str, b: str)\\nExecutes magic operation on 2 strings and returns unified result', name='magic_operation_tool', fn_schema=<class 'llama_index.core.tools.utils.magic_operation_tool'>, return_direct=False)\n"
     ]
    }
   ],
   "source": [
    "magic_operation_tool_llamaindex = FunctionTool.from_defaults(magic_operation_tool)# Ваш код здесь\n",
    "print(magic_operation_tool_llamaindex.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73306d2",
   "metadata": {},
   "source": [
    "Давайте создадим ReActAgent: ему нужно передать tools, llm, memory=None и verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84236969",
   "metadata": {},
   "outputs": [],
   "source": [
    "LI_TOOLS = [magic_operation_tool_llamaindex]\n",
    "\n",
    "agent = ReActAgent(llm=llm, tools=LI_TOOLS, memory=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12a204d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 618ca839-9e26-4c22-9cf1-5d022ca72029. Step input: Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: magic_operation_tool\n",
      "Action Input: {'a': '456', 'b': '321'}\n",
      "\u001b[0mINSIDE FUNCTION CALL\n",
      "\u001b[1;3;34mObservation: 123456\n",
      "\u001b[0m> Running step 721b6f8c-db8b-4c73-aa34-84da1aca2a85. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The result of the magic operation on strings '456' and '321' is 123456.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"The result of the magic operation on strings '456' and '321' is 123456.\", sources=[ToolOutput(content='123456', tool_name='magic_operation_tool', raw_input={'args': (), 'kwargs': {'a': '456', 'b': '321'}}, raw_output='123456', is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"\n",
    "agent.chat(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725419ad",
   "metadata": {},
   "source": [
    "# Agents - 10\n",
    "\n",
    "Настала пора сделать своего агента!\n",
    "Попробуем сделать финансового аналитика. Требования следующие:\n",
    "бот должен по запросу данных о какой-либо компании смотреть самые большие изменения цены ее акций за последний месяц, после чего бот должен объяснить, с какой новостью это связано.\n",
    "\n",
    "Предлагается не строить сложную систему с классификаторами, а отдать всю сложную работу агенту. Давайте посмотрим, какие API нам доступны.\n",
    "\n",
    "Первым делом получение котировок - для этого нам поможет библиотека yfinance. По названию компании и периоду отчетности можно посмотреть открывающие цены на момент открытия и закрытия биржи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a00d2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-06 00:00:00-05:00</th>\n",
       "      <td>244.309998</td>\n",
       "      <td>245.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-07 00:00:00-05:00</th>\n",
       "      <td>242.979996</td>\n",
       "      <td>242.210007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-08 00:00:00-05:00</th>\n",
       "      <td>241.919998</td>\n",
       "      <td>242.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-10 00:00:00-05:00</th>\n",
       "      <td>240.009995</td>\n",
       "      <td>236.850006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-13 00:00:00-05:00</th>\n",
       "      <td>233.529999</td>\n",
       "      <td>234.399994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-14 00:00:00-05:00</th>\n",
       "      <td>234.750000</td>\n",
       "      <td>233.279999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-15 00:00:00-05:00</th>\n",
       "      <td>234.639999</td>\n",
       "      <td>237.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-16 00:00:00-05:00</th>\n",
       "      <td>237.350006</td>\n",
       "      <td>228.259995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-17 00:00:00-05:00</th>\n",
       "      <td>232.119995</td>\n",
       "      <td>229.979996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-21 00:00:00-05:00</th>\n",
       "      <td>224.000000</td>\n",
       "      <td>222.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-22 00:00:00-05:00</th>\n",
       "      <td>219.789993</td>\n",
       "      <td>223.830002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-23 00:00:00-05:00</th>\n",
       "      <td>224.740005</td>\n",
       "      <td>223.660004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-24 00:00:00-05:00</th>\n",
       "      <td>224.779999</td>\n",
       "      <td>222.779999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-27 00:00:00-05:00</th>\n",
       "      <td>224.020004</td>\n",
       "      <td>229.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-28 00:00:00-05:00</th>\n",
       "      <td>230.850006</td>\n",
       "      <td>238.259995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-29 00:00:00-05:00</th>\n",
       "      <td>234.119995</td>\n",
       "      <td>239.360001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-30 00:00:00-05:00</th>\n",
       "      <td>238.669998</td>\n",
       "      <td>237.589996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-31 00:00:00-05:00</th>\n",
       "      <td>247.190002</td>\n",
       "      <td>236.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-03 00:00:00-05:00</th>\n",
       "      <td>229.990005</td>\n",
       "      <td>228.009995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-04 00:00:00-05:00</th>\n",
       "      <td>227.250000</td>\n",
       "      <td>232.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-05 00:00:00-05:00</th>\n",
       "      <td>228.529999</td>\n",
       "      <td>232.470001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open       Close\n",
       "Date                                             \n",
       "2025-01-06 00:00:00-05:00  244.309998  245.000000\n",
       "2025-01-07 00:00:00-05:00  242.979996  242.210007\n",
       "2025-01-08 00:00:00-05:00  241.919998  242.699997\n",
       "2025-01-10 00:00:00-05:00  240.009995  236.850006\n",
       "2025-01-13 00:00:00-05:00  233.529999  234.399994\n",
       "2025-01-14 00:00:00-05:00  234.750000  233.279999\n",
       "2025-01-15 00:00:00-05:00  234.639999  237.869995\n",
       "2025-01-16 00:00:00-05:00  237.350006  228.259995\n",
       "2025-01-17 00:00:00-05:00  232.119995  229.979996\n",
       "2025-01-21 00:00:00-05:00  224.000000  222.639999\n",
       "2025-01-22 00:00:00-05:00  219.789993  223.830002\n",
       "2025-01-23 00:00:00-05:00  224.740005  223.660004\n",
       "2025-01-24 00:00:00-05:00  224.779999  222.779999\n",
       "2025-01-27 00:00:00-05:00  224.020004  229.860001\n",
       "2025-01-28 00:00:00-05:00  230.850006  238.259995\n",
       "2025-01-29 00:00:00-05:00  234.119995  239.360001\n",
       "2025-01-30 00:00:00-05:00  238.669998  237.589996\n",
       "2025-01-31 00:00:00-05:00  247.190002  236.000000\n",
       "2025-02-03 00:00:00-05:00  229.990005  228.009995\n",
       "2025-02-04 00:00:00-05:00  227.250000  232.800003\n",
       "2025-02-05 00:00:00-05:00  228.529999  232.470001"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "stock = yf.Ticker(\"AAPL\") # посмотрим котировки APPLE\n",
    "df = stock.history(period=\"1mo\")\n",
    "df[[\"Open\", \"Close\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d2b6b",
   "metadata": {},
   "source": [
    "Для поиска новостей нам поможет https://newsapi.org/\n",
    "Можно легко получить свой ключ за короткую регистрацию, дается 1000 запросов в день, каждый запрос может включать в себя ключевое слово и промежуток дат. По бесплатному апи ключу дается ровно 1 месяц, что нам подходит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "639e4310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vizio 50-Inch Smart TV Drops to Its Lowest Price Just in Time for Super Bowl Prep\n",
      "This Vizio TV is Apple AirPlay and Chromecast built-in.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.getenv(\"NEWSAPI_API_KEY\") # ваш API ключ здесь!\n",
    "api_template = \"https://newsapi.org/v2/everything?q={keyword}&apiKey={api_key}&from={date_from}\"\n",
    "\n",
    "articles = requests.get(api_template.format(keyword=\"Apple\", api_key=api_key, date_from=\"2025-01-25\")).json()\n",
    "\n",
    "for article in articles[\"articles\"]:\n",
    "    if article[\"title\"] != \"[Removed]\":\n",
    "        print(article[\"title\"])\n",
    "        print(article[\"description\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f75126",
   "metadata": {},
   "source": [
    "Очень много статей заблокированы и имеют название `[Removed]`, нужно их отфильтровать. В оставшихся статьях будем брать только title (заголовок) и description (описание или краткий пересказ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9378f6",
   "metadata": {},
   "source": [
    "Вам необходимо реализовать [ReAct Agent](https://react-lm.github.io/). Особенность этого агента заключается в том, что он вначале формирует мысль, а потом вызывает действие (function call) для достижения какой-либо цели.\n",
    "\n",
    "Что нужно сделать:\n",
    "1. Описать и реализовать function call для определения, в какой день была самая большая разница в цене акций в момент открытия и закрытия биржи. Функция получает один аргумент - название акций компании (например AAPL для Apple), а выдает словарь с 2мя полями: с датой максимальной разницы в ценах и самой разницей в ценах.\n",
    "2. Описать и реализовать function call для получения 5 релевантных новостей о компании. В качестве аргумента принимаются название компании и дата. Ваша задача - сходить в newsapi, получить новости и вернуть 5 случайных новостей, которые произошли не позже чем день торгов. Если новостей меньше 5, то верните столько, сколько получится.\n",
    "3. После этого агент должен вернуть ответ, в котором постарается аргументировать изменения в цене.\n",
    "\n",
    "\n",
    "Реализовывать агента можно любым удобным способом, в том числе взять готовые имплементации.\n",
    "1. [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/) - вдобавок можно посмотреть предыдущее задание, где он уже используется.\n",
    "2. [Langchain/Langgraph](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/#code)\n",
    "3. Написать полностью свою реализацию\n",
    "\n",
    "\n",
    "Не забудьте, что очень важно описать задачу в промпте: нужно сказать, какие цели у агента и что он должен сделать. У функций должны быть говорящие описания, чтобы LLM без лишних проблем поняла, какие есть функции и когда их использовать. По всем вопросам можно обращаться в наш телеграм-чат в канал \"Tools & Agents\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2b0dac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gf/733p_9w13nqf8l5ss3r78ybw0000gp/T/ipykernel_8011/533545011.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_diff_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mget_max_abs_price_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AAPL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gf/733p_9w13nqf8l5ss3r78ybw0000gp/T/ipykernel_8011/533545011.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1mo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"diff\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Open\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Close\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"diff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Price increase\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Open\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Close\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"Price decrease\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmax_diff_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_diff_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_diff_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my/deepschool_llm/venv/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "\n",
    "def get_max_abs_price_diff(ticker: str) -> str:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    df = stock.history(period=\"1mo\")\n",
    "    df[\"diff\"] = abs(df[\"Open\"] - df[\"Close\"])\n",
    "\n",
    "    srt = df.sort_values(by=\"diff\", ascending=False)\n",
    "    direction = \"Price increase\" if (df[\"Open\"] - df[\"Close\"]) > 0 else \"Price decrease\"\n",
    "    max_diff_ts = srt.head(1).axes[0][0]\n",
    "    max_diff_date = max_diff_ts.date()\n",
    "\n",
    "    return direction + \" \" + str(max_diff_date)\n",
    "\n",
    "\n",
    "get_max_abs_price_diff(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46708f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Use Apple’s Genmoji to Create New Kinds of Emojis\n",
      "Apple Intelligence lets you generate custom emojis when the default options aren’t hitting it.\n",
      "---\n",
      "Parallels is testing x86 emulation on Apple silicon Macs\n",
      "Parallels version 20.2 has added an early preview of x86 emulation support for Apple silicon Macs, with several limitations that mean most won’t want to use it.\n",
      "---\n",
      "The 4.8-Star-Rated 2024 15-Inch Apple MacBook Air Is $250 Off on Amazon for MLK Weekend\n",
      "The lightning-fast M3-powered Apple laptop is built for the all-new Apple Intelligence and has an incredible 18-hour battery life.\n",
      "---\n",
      "Honeywell Home’s first Matter thermostat costs just $80\n",
      "This entry-level Wi-Fi-connected smart thermostat supports Apple Home, Amazon Alexa, Google Home, Samsung SmartThings, and more platforms through Matter.\n",
      "---\n",
      "Apple is pausing notification summaries for news in the latest iOS 18.3 beta\n",
      "Apple has temporarily stopped showing notification summaries for news and entertainment apps in the iOS 18.3 beta, according to MacRumors and 9to5Mac.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_company_news(company_name: str, date_to: str) -> str:\n",
    "    args = {\n",
    "        \"api_key\": os.getenv(\"NEWSAPI_API_KEY\"),\n",
    "        \"company_name\": company_name,\n",
    "        \"date_to\": date_to,\n",
    "        \"sort_by\": \"relevancy\" # relevancy, popularity, publishedAt\n",
    "    }\n",
    "\n",
    "    num_max_results = 5\n",
    "\n",
    "    api_template = \"https://newsapi.org/v2/everything?apiKey={api_key}&q={company_name}&to={date_to}&sort_by={sort_by}\"\n",
    "    articles = requests.get(api_template.format(**args)).json()\n",
    "\n",
    "    results = \"\"\n",
    "    num_results = 0\n",
    "    for article in articles[\"articles\"]:\n",
    "        title = article[\"title\"]\n",
    "        description = article[\"description\"]\n",
    "\n",
    "        if title == \"[Removed]\":\n",
    "            continue\n",
    "\n",
    "        if article[\"title\"] != \"[Removed]\":\n",
    "            if num_results > 0:\n",
    "                results+=\"\\n---\\n\"\n",
    "\n",
    "            results += f\"{title}\\n{description}\"\n",
    "            num_results += 1\n",
    "        \n",
    "        if num_results >= num_max_results:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(get_company_news(\"Apple\", \"2025-01-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e8849",
   "metadata": {},
   "source": [
    "# Langchain agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "\n",
    "\n",
    "def init_langchain_react_agent(model_name: str) -> CompiledGraph:\n",
    "    @tool\n",
    "    def get_max_abs_price_diff_tool_lc(ticker: str) -> str:\n",
    "        \"\"\"Tool gets a date when stock prices changed the most\"\"\"\n",
    "        return get_max_abs_price_diff(ticker)\n",
    "\n",
    "    @tool\n",
    "    def get_company_news_tool_lc(company_name: str, date_to: str) -> str:\n",
    "        \"\"\"Tool gets news for specific company name. date_to field should be in format YYYY-MM-DD\"\"\"\n",
    "        return get_company_news(company_name, date_to)\n",
    "\n",
    "\n",
    "    llm = ChatTogether(model=model_name)\n",
    "    tools = [get_max_abs_price_diff_tool_lc, get_company_news_tool_lc]\n",
    "    graph = create_react_agent(llm, tools=tools)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "agent = init_langchain_react_agent(model_70b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "511b1dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64335f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Analyze stock price changes for Google\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_max_abs_price_diff_tool_lc (call_egun28zklpeedrvxok61sspl)\n",
      " Call ID: call_egun28zklpeedrvxok61sspl\n",
      "  Args:\n",
      "    ticker: GOOG\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_max_abs_price_diff_tool_lc\n",
      "\n",
      "2025-02-04\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_company_news_tool_lc (call_9ameovb0df7eif6yv46ueva5)\n",
      " Call ID: call_9ameovb0df7eif6yv46ueva5\n",
      "  Args:\n",
      "    company_name: Google\n",
      "    date_to: 2025-02-04\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_company_news_tool_lc\n",
      "\n",
      "China Is Investigating Google Over Trump's Tariffs\n",
      "Google's limited presence in China gives Beijing room to hit back harder if President Donald Trump’s trade war escalates.\n",
      "---\n",
      "Google Pixel 4a's update kills its battery life on purpose\n",
      "Google’s Pixel 4a has long been considered a great smartphone for those on a budget\n",
      ", but it just received a software update that calls that into question. The update lowers the reported battery life\n",
      ". This isn’t a side-effect of some new software. This is …\n",
      "---\n",
      "Google reportedly worked directly with Israel’s military on AI tools\n",
      "The company raced to beat Amazon to sell AI tools to the IDF, while publicly denying its involvement with Israel’s military.\n",
      "---\n",
      "Google buys part of HTC's Vive VR team for $250 million\n",
      "Google is paying HTC $250 million in cash for a deal that will give the bigger company's plans for Android XR a boost. Under the terms of their agreement, some members of the HTC Vive engineering team will be joining Google, which describes them as an \"incred…\n",
      "---\n",
      "Gemini AI is coming to Google TV devices in 2025, making them easier to talk to\n",
      "This week at CES, Google presented an early look at new software and hardware upgrades coming to Google TV devices. The new features include the integration of Gemini, Google's AI model, to the Google Assistant, as well as a new ambient experience. New smart …\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The stock price of Google changed the most on 2025-02-04. The news for Google on that date includes:\n",
      "\n",
      "* China is investigating Google over Trump's tariffs\n",
      "* Google's Pixel 4a's update kills its battery life on purpose\n",
      "* Google reportedly worked directly with Israel's military on AI tools\n",
      "* Google buys part of HTC's Vive VR team for $250 million\n",
      "* Gemini AI is coming to Google TV devices in 2025, making them easier to talk to\n",
      "\n",
      "These news articles may be the root cause of the stock price change.\n"
     ]
    }
   ],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are the agent that reasons why stocks for particular company went up or down.\n",
    "First step is to get the date when stock value changed the most. After that, query and analyze news for that company and reason why stocks have changed.\n",
    "\"\"\"\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"Analyze stock price changes for Google\")\n",
    "    ]\n",
    "}\n",
    "print_stream(agent.stream(inputs, stream_mode=\"values\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
