{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c44059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "from together import Together\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f4847",
   "metadata": {},
   "source": [
    "# Предобработка входных данных\n",
    "\n",
    "В данном задании мы будем ходить в онлайн модель. Предлагается все также ходить в together.ai, т.к. они дают $5 кредита при регистрации.\n",
    "\n",
    "Вначале давайте руками поиграемся с API, посмотрим, как походы в API соотносятся с тем, что мы делали в домашнем задании \"Доступные LLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3.1-70B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c702b9d",
   "metadata": {},
   "source": [
    "## Ручное форматирование промпта - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad05b7",
   "metadata": {},
   "source": [
    "Давайте попробуем собрать вход для llama3.1 руками, для этого допишем функцию `format_messages_to_prompt`.\n",
    "Она принимает messages - массив словарей, где указаны роли и текст сообщений, а возвращает она текст в формате, который нужно подать модели.\n",
    "\n",
    "Например для истории сообщений\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "```\n",
    "\n",
    "должен выдаваться итоговый промпт\n",
    "\n",
    "```text\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\n",
    "```\n",
    "\n",
    "Что важно:\n",
    "1. Текст начинается со спецтокена bos\n",
    "2. Дальше идет заголовок start_header_id + end_header_id, которые содержат роль\n",
    "3. Дальше после \\n\\n идет текст, заканчивающийся на eot_id\n",
    "4. Дальше следующий заголовок с новой ролью и т.д.\n",
    "\n",
    "**Важно** - в данной функции нельзя использовать `tokenizer.apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages_to_prompt(messages: List[Dict[str, str]]) -> str:\n",
    "    # Ваш код здесь\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "reference_text = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\"\"\"\n",
    "\n",
    "\n",
    "assert format_messages_to_prompt(messages) == reference_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a536107",
   "metadata": {},
   "source": [
    "Мы также помним, что раньше у нас была `tokenizer.apply_chat_template`. Т.к. у нас неофициальный форк llama3.1, то chat_template в токенайзер нам не завезли, поэтому придется добавить его руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5db895",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "{{- bos_token }}\n",
    "{%- if custom_tools is defined %}\n",
    "    {%- set tools = custom_tools %}\n",
    "{%- endif %}\n",
    "{%- if not tools_in_user_message is defined %}\n",
    "    {%- set tools_in_user_message = true %}\n",
    "{%- endif %}\n",
    "{%- if not date_string is defined %}\n",
    "    {%- set date_string = \"26 Jul 2024\" %}\n",
    "{%- endif %}\n",
    "{%- if not tools is defined %}\n",
    "    {%- set tools = none %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
    "{%- if messages[0]['role'] == 'system' %}\n",
    "    {%- set system_message = messages[0]['content']|trim %}\n",
    "    {%- set messages = messages[1:] %}\n",
    "{%- else %}\n",
    "    {%- set system_message = \"\" %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- System message + builtin tools #}\n",
    "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
    "{%- if builtin_tools is defined or tools is not none %}\n",
    "    {{- \"Environment: ipython\\n\" }}\n",
    "{%- endif %}\n",
    "{%- if builtin_tools is defined %}\n",
    "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
    "{%- endif %}\n",
    "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
    "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
    "{%- if tools is not none and not tools_in_user_message %}\n",
    "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
    "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
    "    {{- \"Do not use variables.\\n\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \"\\n\\n\" }}\n",
    "    {%- endfor %}\n",
    "{%- endif %}\n",
    "{{- system_message }}\n",
    "{{- \"<|eot_id|>\" }}\n",
    "\n",
    "{#- Custom tools are passed in a user message with some extra guidance #}\n",
    "{%- if tools_in_user_message and not tools is none %}\n",
    "    {#- Extract the first user message so we can plug it in here #}\n",
    "    {%- if messages | length != 0 %}\n",
    "        {%- set first_user_message = messages[0]['content']|trim %}\n",
    "        {%- set messages = messages[1:] %}\n",
    "    {%- else %}\n",
    "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
    "{%- endif %}\n",
    "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
    "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
    "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
    "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
    "    {{- \"Do not use variables.\\n\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \"\\n\\n\" }}\n",
    "    {%- endfor %}\n",
    "    {{- first_user_message + \"<|eot_id|>\"}}\n",
    "{%- endif %}\n",
    "\n",
    "{%- for message in messages %}\n",
    "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
    "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
    "    {%- elif 'tool_calls' in message %}\n",
    "        {%- if not message.tool_calls|length == 1 %}\n",
    "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
    "        {%- endif %}\n",
    "        {%- set tool_call = message.tool_calls[0].function %}\n",
    "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
    "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
    "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
    "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
    "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
    "                {%- if not loop.last %}\n",
    "                    {{- \", \" }}\n",
    "                {%- endif %}\n",
    "                {%- endfor %}\n",
    "            {{- \")\" }}\n",
    "        {%- else  %}\n",
    "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
    "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
    "            {{- '\"parameters\": ' }}\n",
    "            {{- tool_call.arguments | tojson }}\n",
    "            {{- \"}\" }}\n",
    "        {%- endif %}\n",
    "        {%- if builtin_tools is defined %}\n",
    "            {#- This means we're in ipython mode #}\n",
    "            {{- \"<|eom_id|>\" }}\n",
    "        {%- else %}\n",
    "            {{- \"<|eot_id|>\" }}\n",
    "        {%- endif %}\n",
    "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
    "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
    "        {%- if message.content is mapping or message.content is iterable %}\n",
    "            {{- message.content | tojson }}\n",
    "        {%- else %}\n",
    "            {{- message.content }}\n",
    "        {%- endif %}\n",
    "        {{- \"<|eot_id|>\" }}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "{%- endif %}\n",
    "\"\"\".strip()\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea72a3",
   "metadata": {},
   "source": [
    "## Автоматическая сборка промпта - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe9b9f",
   "metadata": {},
   "source": [
    "Давайте вспомним теперь на деле, как используется chat_template! Попробуем использовать функцию `tokenizer.apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e493aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template ...# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee33f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 26 Jul 2024\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\"\"\"\n",
    "\n",
    "assert prompt == reference_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed104b62",
   "metadata": {},
   "source": [
    "Обратите внимание, что в заданном chat_template указаны Cutting Knowledge Date, т.е. до данные до какого периода видела модели, и Today Date - захардкоженная дата текущего диалога.\n",
    "\n",
    "**Вопрос, обязательно напишите свой ответ здесь!**\n",
    "На что влияет аргумент `add_generation_prompt` в функции `tokenizer.apply_chat_template`? Зачем его использовать?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab24edd",
   "metadata": {},
   "source": [
    "## Походы в API - 10 баллов\n",
    "\n",
    "Теперь давайте посмотрим, как можно ходить в API. Для примера мы будем ходить в together.ai, который щедро предоставляет $5 всем зарегистрировавшимся. Вообще говоря различных провайдеров много, API у них у всех очень похожий, т.к. все мимикрируют под OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вставьте свой ключ из https://api.together.ai/\n",
    "API_KEY = \"PASTE YOUR KEY HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df9d33",
   "metadata": {},
   "source": [
    "Есть несколько способов сходить в API. Можно ходить напрямую через библиотеку **requests**. Допишите post запрос в `url` с данными `data` и заголовками `headers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Authorization': 'Bearer ' + API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"messages\": messages\n",
    "}\n",
    "\n",
    "response = requests ...# Ваш код здесь\n",
    "model_answer = response.json() ...# Ваш код здесь\n",
    "assert \"london\" in model_answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efb521",
   "metadata": {},
   "source": [
    "Мы подали messages, дальше они каким-то образом собрались в promt и подались модели. Мы не знаем, какой промпт используется на стороне провайдера. Вспомним про Today Date из предыдущего пункта задания - использует ли его together? Обновляют ли они его сегодняшним днем или оставляют 26 июля? Если обновляют, то по какому часовому поясу?\n",
    "\n",
    "Чтобы ответы на эти и многие другие вопросы не мучали нас по ночам, можно использовать prompt формат, а именно подать модели текст напрямую на генерацию. Давайте для этого используем `tokenizer.apply_chat_template`. Модель будет принимать текст ровно так, как вы его подадите, без каких-либо предобработок. Подумайте, нужно ли вам использовать аргумент `add_generation_prompt`?\n",
    "\n",
    "Чтобы послать запрос напрямую, нужно в предыдущем запросе убрать messages, который представляет из себя список словарей, и послать поле prompt - строку с промптом для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bd506",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Authorization': 'Bearer ' + API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template ...\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    ...# Ваш код здесь\n",
    "}\n",
    "\n",
    "response = requests ...# Ваш код здесь\n",
    "model_answer = response.json() ...# Ваш код здесь\n",
    "assert \"london\" in model_answer.lower() and \"assistant\" not in model_answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789bff9",
   "metadata": {},
   "source": [
    "## Клиент - 5 баллов\n",
    "\n",
    "Теперь мы понимаем общую схему взаимодействия с провайдером - они предоставляют апи, куда можно посылать или промтп или историю диалога. При посылке промпта вся ответственность за формат ложится на нас, при посылке messages форматтинг происходит на стороне провайдера, но мы не всегда представляем, как он работает. Выбор в пользу того или иного варианта всегда остается на вас.\n",
    "\n",
    "Мы использовали выше библиотеку requests, чтобы послать HTTP-запрос на сервера together, однако есть способ и проще - python client. Давайте познакомимся с ним поближе. Для этого давайте используем функцию `client.chat.completions.create`. Также давайте добавим опции сэмплинга, которые в этой функции поддержаны. Их можно посылать и в запросах через requests, но мы здесь и далее будем пользоваться клиентом.\n",
    "* top_k = 100\n",
    "* temperature = 0.5\n",
    "* top_p = 0.9\n",
    "* repetition_penalty = 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Together(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    ...# Ваш код здесь\n",
    ")\n",
    "\n",
    "response_text = response ...# Ваш код здесь\n",
    "assert \"london\" in response_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c4a4c",
   "metadata": {},
   "source": [
    "Аналогично посылать просто prompt можно через `client.completions.create`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5758f",
   "metadata": {},
   "source": [
    "## Tools - 5 баллов\n",
    "\n",
    "Давайте теперь посмотрим, как можно использовать tools в связке с моделями. У нас есть функция, которая входит в базу данных и получает информацию о юзере. Базы данных, конечно же, у нас никакой нет, но у нас есть некоторая функция, которая эмулирует это поведение, так что давайте попробуем ее описать.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_info_from_db(person_name: str) -> Dict[str, str]:\n",
    "    database = {\n",
    "        \"ilya\": {\n",
    "            \"job\": \"Software Developer\",\n",
    "            \"pets\": \"dog\",\n",
    "        },\n",
    "        \"farruh\": {\n",
    "            \"job\": \"Senior Data & Solution Architect\",\n",
    "            \"hobby\": \"travelling, hiking\",\n",
    "        },\n",
    "        \"timur\": {\n",
    "            \"job\": \"DeepSchool Founder\",\n",
    "            \"city\": \"Novosibirsk\",\n",
    "        }\n",
    "    }\n",
    "    no_info = {\"err\": f\"No info about {person_name}\"}\n",
    "    return database.get(person_name.lower(), no_info)\n",
    "\n",
    "print(get_user_info_from_db(\"Timur\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e8052",
   "metadata": {},
   "source": [
    "Давайте попробуем описать эту функцию в формате json, чтобы модель могла ее увидеть!\n",
    "Заполните поля в определении дальше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_info_from_db_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_user_info_from_db\",\n",
    "        \"description\": ..., # Напишите, что функция делает своими словами\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"person_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": ...# Опишите смысл аргумента\n",
    "                }\n",
    "            },\n",
    "            \"required\": [...] # укажите обязательные аргументы для функции\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f5bd6",
   "metadata": {},
   "source": [
    "Теперь давайте подадим это описание в `tokenizer.apply_chat_template`. Обратите внимание на его аргумент `tools`! Не забудьте `add_generation_prompt`, если он нужен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template ...# Ваш код здесь\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980a9be",
   "metadata": {},
   "source": [
    "Давайте пошлем наш запрос в модель. На выбор 2 модели, если не будет работать с 8b, то предлагается посылать в 70b.\n",
    "Для данного запроса для 8b был подобран работающий `seed=9706540181089681000`, который можно подать в функцию.\n",
    "\n",
    "Давайте воспользуемся `client.completions.create` для генерации ответа от модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e080b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8b = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "model_70b = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdaebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_8b = client.completions.create ... # Ваш код здесь\n",
    "response_70b = client.completions.create... # Ваш код здесь\n",
    "\n",
    "print(response_8b.choices[0].text)\n",
    "print(response_70b.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b870a67",
   "metadata": {},
   "source": [
    "Если все хорошо, то мы получили ответ от модели, который выглядит как некоторый структурированный вывод, который можно использовать для вызова модели. Давайте попробуем написать функцию, которая принимает ответ модели в \"сыром виде\", выбирает, какую функцию с какими аргументами вызвать. \n",
    "\n",
    "Здесь нам поможет FUNCTION_REGISTRY и то, что параметры в функцию можно передавать как словарь, например так\n",
    "```python\n",
    "def foo(a, b, c):\n",
    "    print(a, b, c)\n",
    "\n",
    "obj = {'b':10, 'c':'lee'}\n",
    "\n",
    "foo(100, **obj)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92feaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_REGISTRY = {\"get_user_info_from_db\": get_user_info_from_db}\n",
    "# На случай, если модель не генерит function call\n",
    "reference_answer = \"\"\"{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\"\"\"\n",
    "\n",
    "\n",
    "def parse_function_call(model_answer):\n",
    "    # Ваш код здесь\n",
    "    # 1. Проверим, является ли это function call.\n",
    "    # 2. Вызов нужной функции с указанными аргументами\n",
    "    ...\n",
    "    \n",
    "\n",
    "assert parse_function_call(reference_answer) == get_user_info_from_db(\"Ilya\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed947f2",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем объединить все это в историю диалога и сгенерировать моделью финальный ответ.\n",
    "Для этого в messages, где хранится наша история диалога нужно добавить\n",
    "1. Вызов function call моделью с ролью ХХХ (это часть задания, напишите сами)\n",
    "2. Ответ function call с ролью tool\n",
    "\n",
    "После этого данный промпт нужно послать модели снова, чтобы получить финальный ответ.\n",
    "Для этого опять используем `tokenizer.apply_chat_template` и `client.completions.create`.\n",
    "\n",
    "В зависимости от модели может понадобиться убрать tools (на 8b, 70b должна справиться). Для 8b опять же подобран seed=2017684582943914000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "# Добавляем ответ модели\n",
    "messages.append(...)# Ваш код здесь\n",
    "# Добавляем ответ tool\n",
    "messages.append(...)# Ваш код здесь\n",
    "\n",
    "prompt = tokenizer.apply_chat_template ... # Ваш код здесь\n",
    "# print(prompt)\n",
    "\n",
    "response_8b = client.completions.create(model=model_70b, prompt=prompt, seed=2017684582943914000)\n",
    "print(response_8b.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce7746",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на chat-API, как обрабатываются function calls там?\n",
    "Используем для этого уже знакомый `client.chat.completions.create`, обратим внимание на аргумент tools внутри него. Здесь рекомендуется использовать 70b модель. На всякий случай работающий seed=14157400267283583000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae78906",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "response = client.chat.completions.create ... # Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2dd36",
   "metadata": {},
   "source": [
    "Мы можем видеть, что у нас не работает предыдущий подход с полем `content`, однако должно было появиться поле `tool_calls`, которое содержит в себе информацию о вызове инструмента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c84279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907c917",
   "metadata": {},
   "source": [
    "# Использование библиотек\n",
    "\n",
    "Теперь, когда мы руками прошли весь пути обработки function call можно посмотреть уже на готовые инструменты.\n",
    "Мы много чего сделали руками:\n",
    "1. Писали описание функции\n",
    "2. Обрабатывали ответ\n",
    "3. Вызывали функцию\n",
    "4. Возвращали все это в модель\n",
    "\n",
    "Давайте теперь посмотрим, как оно работает в библиотеках!\n",
    "\n",
    "**NB** - библиотеки развиваются и вполне, возможно, что к концу курса те интерфейсы, которые мы используем в этом домашнем задании будут уже неактуальны, но я уверен, что знаний и принципов, полученных из этих заданий хватит, чтобы адаптироваться к будущим вызовам!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89c627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain==0.2.16 llama-index-core==v0.11.16 langchain-together==0.2.0 llama-index-llms-together==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b964f9",
   "metadata": {},
   "source": [
    "# LangChain - 5 баллов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef83e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b08ae",
   "metadata": {},
   "source": [
    "Давайте ознакомимся с langchain-интеграцией together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f75658",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOGETHER_API_KEY\"] = API_KEY\n",
    "\n",
    "\n",
    "llm = ChatTogether(...)# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d548c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d698905",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрались, как базово работать с langchain, давайте попробуем добавить инструментов. Чтобы нам было не так скучно, давайте напишем новую функцию, которая считает \"волшебную операцию\".\n",
    "\n",
    "Эта функция принимает 2 строки, возвращает строку строку b в обратном порядке, сконкатенированную со строкой a. Допишите эту функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6976bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_operation(a, b):\n",
    "    ... # Ваш код здесь\n",
    "\n",
    "assert magic_operation(\"456\", \"321\") == \"123456\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89726ec1",
   "metadata": {},
   "source": [
    "Теперь давайте обернем эту функцию в декоратор tool из langchain, аннотируем типы и допишем docstring. После этого можно будет автоматически сгенерировать описани функции в function call формате!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # декоратор\n",
    "def magic_operation_tool(a, b): # аннотации типов\n",
    "    \"\"\"\"\"\" # docstring\n",
    "    return magic_operation(a, b)\n",
    "\n",
    "print(magic_operation_tool.args_schema.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a60cf8b",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем подать запрос в нашу LLM и обогатить ее нашим function_call. Для этого нужна функция `llm.bind_tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c465f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools...# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db362d",
   "metadata": {},
   "source": [
    "Теперь давайте как и раньше:\n",
    "1. Сгенерируем ответ на messages\n",
    "2. Проверим в ответе resp.tool_calls, вызовем нужный инструмент\n",
    "3. Расширим messages ответом модели и ответом инструмента, сгенерируем финальный ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"}\n",
    "]\n",
    "resp = llm_with_tools.invoke(messages)\n",
    "...# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a20e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(messages) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d462fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(messages).content\n",
    "assert \"123456\" in res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb1a5b",
   "metadata": {},
   "source": [
    "# LlamaIndex - 5 баллов\n",
    "\n",
    "Аналогичный инструмент LlamaIndex. В ней не так хороша поддержка function calls не для OpenAI, поэтому придется забежать вперед и использовать ReActAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6485db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.together import TogetherLLM\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = TogetherLLM(model=model_70b, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea9b3e",
   "metadata": {},
   "source": [
    "Скопируйте magic_operation_tool из части с langchain сюда,  но без декоратора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_operation_tool(a, b): # аннотации типов\n",
    "    \"\"\"\"\"\" # docstring\n",
    "    print(\"INSIDE FUNCTION CALL\")\n",
    "    return magic_operation(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc72e3",
   "metadata": {},
   "source": [
    "Мы можем аналогично создать инструмент с помощью `FunctionTool.from_defaults`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_operation_tool_llamaindex = ...# Ваш код здесь\n",
    "print(magic_operation_tool_llamaindex.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73306d2",
   "metadata": {},
   "source": [
    "Давайте создадим ReActAgent: ему нужно передать tools, llm, memory=None и verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84236969",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent...# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a204d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"\n",
    "agent.chat(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725419ad",
   "metadata": {},
   "source": [
    "# Agents - 10\n",
    "\n",
    "Настала пора сделать своего агента!\n",
    "Попробуем сделать финансового аналитика. Требования следующие:\n",
    "бот должен по запросу данных о какой-либо компании смотреть самые большие изменения цены ее акций за последний месяц, после чего бот должен объяснить, с какой новостью это связано.\n",
    "\n",
    "Предлагается не строить сложную систему с классификаторами, а отдать всю сложную работу агенту. Давайте посмотрим, какие API нам доступны.\n",
    "\n",
    "Первым делом получение котировок - для этого нам поможет библиотека yfinance. По названию компании и периоду отчетности можно посмотреть открывающие цены на момент открытия и закрытия биржи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "stock = yf.Ticker(\"AAPL\") # посмотрим котировки APPLE\n",
    "df = stock.history(period=\"1mo\")\n",
    "df[[\"Open\", \"Close\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d2b6b",
   "metadata": {},
   "source": [
    "Для поиска новостей нам поможет https://newsapi.org/\n",
    "Можно легко получить свой ключ за короткую регистрацию, дается 1000 запросов в день, каждый запрос может включать в себя ключевое слово и промежуток дат. По бесплатному апи ключу дается ровно 1 месяц, что нам подходит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"...\" # ваш API ключ здесь!\n",
    "api_template = \"https://newsapi.org/v2/everything?q={keyword}&apiKey={api_key}&from={date_from}\"\n",
    "\n",
    "articles = requests.get(api_template.format(keyword=\"Apple\", api_key=api_key, date_from=\"2025-01-25\")).json()\n",
    "\n",
    "for article in articles[\"articles\"]:\n",
    "    if article[\"title\"] != \"[Removed]\":\n",
    "        print(article[\"title\"])\n",
    "        print(article[\"description\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f75126",
   "metadata": {},
   "source": [
    "Очень много статей заблокированы и имеют название `[Removed]`, нужно их отфильтровать. В оставшихся статьях будем брать только title (заголовок) и description (описание или краткий пересказ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9378f6",
   "metadata": {},
   "source": [
    "Вам необходимо реализовать [ReAct Agent](https://react-lm.github.io/). Особенность этого агента заключается в том, что он вначале формирует мысль, а потом вызывает действие (function call) для достижения какой-либо цели.\n",
    "\n",
    "Что нужно сделать:\n",
    "1. Описать и реализовать function call для определения, в какой день была самая большая разница в цене акций в момент открытия и закрытия биржи. Функция получает один аргумент - название акций компании (например AAPL для Apple), а выдает словарь с 2мя полями: с датой максимальной разницы в ценах и самой разницей в ценах.\n",
    "2. Описать и реализовать function call для получения 5 релевантных новостей о компании. В качестве аргумента принимаются название компании и дата. Ваша задача - сходить в newsapi, получить новости и вернуть 5 случайных новостей, которые произошли не позже чем день торгов. Если новостей меньше 5, то верните столько, сколько получится.\n",
    "3. После этого агент должен вернуть ответ, в котором постарается аргументировать изменения в цене.\n",
    "\n",
    "\n",
    "Реализовывать агента можно любым удобным способом, в том числе взять готовые имплементации.\n",
    "1. [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/) - вдобавок можно посмотреть предыдущее задание, где он уже используется.\n",
    "2. [Langchain/Langgraph](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/#code)\n",
    "3. Написать полностью свою реализацию\n",
    "\n",
    "\n",
    "Не забудьте, что очень важно описать задачу в промпте: нужно сказать, какие цели у агента и что он должен сделать. У функций должны быть говорящие описания, чтобы LLM без лишних проблем поняла, какие есть функции и когда их использовать. По всем вопросам можно обращаться в наш телеграм-чат в канал \"Tools & Agents\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0dac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
